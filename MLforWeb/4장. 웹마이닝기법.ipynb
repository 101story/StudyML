{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영화 리뷰 쿼리 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import files\n",
    "import os\n",
    "import numpy as np\n",
    "# get titles 영화 정보에 대한 사전 만듬\n",
    "from bs4 import BeautifulSoup\n",
    "moviehtmldir='./machine_learning_for_the_web-master/chapter_4/movie/' # 제목을 파싱\n",
    "moviedict = {} # 사전을 만듬\n",
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id = filename.split('.')[0]\n",
    "    \n",
    "    #f = open(moviehtmldir+'/'+filename, encoding='ISO-8859-1')\n",
    "    f = open(moviehtmldir+'/'+filename, encoding='ISO-8859-1')\n",
    "    #parsed_html = BeautifulSoup(f.read(), \"lxml\")\n",
    "    parsed_html = BeautifulSoup(f.read(), 'html.parser')\n",
    "    try:\n",
    "        title = parsed_html.body.h1.text\n",
    "    except:\n",
    "        title = 'none'\n",
    "    moviedict[id] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwards: Package 'stopwards' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "nltk.download('stopwards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jhee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers'] ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "print(stoplist[:20],'...')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer() # 어간 추출 \n",
    "def ListDocs(dirname):\n",
    "    docs = []\n",
    "    titles = []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:\n",
    "        f = open(dirname+'/'+filename, 'r')\n",
    "        id = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(moviedict[id])\n",
    "        docs.append(f.read())\n",
    "    return docs, titles\n",
    "\n",
    "dir='./machine_learning_for_the_web-master/chapter_4/review_polarity/txt_sentoken/'\n",
    "pos_textreviews, pos_tiltes = ListDocs(dir+'pos/') # 긍정리뷰\n",
    "neg_textreviews, neg_tiltes = ListDocs(dir+'neg/') # 부정리뷰\n",
    "tot_textreviews = pos_textreviews+neg_textreviews # 2000개의 리뷰 저장 리스트\n",
    "tot_titles = pos_tiltes+neg_tiltes # 제목 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.40417776  4.06880284  7.90825515 ...,  7.50279005  7.90825515\n",
      "  7.90825515]\n",
      "['00', '000', '0009f', '007', '00s', '03', '04', '05', '05425', '10'] ...\n"
     ]
    }
   ],
   "source": [
    "#test tf-idf 모델\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 문서 사전처리, 불용어제거, 토큰화, 스테밍 실행 \n",
    "def PreprocessTfidf(texts, stoplist=[],stem=False):\n",
    "    newtexts = []\n",
    "    for text in texts:\n",
    "        if stem:\n",
    "            tmp = [w for w in tknzr.tokenize(text) if w not in stoplist]\n",
    "        else:\n",
    "            tmp = [stemmer.tem(w) for w in [w for w in tknzr.tokenize(text) if w not in soplist]]\n",
    "        newtexts.append(' '.join(tmp))\n",
    "    return newtexts\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "processed_reviews = PreprocessTfidf(tot_textreviews, stoplist, True)\n",
    "mod_tfidf = vectorizer.fit(processed_reviews)\n",
    "vec_tfidf = mod_tfidf.transform(processed_reviews)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_)) # 사전을 만듬\n",
    "print(vectorizer.idf_)\n",
    "print(vectorizer.get_feature_names()[:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 -- 39516\n",
      "  (0, 10607)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# dump tf-idf into file\n",
    "import _pickle as pickle\n",
    "\n",
    "print(len(processed_reviews),'--',len(mod_tfidf.get_feature_names()))\n",
    "v = mod_tfidf.transform(processed_reviews)\n",
    "\n",
    "# vectorizer.pk 파일로 덤프 뜨기\n",
    "with open('vectorizer.pk', 'wb') as fin:\n",
    "    pickle.dump(mod_tfidf, fin)\n",
    "    \n",
    "# 확인 \n",
    "file = open(\"vectorizer.pk\",'rb')\n",
    "load_tfidf =  pickle.load(file)\n",
    "        \n",
    "print(load_tfidf.transform(PreprocessTfidf([' '.join(['drama'])],stoplist,True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhee\\Documents\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 153\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "# 문서를 사전 처리\n",
    "class GenSimCorpus(object):\n",
    "    def __init__(self, texts, stoplist=[],stem=False):\n",
    "        self.texts = texts\n",
    "        self.stoplist = stoplist\n",
    "        self.stem = stem\n",
    "        self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "               \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    def iter_docs(self,texts, stoplist):\n",
    "        for text in texts:\n",
    "            if self.stem:\n",
    "                yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "            else:\n",
    "                yield (x for x in tknzr.tokenize(text) if x not in stoplist)\n",
    "print(len(tot_textreviews),len(stoplist))\n",
    "corpus = GenSimCorpus(tot_textreviews, stoplist) # stem 사용 안함 원랜 True \n",
    "dict_corpus = corpus.dictionary\n",
    "ntopics = 10 # 잠재 차원 10\n",
    "# 모델이 읽을수 있는 형식으로 변환\n",
    "lsi = models.LsiModel(corpus, num_topics=ntopics, id2word=dict_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yield 키워드....generator를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lsi 객체로 쿼리 잠재 공간으로 변환에 쓰일 U,V,S 행렬 만듬\n",
    "U = lsi.projection.u\n",
    "# np.eye 대각행렬\n",
    "Sigma = np.eye(ntopics)*lsi.projection.s\n",
    "\n",
    "# calculate V\n",
    "# dict_corpus 단어 색인 dict_words\n",
    "V = gensim.matutils.corpus2dense(lsi[corpus], len(lsi.projection.s)).T / lsi.projection.s\n",
    "dict_words = {}\n",
    "for i in range(len(dict_corpus)):\n",
    "    dict_words[dict_corpus[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genism 의 Doc2Vec 보델이 다룰수 있도록 데이터 형식을 맞춰줌\n",
    "from collections import namedtuple\n",
    "\n",
    "# 사전처리\n",
    "def PreprocessDoc2Vec(text,stop=[],stem=False):\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "       words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "       words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean\n",
    "\n",
    "# namedtuple 에 각 리뷰를 담음 \n",
    "Review = namedtuple('Review','words tags')\n",
    "dir = './machine_learning_for_the_web-master/chapter_4/review_polarity/txt_sentoken/'\n",
    "do2vecstem = False\n",
    "\n",
    "reviews_pos = []\n",
    "cnt = 0\n",
    "# 사전 처리된 단어와 파일명으로 된 태그로 Review 객체를 구성\n",
    "for filename in [f for f in os.listdir(dir+'pos/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'pos/'+filename,'r')\n",
    "    reviews_pos.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['pos_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "    \n",
    "reviews_neg = []\n",
    "cnt= 0\n",
    "for filename in [f for f in os.listdir(dir+'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'neg/'+filename,'r')\n",
    "    reviews_neg.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['neg_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "\n",
    "tot_reviews = reviews_pos + reviews_neg\n",
    "\n",
    "# 일반적으로 stemmer 를 적용하지 않은 모델이 더좋게 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec 모델 생성\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# DM 아키텍처 구성(dm=1), 은닉계층(size=vec_size), 윈도우크기(window=10 단어), \n",
    "# 최소 한번이상 나타난 단어는 모델에 고려 min_count=1\n",
    "# negative 부정 샘플링, \n",
    "# hs 계층적 소프트맥스\n",
    "cores = multiprocessing.cpu_count()\n",
    "vec_size = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, size=vec_size, window=10, \n",
    "                    negative=0, hs=0, min_count=1, workers=cores)\n",
    "\n",
    "#build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "\n",
    "#train\n",
    "# 훈련은 20 epoch 동안 진행\n",
    "# 학습률 0.00\n",
    "numepochs = 20\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        print('epoch %d' % (epoch))\n",
    "        model_d2v.train(tot_reviews,total_examples=vec_size, epochs=numepochs )\n",
    "        model_d2v.alpha *= 0.99\n",
    "        model_d2v.min_alpha = model_d2v.alpha\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 쿼리 작성 단어목록\n",
    "query = ['science','future','action']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim:  0.177948650457  title:  No Telling (1991)\n",
      "sim:  0.177821146567  title:  Total Recall (1990)\n",
      "sim:  0.173783798661  title:  Time Machine, The (1960)\n",
      "sim:  0.163031796224  title:  Bicentennial Man (1999)\n",
      "sim:  0.160582512878  title:  Andromeda Strain, The (1971)\n"
     ]
    }
   ],
   "source": [
    "# 가장 비슷한 웹 페이지 다섯 개를 반환하는 스크립트 \n",
    "#sparse matrix so the metrics transform into regular vectors before computing cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# sparse matrix 포맷을 사용해 데이터 저장\n",
    "query_vec = mod_tfidf.transform(PreprocessTfidf([' '.join(query)], stoplist, True))\n",
    "\n",
    "# cosine_similarity 벡터를 정규 벡터로 변환 -> 코사인 유사도계산\n",
    "sims= cosine_similarity(query_vec,vec_tfidf)[0]\n",
    "indxs_sims = sims.argsort()[::-1]\n",
    "for d in list(indxs_sims)[:5]:\n",
    "    print('sim: ', sims[d], ' title: ', tot_titles[d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim: 3.32004078148  doc: Star Wars: Episode I - The Phantom Menace (1999)\n",
      "sim: 3.20984088836  doc: Rocky Horror Picture Show, The (1975)\n",
      "sim: 3.09025340245  doc: Alien³ (1992)\n",
      "sim: 2.78900587052  doc: Starship Troopers (1997)\n",
      "sim: 2.66131573666  doc: Wild Things (1998)\n"
     ]
    }
   ],
   "source": [
    "# 쿼리를 LSA 의 qk로 변환하고 다섯개의 유사한 웹 페이지 출력\n",
    "#LSA query\n",
    "def TransformWordsListtoQueryVec(wordslist,dict_words,stem=False):\n",
    "    q = np.zeros(len(dict_words.keys()))\n",
    "    for w in wordslist:\n",
    "        if stem:\n",
    "            q[dict_words[stemmer.stem(w)]]=1.\n",
    "        else:\n",
    "            q[dict_words[w]] = 1.\n",
    "    return q\n",
    "\n",
    "q = TransformWordsListtoQueryVec(query,dict_words) # stemm true\n",
    "\n",
    "qk =   np.dot(np.dot(q,U),Sigma)\n",
    "\n",
    "sims = np.zeros(len(tot_textreviews))\n",
    "for d in range(len(V)):\n",
    "    sims[d]=np.dot(qk,V[d])\n",
    "indxs_sims = np.argsort(sims)[::-1]  \n",
    "for d in list(indxs_sims)[:5]:\n",
    "    print('sim:',sims[d],' doc:',tot_titles[d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-fc71f03434a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mreviews_related\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_d2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery_docvec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#model_d2v.docvecs.most_similar([query_docvec], topn=3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreviews_related\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relevance:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'  title:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtot_titles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# doc2vec 모델에서 infer_vector 함수로 쿼리 목록을 벡터로 변환\n",
    "# most_similar 함수로 가장 유사한 리뷰를 찾음\n",
    "# random 파라미터는 최적화 방식에서 정해진 결과를 얻기위해 고정 값 사용\n",
    "\n",
    "#force inference to get the same result\n",
    "model_d2v.random = np.random.RandomState(1)\n",
    "query_docvec = model_d2v.infer_vector(PreprocessDoc2Vec(' '.join(query),stoplist,do2vecstem))\n",
    "\n",
    "reviews_related = model_d2v.docvecs.most_similar([query_docvec], topn=5) #model_d2v.docvecs.most_similar([query_docvec], topn=3)\n",
    "for review in reviews_related:\n",
    "    print('relevance:',review[1],'  title:',tot_titles[review[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF 가 고급 알고리즘 LSA와 Doc2Vec 보다 좋은 결과가 나옴\n",
    "# http://www.cs.cornell.edu/people/pabo/movie-review-data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사후 처리 정보 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 사전처리된 tot_textreviews 리스트 이용\n",
    "# LDA 가 다른 주제에 대한 리뷰를 수집할 수 있는지 테스트 \n",
    "# 사전 처리 작업 1, 2, 되야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"movie\" + 0.000*\"film\" + 0.000*\"one\" + 0.000*\"much\" + 0.000*\"like\" + 0.000*\"even\"'), (1, '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"movie\" + 0.000*\"film\" + 0.000*\"one\" + 0.000*\"much\" + 0.000*\"like\" + 0.000*\"even\"'), (2, '0.334*\" \" + 0.024*\"\\n\" + 0.016*\"&\" + 0.009*\"=\" + 0.006*\"nbsp\" + 0.005*\"files\" + 0.005*\"-\" + 0.005*\"x\" + 0.005*\"\\'\" + 0.004*\"series\"'), (3, '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"movie\" + 0.000*\"one\" + 0.000*\"much\" + 0.000*\"like\" + 0.000*\"get\"'), (4, '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"movie\" + 0.000*\"one\" + 0.000*\"much\" + 0.000*\"even\" + 0.000*\"like\"'), (5, '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"movie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"much\" + 0.000*\"get\"'), (6, '0.726*\" \" + 0.037*\"\\n\" + 0.017*\"\\'\" + 0.009*\"-\" + 0.003*\"film\" + 0.002*\"one\" + 0.002*\"movie\" + 0.001*\"like\" + 0.001*\"even\" + 0.001*\"time\"'), (7, '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"movie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"story\" + 0.000*\"/\"'), (8, '0.035*\" \" + 0.003*\"\\n\" + 0.001*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"movie\" + 0.000*\"one\" + 0.000*\"much\" + 0.000*\"story\" + 0.000*\"like\"'), (9, '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"movie\" + 0.000*\"film\" + 0.000*\"one\" + 0.000*\"even\" + 0.000*\"like\" + 0.000*\"much\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "from gensim import models\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# 문서를 토큰으로 변환 하고 불용어 제거(다른 토크나이저 사용해봄)\n",
    "class GenSimCorpus(object):\n",
    "    def __init__(self, texts, stoplist=[], bestwords=[], stem=False):\n",
    "        self.texts = texts\n",
    "        self.stoplist = stoplist\n",
    "        self.stem = stem\n",
    "        self.bestwords = bestwords\n",
    "        self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    def iter_docs(self, texts, stoplist):\n",
    "        for text in texts:\n",
    "            if self.stem:\n",
    "                yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "            else:\n",
    "                if len(self.bestwords) > 0:\n",
    "                       yield (x for x in tknzr.tokenize(text) if x in self.bestwords)\n",
    "                else:\n",
    "                       yield (x for x in tknzr.tokenize(text) if x not in stoplist)            \n",
    "        \n",
    "num_topics = 10\n",
    "corpus = GenSimCorpus(tot_textreviews, stoplist,[],False)\n",
    "dict_lda = corpus.dictionary\n",
    "lda = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lda,passes=10, iterations=50)\n",
    "print(lda.show_topics(num_topics=num_topics))                       \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18480\n",
      "[1, 8, 24, 26, 41, 45, 48, 51, 52, 70, 73, 75, 96, 98, 114, 115, 120, 123, 129, 131] ...\n",
      "Dictionary(18480 unique tokens: ['films', 'adapted', 'comic', 'books', 'plenty']...)\n"
     ]
    }
   ],
   "source": [
    "# 출현 빈도가 높은 단어를 걸러냄 1000번보다 크고 3번보다 작게 나타나는 단어 제외\n",
    "import copy\n",
    "from six import iteritems # 추가함 iteritems 는 dictionary 에 없음 \n",
    "out_ids = [tokenid for tokenid, docfreq in iteritems(dict_lda.dfs) if docfreq > 1000 or docfreq < 3 ]\n",
    "dict_lfq = copy.deepcopy(dict_lda)\n",
    "dict_lfq.filter_tokens(out_ids)\n",
    "dict_lfq.compactify()\n",
    "print(len(dict_lfq))\n",
    "print(out_ids[:20],'...')\n",
    "print(dict_lfq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 18480 is out of bounds for axis 1 with size 18480",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-30bc6803df18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 주제별로 나타날 확률이 가장 높은 단어 10개씩 반환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m lda_lfq = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lfq, \n\u001b[1;32m----> 5\u001b[1;33m                           passes=10, iterations=50, alpha=0.01, eta=0.01)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# topic2 의 disney, mulan, love, life 는 애니\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jhee\\Documents\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jhee\\Documents\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0meval_every\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreallen\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_no\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0meval_every\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumworkers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_perplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_docs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlencorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jhee\\Documents\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mlog_perplexity\u001b[1;34m(self, chunk, total_docs)\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[0mcorpus_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[0msubsample_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtotal_docs\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m         \u001b[0mperwordbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubsample_ratio\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msubsample_ratio\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcorpus_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m         logger.info(\"%.3f per-word bound, %.1f perplexity estimate based on a held-out corpus of %i documents with %i words\" %\n\u001b[0;32m    529\u001b[0m                     (perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words))\n",
      "\u001b[1;32mC:\\Users\\jhee\\Documents\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mbound\u001b[1;34m(self, corpus, gamma, subsample_ratio)\u001b[0m\n\u001b[0;32m    732\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bound: at document #%i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m                 \u001b[0mgammad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m                 \u001b[0mgammad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jhee\\Documents\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m             \u001b[0mexpElogbetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[1;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 18480 is out of bounds for axis 1 with size 18480"
     ]
    }
   ],
   "source": [
    "# LDA 모델을 10개의 주제에 대해 훈련시킴\n",
    "# passes : 코퍼스에 대한 훈련 횟수\n",
    "# 주제별로 나타날 확률이 가장 높은 단어 10개씩 반환\n",
    "lda_lfq = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lfq, \n",
    "                          passes=10, iterations=50, alpha=0.01, eta=0.01)\n",
    "\n",
    "# topic2 의 disney, mulan, love, life 는 애니\n",
    "# topic6 의 action, alien, bad, planet 은 판타니 sci-fi \n",
    "for t in range(num_topics):\n",
    "    print('topic ',t,' words: ', la_lfq.print_topic(t,topn=10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_lfq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-b5dc77914620>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_titles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcorpus_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_lfq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mGenerateDistrArrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_lda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_lfq' is not defined"
     ]
    }
   ],
   "source": [
    "#topics for each doc\n",
    "# 실제 가장 확률이 높은 주제가 6인 영화를 쿼리 하는 코드\n",
    "def GenerateDistrArrays(corpus):\n",
    "    for i, idst in enumerate(corpus[:10]):\n",
    "        dist_array = np.zeros(num_topics)\n",
    "        for d in dist:\n",
    "            dist_array[d[0]] = d[1]\n",
    "        if dist_array.argmax() == 6:\n",
    "            print(tot_titles[i])\n",
    "\n",
    "corpus_lda = lda_lfq[corpus]\n",
    "GenerateDistrArrays(corpus_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 제목이 대부분 sci-fi 와 판다지 영화 \n",
    "# 주제 공간에서 문서 표현 lda_lfq[corpus] 를 군집화 알고리즘에 적용 (숙제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opinion Mining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jhee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'WordListCorpusReader' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-7ba478163eeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'pos/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mreviews_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPreprocessReviews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstoplist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo2vecstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmoviedict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pos_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mcnt\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mtot_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreviews_pos\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreviews_neg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-7ba478163eeb>\u001b[0m in \u001b[0;36mPreprocessReviews\u001b[1;34m(text, stop, stem)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtknzr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mwords_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mwords_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-7ba478163eeb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtknzr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mwords_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mwords_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'WordListCorpusReader' is not iterable"
     ]
    }
   ],
   "source": [
    "# 데이터 사전처리 \n",
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "def PreprocessReviews(text, stop=[], stem=False):\n",
    "    #print profile\n",
    "    words =tknzr.tokenize(text)\n",
    "    if stem:\n",
    "        words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stopwords]]\n",
    "    else:\n",
    "        words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean\n",
    "\n",
    "Review = namedtuple('Review', 'woruuds title tags')\n",
    "dir = './machine_learning_for_the_web-master/chapter_4/review_polarity/txt_sentoken/'\n",
    "do2vecstem = True\n",
    "reviews_pos = []\n",
    "cnt = 0\n",
    "for filename in [f for f in os.listdir(dir+'pos/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'pos/'+filename, 'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_pos.append(Review(PreprocessReviews(f.read(), stoplist, do2vecstem),moviedict[id],['pos_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "tot_reviews = reviews_pos + reviews_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 800\n",
      "800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#split in test training sets\n",
    "# 데이터를 nltk 라이브러리가 처리할 수 있는 방식으로 데이터 분리\n",
    "\n",
    "\n",
    "def word_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "negfeatures = [(word_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(word_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print(portionpos,'-',portionneg)\n",
    "# training 80 test 20\n",
    "# 훈련, 테스트 집합별로 튜플 목록을 구성 하거나 또는 \n",
    "# 문서의 단어가 들어있는 사전과 레이블과 함께 튜플 목록을 구성\n",
    "trainfeatures = negfeatures[:portionneg] + posfeatures[:portionpos]\n",
    "print(len(trainfeatures))\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "print(len(testfeatures))\n",
    "#shuffle(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  200\n",
      "error rate:  0.0\n"
     ]
    }
   ],
   "source": [
    "# nltk 라이브러리를 NaiveBayesClassifier 이용 다항분포 훈련\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "#training naive bayes \n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "\n",
    "# 오류 체크 \n",
    "err = 0\n",
    "print('test on: ',len(testfeatures))\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print('error rate: ',err/float(len(testfeatures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 바이그램 (연속된 단어의 쌍 : 연어) 계산해 결과를 개선 \n",
    "# 높은 빈도로 발생하는 바이그램을 찾을 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  -  800\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A ELE probability distribution must have at least one bin.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-ddad4e578e67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mtrainfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnegfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mportionpos\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mposfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mportionneg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m##test bigram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jhee\\Documents\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;31m# Create the P(label) distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mlabel_probdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_freqdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Create the P(fval|label, fname) distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jhee\\Documents\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, freqdist, bins)\u001b[0m\n\u001b[0;32m    889\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspecified\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m         \"\"\"\n\u001b[1;32m--> 891\u001b[1;33m         \u001b[0mLidstoneProbDist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jhee\\Documents\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, freqdist, gamma, bins)\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m             raise ValueError('A %s probability distribution ' % name +\n\u001b[1;32m--> 771\u001b[1;33m                              'must have at least one bin.')\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A ELE probability distribution must have at least one bin."
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from random import shuffle\n",
    "\n",
    "# x²테스트 : 파이계수의 제곱과 바이그램 총 발생건수 N 의 곱\n",
    "# x²테스트는 전체 코퍼스에서 가장 유익한 단어를 추출하는데 사용\n",
    "# x²척도로 문서별 500 개의 가장 좋은 바이그램 선택 \n",
    "#train bigram:\n",
    "def bigrams_words_features(words, nbigrams=200, measure=BigramAssocMeasures.chi_sq):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(measure, nbigrams)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "\n",
    "\n",
    "negfeatures = [(bigrams_words_features(r.words,500), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(bigrams_words_features(r.words,500), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print(portionpos, ' - ', portionneg)\n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print(len(trainfeatures))\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "\n",
    "##test bigram\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print('test on: ',len(testfeatures))\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print('error rate: ',err/float(len(testfeatures)))\n",
    "\n",
    "# 단어의 중요도 점수화 : 긍정 혹은 부정 문서의 빈도와 비교\n",
    "# 예) great : 긍정 리뷰에서 x²이 높고 부정리뷰 낮다면 이단어는 긍정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-55-e09e7da86e8f>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-55-e09e7da86e8f>\"\u001b[1;36m, line \u001b[1;32m32\u001b[0m\n\u001b[1;33m    best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:10000]\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 코퍼스의 가장 중요한 단어 만개는 전체 코퍼스에서 총 빈도와 \n",
    "# 긍정 및 부정 부분집합에서의 빈도를 계산\n",
    "\n",
    "import nltk.classify.util, nltk.metrics\n",
    "tot_poswords = [val for l in [r.words for r in reviews_pos] for val in l]\n",
    "tot_negwords = [val for l in [r.words for r in reviews_neg] for val in l]\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()\n",
    " \n",
    "for word in tot_poswords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['pos'][word.lower()] +=1\n",
    " \n",
    "for word in tot_negwords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['neg'][word.lower()] +=1\n",
    "pos_words = len(tot_poswords)\n",
    "neg_words = len(tot_negwords)\n",
    "\n",
    "tot_words = pos_words + neg_words\n",
    "#select the best words in terms of information contained in the two classes pos and neg\n",
    "word_scores = {}\n",
    " \n",
    "for word, freq in word_fd.iteritems():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "                (freq, pos_words), tot_words)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "                (freq, neg_words), tot_words)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "print('total: ',len(word_scores))\n",
    "best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:10000]\n",
    "bestwords = set([w for w, s in best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-56-7ec3359b4041>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-56-7ec3359b4041>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    print portionpos,'-',portionneg\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# bestwords 의 단어만 니용해 나이브 베이즈에 훈련\n",
    "#training naive bayes with chi square feature selection of best words\n",
    "def best_words_features(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])\n",
    "\n",
    "negfeatures = [(best_words_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(best_words_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print portionpos,'-',portionneg\n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print len(trainfeatures)\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "##test with feature chi square selection\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print 'test on: ',len(testfeatures)\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print 'error rate: ',err/float(len(testfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-57-d141b1738c03>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-57-d141b1738c03>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    print 'epoch %d' % (epoch)\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "shuffle(tot_reviews)\n",
    "cores = multiprocessing.cpu_count()\n",
    "vec_size = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, size=vec_size, window=5, negative=0, hs=0, min_count=1, workers=cores)\n",
    "\n",
    "#build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "#train\n",
    "numepochs= 20\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        print 'epoch %d' % (epoch)\n",
    "        model_d2v.train(tot_reviews)\n",
    "        model_d2v.alpha *= 0.99\n",
    "        model_d2v.min_alpha = model_d2v.alpha\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터가 작아 교차 검증을 해야 한다. (3장..) 숙제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doc2Vec 벡터는 분류기를 훈련하기 위해 사용\n",
    "# Doc2vec 벡터는 이미 훈련해 model_d2v.docvecs 객체 저장 (가정)\n",
    "# training 80 test 20 \n",
    "\n",
    "#split train,test sets\n",
    "trainingsize = 2*int(len(reviews_pos)*0.8)\n",
    "\n",
    "train_d2v = np.zeros((trainingsize, vec_size))\n",
    "train_labels = np.zeros(trainingsize)\n",
    "test_size = len(tot_reviews)-trainingsize\n",
    "test_d2v = np.zeros((test_size, vec_size))\n",
    "test_labels = np.zeros(test_size)\n",
    "\n",
    "cnt_train = 0\n",
    "cnt_test = 0\n",
    "for r in reviews_pos:\n",
    "    name_pos = r.tags[0]\n",
    "    if int(name_pos.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_pos]\n",
    "        test_labels[cnt_test] = 1\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_pos]\n",
    "        train_labels[cnt_train] = 1\n",
    "        cnt_train +=1\n",
    "\n",
    "for r in reviews_neg:\n",
    "    name_neg = r.tags[0]\n",
    "    if int(name_neg.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_neg]\n",
    "        test_labels[cnt_test] = 0\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_neg]       \n",
    "        train_labels[cnt_train] = 0\n",
    "        cnt_train +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-60-6c8d41c98139>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-60-6c8d41c98139>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    print 'accuracy:',classifier.score(test_d2v,test_labels)\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# SVM 분류기 혹은 로지스틱 회귀 분석모델로도 훈련 가능함\n",
    "# 훈련 데이터 집합이 작아 정확도가 낮음 \n",
    "# 데이터 집합이 작으면 신경망 처럼 대규모 파라미터 훈련은 힘듬\n",
    "\n",
    "#train log regre\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_d2v, train_labels)\n",
    "print 'accuracy:',classifier.score(test_d2v,test_labels)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(train_d2v, train_labels)\n",
    "print 'accuracy:',clf.score(test_d2v,test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-61-52a9a951074d>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-61-52a9a951074d>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print clf.score(test_d2v,test_labels)\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#svm linear\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_d2v, train_labels)\n",
    "print clf.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
