{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손실함수 : 결과값을 가장 작게 만드는 함수 가중치 매개변수를 찾을떄 사용 \n",
    "\n",
    "# 4.1 데이터 학습\n",
    "\n",
    "## 4.1.1 데이터 주도학습\n",
    "특징을 추출하고 벡터가 가지는 가중치를 부여\n",
    "\n",
    "종단간 기계학습 : 처음부터 끝까지 입력에서 결과를 얻는다. \n",
    "\n",
    "## 4.1.2 훈련데이터와 시험데이터 \n",
    "범용능력을 평가하기 위해서 train, test 데이터 분리 \n",
    "\n",
    "# 4.2 손실함수 \n",
    "loss function : 최적의 매겨변수 값을 탐색 \n",
    "\n",
    "평균 제곱 오차, 교차 엔트로피 오차 사용\n",
    "\n",
    "\n",
    "## 4.2.1 평균 제곱 오차 \n",
    "\n",
    "mean squared error MSE : 각 원소의 출력(추정) 값과 정답 레이블의 차를 제곱한 후 그 총합 \n",
    "\n",
    "one hot encoding: 한원소만 1로 하고 그외는 0으로 나타내는 표기범\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.095\n",
      "0.415\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.2,0.7,0.2,0.1,0,0,0,0,0]\n",
    "\n",
    "print(mean_squared_error(np.array(y), np.array(t)))\n",
    "\n",
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.2,0.7,0.2,0.1,0,0.8,0,0,0]\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오답에 가까울 수록 결과가 크다.\n",
    "\n",
    "## 4.2.2 교차 엔트로피 \n",
    "cross entroypy CEE : 정답에 해당하는 인덱스만 1 log 를 곱해도 나머진 모두 0이므로 결과에 영향을 주지 않는다. 정답일떄의 출력이 전체 값을 정함\n",
    "\n",
    "정답에 해당하는 출력이 커질수록 0에 다가가다가 출력이 1일떄 0이 됨, 반대로 정답일때의 추렭이 작아질수록 오차는 커짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "# np.log(0) 이면 -inf 가 되어 계산을 진행할수 없음 0이 되지 않도록 해줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356674801082\n",
      "0.69314698056\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.2,0.7,0.2,0.1,0,0,0.1,0.2,0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "y = [0.1,0.2,0.5,0.2,0.1,0,0.8,0,0,0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차가 작은 처번째 추정이 정답일 가능성이 높다고 판단\n",
    "\n",
    "## 4.2.3 미니배치 학습\n",
    "\n",
    "모두에 대한 손실 함수의 합을 구함 평균 손실 함수 \n",
    "\n",
    "일부만 골라서 학습 미니배치 학습 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 \n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataset.mnist import load_mnist\n",
    "from common.functions import sigmoid, softmax\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize =True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 훈련 데이터에서 무작위로 10장만 뺌 np.random.choice 이용\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3224, 33904, 37386,  9357, 15273, 21739, 43102, 31675,   318, 50811])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4 (배치용) 교차 엔트로피 오차 구현하기 \n",
    "\n",
    "데이터 하나당 교차 엔트로피 오차를 구하는 경우 reshape 함수로 데이터 shape 을 바꿈\n",
    "배치크기로 나눠 정규화를 하고 이미지 1장단 평균의 교차 엔트로피 오차를 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot 이 아닌 숫자 레이블일 경우 \n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[arange(batch_size), t]))/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* np.log(y[arange(batch_size), t])  설명<br>\n",
    "0 부터 batch_size-1 까지 배열을 생성 <br>\n",
    "각 데이터 정답 레이블에 해당하는 신경망의 출력을 추출 <br>\n",
    "예) [y[0,2],y[1,7],y[3,9],y[4,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.5 왜 손실 함수를 설정하는가?\n",
    "\n",
    "손실함수의 미분 : 가중치 매개변수의 값을 아주 조금 변화시켰을때, 손실함수가 어떻게 변하냐 <br>\n",
    "음수면 가중치 매개변수를 양으로 양이면 음수로 변화 시킴 <br>\n",
    "미분 값이 0 이면 멈춤 <br>\n",
    "\n",
    "계단 함수의 경우 미분한 값이 대부분 0 시그모이드는 연속적으로 변함 <br>\n",
    "기울기가 0이 되지 않아 올바르게 학습할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 수치 미분\n",
    "\n",
    "경사법 기울기(경사) 값을 기준으로 나아가야 하는 방향 \n",
    "\n",
    "## 4.3.1 미분\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=10e-50\n",
    "    return (f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제점1) 반올림 오차 소숫점 8자리 이하는 생략되어 0이 되버림<br>\n",
    "문제점2) 차분 x+h 와 x 사이의 기울기에 해당됨 x 의 진정한 접선이 아니다. \n",
    "\n",
    "x+h 와 x-h 를 사용하여 x 를 중심으로 전후 차분을 계산 : 중심차분(중앙차분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=1e-4 #0.001\n",
    "    return (f(x+h)-f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 수치미분의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLyISJBT4IiJB4v8Avs8d1Tqeb80AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d5734e9908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5일떄 변화량\n",
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10일떄 변화량\n",
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3 편미분\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 #np.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0=3 x1=4 x0 에 대한 편미분\n",
    "def function_tmp1(x):\n",
    "    return x*x + 4.0**2.0\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0=3 x1=4 x1 에 대한 편미분\n",
    "def function_tmp1(x):\n",
    "    return  3.0**2.0+x*x\n",
    "numerical_diff(function_tmp1, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 기울기 (gradient)\n",
    "\n",
    "모든 변수의 평미분을 벡터로 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 변수의 편미분 (기울기) 값 구하기 \n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # k 와 shape 이 같은 0배열을 생성 \n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  8.]\n",
      "[ 0.  4.]\n",
      "[ 6.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "기울기가 가리키는 쪽: 각 장소에서 함수의 출력 값을 가장 줄이는 방향 (최소값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.4.1 경사 하강법\n",
    "최적의 매개변수 찾기 \n",
    "\n",
    "* 극소값, 최소값, 안장점 (sddle point) 이 되는 장소에서는 기울기가 0 \n",
    "    * 복잡하고 찌그러진 모양의 함수라면 평평한곳으로 파고들면 고원(plateau) 라는 학습이 진행되지 않는 정체기에 빠질수 있다.\n",
    "    \n",
    "* 경사법 : 기울기를 구하고 -> 기울어진 방향으로 일정 거리만큼 이동 반복 \n",
    "    * 학습률 : 매개변수 값을 얼마나 갱신할지 갱신하는 양 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.11110793e-10,   8.14814391e-10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.58983747e+13,  -1.29524862e+12])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 경우 \n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "큰 값으로 발산해버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 작은 경우 \n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "거의 갱신되지 않은 채로 끝남 \n",
    "\n",
    "* 하이퍼파라미터 : 학습률 같은 매개변수<br> \n",
    "    가중치, 편향 같은 매개변수는 훈련 데이터와 학습 알고리즘에 의해 자동으로 획득<br> \n",
    "    하이퍼파라미터는 직접 설정해줘야함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2 신경망의 기울기 \n",
    "\n",
    "가중치 매개변수에 관한 손실함수의 기울기 <br>\n",
    "W 가 조금 변경했을 떄 손실함수 L 이 얼마나 변화하느냐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 신경망 기울기 구하기 \n",
    "# x 입력 데이터, t 정답 레이블 \n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    # 2 * 3 가중치 매개변수\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    # 예측함수\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    #손실함수\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48310344  1.90941639 -0.10516501]\n",
      " [ 0.68532831  0.45945631 -0.85851639]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90665754  1.55916052 -0.83576375]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8723504888714944"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0,0,1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19383368  0.37222683 -0.56606051]\n",
      " [ 0.29075052  0.55834025 -0.84909077]]\n"
     ]
    }
   ],
   "source": [
    "# numerical_gradient 내부에서 f(x) 를 실행 -> 일관성을 위해서 f(W) 를 정의\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "* 전제 <br>\n",
    "    가중치와 편향을 훈련데이터에 적응하는 과정 : 학습\n",
    "* 1단계-미니배치 <br>\n",
    "    훈련데이터중에 일부 선별 \n",
    "* 2단계-기울기 산출 <br>\n",
    "    손실함수 값을 줄이기 위해 가중치 매개변수의 기울기 구함 손실 함수를 작게하는 방향을 구함\n",
    "* 3단계-매개변수 갱신 <br>\n",
    "    가중치 매개변수를 기울기 방향으로 갱신 \n",
    "* 4단계-반복\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **SGD** 확률적 경사 하강법 (stochastic gradient descent) <br>\n",
    "    확률 적으로 무작위로 골라낸 데이터 로 하는 경사하강법\n",
    "    \n",
    "## 4.5.1 2층 신경망 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    # 입력충의 뉴런수, 은닉층의 뉴런수, 출력층의 뉴런수\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        # 1층의 가중치 \n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        # 2층의 가중치 \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    # 예측 : 이미지 데이터\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # 손실함수값 x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    # 예측 정확도\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # 가중치 매개변수 기울기 x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        # 1층 가중치의 기울기\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        # 2층 가중치의 기울기\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # numerical_gradient 개선판 \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# 신경망에 필요한 매개변수 저장\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09661801,  0.09823199,  0.10047899,  0.10625221,  0.09984452,\n",
       "         0.10113933,  0.09909032,  0.10163015,  0.10220259,  0.09451189],\n",
       "       [ 0.09702084,  0.09876809,  0.10043464,  0.10595654,  0.09913132,\n",
       "         0.1010357 ,  0.09889177,  0.10200017,  0.10205234,  0.0947086 ],\n",
       "       [ 0.09642484,  0.09856958,  0.10062087,  0.10651354,  0.09919013,\n",
       "         0.10059105,  0.09887902,  0.10198866,  0.10233317,  0.09488912],\n",
       "       [ 0.09639613,  0.09853308,  0.10061959,  0.1060782 ,  0.09941427,\n",
       "         0.10076057,  0.09907962,  0.10190778,  0.10207527,  0.09513549],\n",
       "       [ 0.096157  ,  0.09864748,  0.10077103,  0.10624788,  0.09990382,\n",
       "         0.10077861,  0.09904095,  0.10149305,  0.10207702,  0.09488317],\n",
       "       [ 0.0965733 ,  0.09866743,  0.10049966,  0.10631646,  0.09950652,\n",
       "         0.100639  ,  0.09905209,  0.10174237,  0.10210188,  0.0949013 ],\n",
       "       [ 0.0964438 ,  0.09849396,  0.10067279,  0.1059192 ,  0.09964911,\n",
       "         0.10096882,  0.09899076,  0.10175873,  0.10223044,  0.09487239],\n",
       "       [ 0.09657587,  0.09866106,  0.10039193,  0.10638895,  0.09922   ,\n",
       "         0.1005554 ,  0.09919081,  0.10173015,  0.102276  ,  0.09500982],\n",
       "       [ 0.09626664,  0.0986377 ,  0.10033836,  0.10638344,  0.09934022,\n",
       "         0.10069705,  0.09888089,  0.10200228,  0.10245828,  0.09499512],\n",
       "       [ 0.09645758,  0.09878853,  0.10018838,  0.10609402,  0.09948069,\n",
       "         0.10090117,  0.09893227,  0.10202529,  0.10230728,  0.09482479],\n",
       "       [ 0.09625684,  0.09836252,  0.10055615,  0.10627081,  0.09987768,\n",
       "         0.10015035,  0.0992955 ,  0.10211087,  0.10219164,  0.09492766],\n",
       "       [ 0.09645473,  0.09828676,  0.10043809,  0.10627269,  0.09968265,\n",
       "         0.1006638 ,  0.09898532,  0.10202748,  0.10220489,  0.09498359],\n",
       "       [ 0.09621455,  0.09832933,  0.10041787,  0.10653199,  0.09962813,\n",
       "         0.10094151,  0.09910892,  0.10180584,  0.10222619,  0.09479567],\n",
       "       [ 0.0965126 ,  0.09871691,  0.10024461,  0.10626239,  0.09952332,\n",
       "         0.10104197,  0.09896554,  0.10162985,  0.10228587,  0.09481694],\n",
       "       [ 0.09661367,  0.09873086,  0.1003321 ,  0.10635276,  0.09958106,\n",
       "         0.1009093 ,  0.09921093,  0.10164345,  0.10199523,  0.09463064],\n",
       "       [ 0.09639374,  0.09888914,  0.10040327,  0.10646484,  0.09919105,\n",
       "         0.10091612,  0.09919126,  0.10187916,  0.10222371,  0.09444772],\n",
       "       [ 0.09620807,  0.09872265,  0.10019579,  0.10651604,  0.09942062,\n",
       "         0.10075857,  0.09907399,  0.10174445,  0.10219115,  0.09516867],\n",
       "       [ 0.09648968,  0.09827297,  0.10056828,  0.10628599,  0.09967929,\n",
       "         0.10076894,  0.09918457,  0.10172367,  0.1021824 ,  0.09484421],\n",
       "       [ 0.0964806 ,  0.09855646,  0.10003011,  0.10634274,  0.09984345,\n",
       "         0.1008231 ,  0.0988445 ,  0.10172835,  0.10216154,  0.09518916],\n",
       "       [ 0.09633662,  0.09854292,  0.10041067,  0.10614175,  0.09964619,\n",
       "         0.10066264,  0.09947008,  0.10168374,  0.10252652,  0.09457887],\n",
       "       [ 0.09636242,  0.09881579,  0.10032797,  0.10624864,  0.09954995,\n",
       "         0.10080709,  0.09900423,  0.10175114,  0.10248303,  0.09464977],\n",
       "       [ 0.09660505,  0.09838743,  0.10035023,  0.10597219,  0.0994768 ,\n",
       "         0.10079491,  0.09977302,  0.10190479,  0.10214646,  0.09458913],\n",
       "       [ 0.09620534,  0.09895214,  0.10060463,  0.10639854,  0.09919797,\n",
       "         0.10101504,  0.09882229,  0.10162335,  0.10216894,  0.09501174],\n",
       "       [ 0.09652111,  0.09830338,  0.10049695,  0.10616254,  0.09981816,\n",
       "         0.10081002,  0.09900986,  0.10186649,  0.10234553,  0.09466596],\n",
       "       [ 0.09644955,  0.09884078,  0.10029594,  0.10602393,  0.09990746,\n",
       "         0.1005583 ,  0.09905926,  0.10178403,  0.1022857 ,  0.09479505],\n",
       "       [ 0.09622067,  0.09824835,  0.10063223,  0.10639181,  0.09957093,\n",
       "         0.10098481,  0.09894833,  0.10161041,  0.10228871,  0.09510376],\n",
       "       [ 0.09642763,  0.09827951,  0.10078432,  0.10622419,  0.09966981,\n",
       "         0.10039943,  0.09958859,  0.10174155,  0.10215368,  0.0947313 ],\n",
       "       [ 0.0966106 ,  0.09853792,  0.10084355,  0.1062928 ,  0.09948356,\n",
       "         0.10058731,  0.0989475 ,  0.10169125,  0.10215264,  0.09485287],\n",
       "       [ 0.09686993,  0.09856873,  0.10055861,  0.1061739 ,  0.09939444,\n",
       "         0.1007276 ,  0.09912766,  0.10185342,  0.10230488,  0.09442082],\n",
       "       [ 0.09651871,  0.09847943,  0.10032398,  0.10621693,  0.09940601,\n",
       "         0.10104754,  0.09917987,  0.101697  ,  0.10219454,  0.09493598],\n",
       "       [ 0.09647787,  0.09848258,  0.10060122,  0.10617802,  0.0992415 ,\n",
       "         0.10100586,  0.09895606,  0.10183974,  0.10190135,  0.0953158 ],\n",
       "       [ 0.09653268,  0.098442  ,  0.10078751,  0.10594655,  0.09958113,\n",
       "         0.10086027,  0.09888634,  0.10195982,  0.10204868,  0.09495502],\n",
       "       [ 0.09664566,  0.0986948 ,  0.10029758,  0.1059169 ,  0.09975832,\n",
       "         0.10054159,  0.0990213 ,  0.10211741,  0.10205941,  0.09494703],\n",
       "       [ 0.09659982,  0.09863483,  0.10048801,  0.10631511,  0.09971997,\n",
       "         0.10043024,  0.09895579,  0.10187114,  0.10206033,  0.09492475],\n",
       "       [ 0.09623993,  0.09873546,  0.10001258,  0.10632841,  0.0994163 ,\n",
       "         0.1010685 ,  0.09955853,  0.10159509,  0.10265519,  0.09439002],\n",
       "       [ 0.09621265,  0.09843948,  0.1005809 ,  0.10660857,  0.0998337 ,\n",
       "         0.10034515,  0.09918182,  0.10191833,  0.10206787,  0.09481154],\n",
       "       [ 0.09626599,  0.09857668,  0.10042009,  0.10624516,  0.09970811,\n",
       "         0.10106869,  0.09874704,  0.10199224,  0.10209225,  0.09488377],\n",
       "       [ 0.09677298,  0.09807155,  0.10026605,  0.10643408,  0.09950761,\n",
       "         0.10057086,  0.09917555,  0.10183786,  0.10250606,  0.09485739],\n",
       "       [ 0.09663224,  0.09843636,  0.10031698,  0.10597171,  0.09958135,\n",
       "         0.10109976,  0.0988094 ,  0.10188088,  0.10212975,  0.09514158],\n",
       "       [ 0.09660555,  0.09881413,  0.10068267,  0.1063568 ,  0.09923195,\n",
       "         0.10041315,  0.09902675,  0.10179832,  0.10203423,  0.09503646],\n",
       "       [ 0.09649538,  0.09885694,  0.10063218,  0.10602167,  0.09967426,\n",
       "         0.10061773,  0.09903412,  0.10177503,  0.10248826,  0.09440445],\n",
       "       [ 0.09649002,  0.09835974,  0.10092072,  0.10645249,  0.09947528,\n",
       "         0.10065358,  0.09889775,  0.10181901,  0.10223151,  0.0946999 ],\n",
       "       [ 0.0963076 ,  0.09894211,  0.10046609,  0.10605027,  0.09953539,\n",
       "         0.1009455 ,  0.09934129,  0.10160852,  0.10209185,  0.09471136],\n",
       "       [ 0.09633035,  0.09869477,  0.10041873,  0.10644907,  0.09939756,\n",
       "         0.10089926,  0.09888049,  0.10162917,  0.10235918,  0.09494142],\n",
       "       [ 0.0963292 ,  0.09839838,  0.10073536,  0.10598817,  0.10013743,\n",
       "         0.1004674 ,  0.09905974,  0.10182286,  0.10260484,  0.09445662],\n",
       "       [ 0.09646699,  0.09877864,  0.10051423,  0.10625241,  0.09956363,\n",
       "         0.10060954,  0.09886484,  0.10160048,  0.10252585,  0.09482337],\n",
       "       [ 0.09663028,  0.09854492,  0.10028905,  0.10623261,  0.09962413,\n",
       "         0.10025695,  0.09932992,  0.10196498,  0.10195127,  0.09517589],\n",
       "       [ 0.09628053,  0.09851359,  0.10023841,  0.10599306,  0.09970814,\n",
       "         0.10088936,  0.09886041,  0.10203728,  0.10254096,  0.09493825],\n",
       "       [ 0.09628607,  0.0985301 ,  0.10048885,  0.10645503,  0.09956688,\n",
       "         0.10056496,  0.09921293,  0.10177158,  0.10196625,  0.09515735],\n",
       "       [ 0.09634938,  0.09847679,  0.10082617,  0.10656433,  0.09930166,\n",
       "         0.10085483,  0.09869019,  0.10166297,  0.10229132,  0.09498238],\n",
       "       [ 0.09637767,  0.09904156,  0.10028704,  0.10596573,  0.0997201 ,\n",
       "         0.10130636,  0.09910533,  0.10164643,  0.10190478,  0.09464501],\n",
       "       [ 0.09635611,  0.0986787 ,  0.10036898,  0.10630559,  0.09977721,\n",
       "         0.10077741,  0.09907931,  0.10159237,  0.10232824,  0.09473607],\n",
       "       [ 0.09640283,  0.09878067,  0.10068182,  0.10656159,  0.09949276,\n",
       "         0.10060918,  0.09908063,  0.10149474,  0.1021032 ,  0.09479258],\n",
       "       [ 0.09635112,  0.09883368,  0.10048091,  0.10601376,  0.09946366,\n",
       "         0.10068361,  0.09900434,  0.10174399,  0.10275674,  0.0946682 ],\n",
       "       [ 0.09630617,  0.09853031,  0.10087493,  0.106362  ,  0.09981856,\n",
       "         0.10020512,  0.09899807,  0.10179798,  0.10230314,  0.09480373],\n",
       "       [ 0.09628468,  0.0983843 ,  0.10076323,  0.10654455,  0.09966309,\n",
       "         0.10057525,  0.09885953,  0.10178146,  0.10230664,  0.09483728],\n",
       "       [ 0.09660009,  0.09839684,  0.10054187,  0.1060627 ,  0.0993197 ,\n",
       "         0.10076486,  0.09912791,  0.10185192,  0.10248016,  0.09485396],\n",
       "       [ 0.09674174,  0.09817201,  0.09997707,  0.10641057,  0.09966651,\n",
       "         0.10101401,  0.09890428,  0.10141658,  0.10251961,  0.09517762],\n",
       "       [ 0.09622865,  0.09852195,  0.10079677,  0.10631497,  0.09944425,\n",
       "         0.10045487,  0.09918821,  0.10186751,  0.10253579,  0.09464702],\n",
       "       [ 0.09649211,  0.09844462,  0.10073001,  0.10658561,  0.09959358,\n",
       "         0.10042348,  0.09897708,  0.10172163,  0.10213676,  0.09489513],\n",
       "       [ 0.09652482,  0.0986798 ,  0.10077047,  0.10615806,  0.0994321 ,\n",
       "         0.10093175,  0.09892278,  0.10156406,  0.10223491,  0.09478126],\n",
       "       [ 0.0963656 ,  0.09881427,  0.10041037,  0.10630376,  0.09951568,\n",
       "         0.10040657,  0.09917291,  0.10206599,  0.10215348,  0.09479138],\n",
       "       [ 0.09648829,  0.09875259,  0.10050019,  0.10609972,  0.09947661,\n",
       "         0.10102667,  0.0991057 ,  0.10155282,  0.10239138,  0.09460602],\n",
       "       [ 0.09619867,  0.09837961,  0.10019036,  0.10612816,  0.09979305,\n",
       "         0.10073895,  0.09928455,  0.10200891,  0.10239418,  0.09488358],\n",
       "       [ 0.09642324,  0.09862681,  0.10091708,  0.10627031,  0.09949689,\n",
       "         0.10051648,  0.09916666,  0.10182589,  0.10222545,  0.0945312 ],\n",
       "       [ 0.09628633,  0.09840678,  0.10043833,  0.10608935,  0.09953134,\n",
       "         0.1007764 ,  0.09914928,  0.10194137,  0.10262519,  0.09475565],\n",
       "       [ 0.09669801,  0.09867291,  0.1004611 ,  0.10614112,  0.09966688,\n",
       "         0.10066005,  0.09892543,  0.10171204,  0.10274463,  0.09431784],\n",
       "       [ 0.09648934,  0.09866885,  0.10053832,  0.10606092,  0.09961338,\n",
       "         0.10085572,  0.09901894,  0.10166857,  0.1021763 ,  0.09490967],\n",
       "       [ 0.09621265,  0.09856165,  0.10044919,  0.10634757,  0.09934844,\n",
       "         0.10066675,  0.0991516 ,  0.101709  ,  0.10268105,  0.0948721 ],\n",
       "       [ 0.09637697,  0.09846975,  0.10080892,  0.10619858,  0.09926895,\n",
       "         0.10054597,  0.09931268,  0.1021157 ,  0.10209299,  0.09480949],\n",
       "       [ 0.09637753,  0.09866195,  0.10051715,  0.10623075,  0.09961596,\n",
       "         0.10074561,  0.09874608,  0.10192066,  0.10236443,  0.09481987],\n",
       "       [ 0.09631584,  0.09859437,  0.10051469,  0.10631948,  0.09954773,\n",
       "         0.10040753,  0.09943023,  0.10168654,  0.10250511,  0.09467848],\n",
       "       [ 0.09613652,  0.09852312,  0.10022512,  0.1061309 ,  0.09976632,\n",
       "         0.10073235,  0.09914596,  0.10194034,  0.10219666,  0.09520269],\n",
       "       [ 0.09640448,  0.09828358,  0.1005201 ,  0.10658389,  0.09988702,\n",
       "         0.10045124,  0.09906002,  0.10183054,  0.10225826,  0.09472087],\n",
       "       [ 0.09611892,  0.09833847,  0.10100163,  0.10602041,  0.09969897,\n",
       "         0.1005861 ,  0.09917961,  0.10178775,  0.10239015,  0.094878  ],\n",
       "       [ 0.09635108,  0.09862377,  0.10081765,  0.10644024,  0.09957707,\n",
       "         0.10064631,  0.0985717 ,  0.10164191,  0.10209917,  0.0952311 ],\n",
       "       [ 0.09627547,  0.0984028 ,  0.10064462,  0.10630389,  0.09979048,\n",
       "         0.10090512,  0.09874539,  0.10182606,  0.10217356,  0.09493261],\n",
       "       [ 0.09637885,  0.0985369 ,  0.10077491,  0.10646452,  0.09949727,\n",
       "         0.10081193,  0.09902306,  0.10167067,  0.10219913,  0.09464277],\n",
       "       [ 0.09633563,  0.09866652,  0.10014644,  0.10638807,  0.0993632 ,\n",
       "         0.10073873,  0.09905788,  0.10197696,  0.10230875,  0.09501783],\n",
       "       [ 0.09646646,  0.09843276,  0.10030958,  0.10602861,  0.09965481,\n",
       "         0.1007779 ,  0.09928819,  0.10163404,  0.10243808,  0.09496956],\n",
       "       [ 0.09628436,  0.09826105,  0.10059347,  0.1064151 ,  0.0994266 ,\n",
       "         0.10083155,  0.09911202,  0.10163478,  0.10241918,  0.09502189],\n",
       "       [ 0.09655696,  0.09887667,  0.10036792,  0.10624158,  0.09983946,\n",
       "         0.10071233,  0.09923676,  0.1017277 ,  0.10210266,  0.09433794],\n",
       "       [ 0.09613343,  0.09856248,  0.10050413,  0.10619565,  0.09989907,\n",
       "         0.10058901,  0.09927966,  0.1016794 ,  0.10225685,  0.09490033],\n",
       "       [ 0.09666495,  0.09852853,  0.1006357 ,  0.10617252,  0.09978856,\n",
       "         0.1008267 ,  0.09881619,  0.10170544,  0.10203038,  0.09483102],\n",
       "       [ 0.09650636,  0.09820383,  0.10028766,  0.1066188 ,  0.09969746,\n",
       "         0.10077086,  0.09921111,  0.10182634,  0.10208067,  0.09479693],\n",
       "       [ 0.09621437,  0.0986122 ,  0.10047169,  0.10659315,  0.09943306,\n",
       "         0.1005242 ,  0.09879891,  0.10179342,  0.10232309,  0.09523591],\n",
       "       [ 0.09648793,  0.09862642,  0.1005897 ,  0.10617888,  0.09972961,\n",
       "         0.10071001,  0.09896318,  0.10170398,  0.10220449,  0.09480581],\n",
       "       [ 0.09638863,  0.09855577,  0.10059523,  0.10659494,  0.09967025,\n",
       "         0.10049333,  0.09916188,  0.1016168 ,  0.10228235,  0.09464082],\n",
       "       [ 0.09677321,  0.09848167,  0.10005722,  0.10625161,  0.09989044,\n",
       "         0.10070466,  0.09899286,  0.10191584,  0.10219026,  0.09474223],\n",
       "       [ 0.09631617,  0.0983308 ,  0.10060156,  0.1062401 ,  0.09953637,\n",
       "         0.10085652,  0.09916846,  0.10150613,  0.10235183,  0.09509204],\n",
       "       [ 0.09613628,  0.09854443,  0.10015691,  0.10632129,  0.09994251,\n",
       "         0.10056401,  0.09908534,  0.10178641,  0.10258654,  0.09487628],\n",
       "       [ 0.0964603 ,  0.09844357,  0.10082095,  0.10610306,  0.09936512,\n",
       "         0.10064505,  0.09926515,  0.10189936,  0.10194274,  0.0950547 ],\n",
       "       [ 0.09622286,  0.09826419,  0.10069852,  0.1063878 ,  0.09965816,\n",
       "         0.10059179,  0.09916881,  0.10173153,  0.10256068,  0.09471565],\n",
       "       [ 0.09651738,  0.09846084,  0.10048527,  0.10607169,  0.09950862,\n",
       "         0.1005772 ,  0.09931038,  0.10176741,  0.10265538,  0.09464582],\n",
       "       [ 0.09651673,  0.09849357,  0.10088443,  0.10641298,  0.09948639,\n",
       "         0.10038318,  0.09920006,  0.10184422,  0.10209089,  0.09468756],\n",
       "       [ 0.09667531,  0.09874435,  0.10049837,  0.10607206,  0.09932818,\n",
       "         0.10029888,  0.09931086,  0.10200083,  0.10229793,  0.09477324],\n",
       "       [ 0.09664365,  0.09840231,  0.10067094,  0.10630684,  0.09940842,\n",
       "         0.10054756,  0.09875886,  0.10174427,  0.10244899,  0.09506817],\n",
       "       [ 0.09655409,  0.09851237,  0.10032468,  0.10633219,  0.09948629,\n",
       "         0.10063653,  0.09917944,  0.10180471,  0.10208946,  0.09508024],\n",
       "       [ 0.09661433,  0.09799781,  0.10073929,  0.10647987,  0.09985843,\n",
       "         0.10069046,  0.09907702,  0.10181122,  0.10183068,  0.09490089],\n",
       "       [ 0.09643124,  0.09879417,  0.10043252,  0.10602886,  0.09957447,\n",
       "         0.10068628,  0.09885045,  0.10180514,  0.10249931,  0.09489755]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 매개변수 예측 처리 (순방향)\n",
    "x = np.random.rand(100, 784) # 더미 입력 데이터 100장 \n",
    "y = net.predict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# 기울기 계산\n",
    "x = np.random.rand(100, 784) # 더미 입력 데이터 (100장)\n",
    "t = np.random.rand(100, 10) # 더미 정답 데이터 (100장)\n",
    "\n",
    "grads = net.numerical_gradient(x,t)\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2 미니배치 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60,000 개의 훈련데이터에서 임의 100개 데이터 추림 (이미지, 정답 데이터) <br>\n",
    "확률 적으로 경사 하강법 수행으로 매개변수 갱신<br>\n",
    "갱신 횟수 10,000 번 (반복횟수) <br>\n",
    "갱신 시, 훈련데이터에 대한 손실함수계산, 배열에 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.0986333333333, 0.0958\n",
      "train acc, test acc | 0.788166666667, 0.7918\n",
      "train acc, test acc | 0.879333333333, 0.8826\n",
      "train acc, test acc | 0.897816666667, 0.9012\n",
      "train acc, test acc | 0.908966666667, 0.9118\n",
      "train acc, test acc | 0.914733333333, 0.916\n",
      "train acc, test acc | 0.92025, 0.9222\n",
      "train acc, test acc | 0.924416666667, 0.9248\n",
      "train acc, test acc | 0.928466666667, 0.9279\n",
      "train acc, test acc | 0.93235, 0.9316\n",
      "train acc, test acc | 0.934816666667, 0.935\n",
      "train acc, test acc | 0.937483333333, 0.9376\n",
      "train acc, test acc | 0.940166666667, 0.9389\n",
      "train acc, test acc | 0.941966666667, 0.9399\n",
      "train acc, test acc | 0.94375, 0.9419\n",
      "train acc, test acc | 0.945783333333, 0.9448\n",
      "train acc, test acc | 0.947166666667, 0.9456\n"
     ]
    }
   ],
   "source": [
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.3 시험 데이터로 평가 \n",
    "\n",
    "* epoch : 학습에서 훈련 데이터를 모두 소진했을 떄의 횟수 (10,000개를 100개의 미지배치로 학습한 경우 100회 1epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8MhUs5YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d576737828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 정확도 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
