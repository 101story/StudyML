{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 오차역전파법 Backpropagation : 오차를 역으로 전파 하는 방법 \n",
    "\n",
    "# 5.1 계산 그래프 \n",
    "\n",
    "계산 과정을 그래프로 나타낸 것 : Node(노드)와 Edge(에지) 로 표현\n",
    "\n",
    "## 5.1.1 계산 그래프로 풀다 \n",
    "\n",
    "[사과 한개] --100원--> [*2 개]  --200원-->  [**1.1 수수료] --220원-->  결제 \n",
    "\n",
    "1. 계산 그래프를 구성한다. \n",
    "2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다. \n",
    "\n",
    "* forward propagation : 계산을 왼쪽에서 오른쪽으로 진행\n",
    "* backward propagation : 계산을 오른쪽에서 왼쪽으로 진행 \n",
    "\n",
    "## 5.1.2 국소적 계산 \n",
    "국소적 : 자신과 직접 관계된 작은 범위 \n",
    "<img width=50% src ='calculateGraph.png' ></img>\n",
    "\n",
    "\n",
    "## 5.1.3 왜 계산 그래프?\n",
    "사과 값이 조금 올랐다. 지불금액이 얼마나 증가 하는가? <br>\n",
    "--> 역전파를 이용해 구할 수 있다. \n",
    "\n",
    "<img width=50% src ='apple.png' ></img>\n",
    "\n",
    "> 사과 가격에 대한 지불금액의 미분 <br>\n",
    "사과 가격이 1원 오르면 최종 금액은 2.2원 오르게 된다. <br>\n",
    "오른쪽에서 왼쪽으로 1 -> 1.1 > 2.2 순으로 미분 값을 전달\n",
    "\n",
    "# 5.2 Chain rule (연쇄법칙)\n",
    "\n",
    "## 5.2.1 계산 그래프의 역전파 \n",
    "\n",
    "신호 E 에 순전파 때의 계산의 미분을 곱한 후 다음 노드로 전달한다. <br>\n",
    "f(x) 가 $x^2$ 라면 미분한 값은 2x 가 된다. \n",
    "\n",
    "## 5.2.2 연쇄법칙이란?\n",
    "* 합성 함수 : 여러 함수로 구성된 함수 \n",
    "    * 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타냄 <br>\n",
    "\n",
    "$z = (x+y)^2$   <br>\n",
    "        =><br>\n",
    "        $z = t^2$<br>\n",
    "        $t = (x+y)$\n",
    "\n",
    "각각에 대해서 편미분 하면 x 에 대한 z 의 미분 값은 = 2(x+y) 가 된다. \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "# 5.3 역전파\n",
    "\n",
    "## 5.3.1 덧셈 노드의 역전파 \n",
    "\n",
    "$z=x+y$ 를 미분 \n",
    "\n",
    "덧셈 노드의 역전파는 1을 곱하기만 할뿐 입력되는 값을 그대로 다음 노드로 보냄\n",
    "\n",
    "입력신호를 다음 노드로 출력할 뿐 그대로 다음노드로 전달함 \n",
    "\n",
    "## 5.3.2 곱셈 노드의 역전파 \n",
    "\n",
    "$z=xy$ 를 미분\n",
    "\n",
    "곱셈 노드의 역전파는 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값' 을 곱해서 하류에 보냄\n",
    "\n",
    "<img  width=50% src ='calculateGraphBack.png'/>\n",
    "\n",
    "10, 5 x -> 50 이 됐으면 곱셈의 역전파에서는 \n",
    "\n",
    "1.3 이라는 임의의 값이 역으로 흘러오면 <br>\n",
    "1.3 x 5 = 6.5 다른 하나는 1.3 x 10 = 13 이 됨 (각각의 편미분 값)<br>\n",
    "6.5 는 10 이 온 방향으로 <br>\n",
    "13 은 5 가 온 방향으로 보내짐 \n",
    "\n",
    "\n",
    "결과의 오차를 반영하기 위해 각각의 노드에 하이퍼파라메터 값을 역으로 변경시켜 나간다. \n",
    "\n",
    "최종 오차에 weight 가 얼마나 영향을 주는지 계산 (오차에 주는 영향을 반영해서 최종오차가 줄어들도록 함)\n",
    "\n",
    "\n",
    "\n",
    "# 5.4 단순 계층 구현\n",
    "\n",
    "## 5.4.1 곱셈 계층 \n",
    "\n",
    "모든 계층이 forward() 와 backward() 라는 공통의 인터페이스를 갖도록 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    # 순전파에서 유지할 x,y 변수 초기화\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        return out\n",
    "    \n",
    "    # 상류에서 넘어온 미분 (dout) 에 순전파 떄의 값을 서로 바꿔 곱한 후 하류로 흘려 보냄 \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # x 와 y 를 바꾼다. \n",
    "        dy = dout * self.x\n",
    "        return dx, dy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "곱셈 계층 순전파 역전파 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 220\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dTax: 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1 \n",
    "# 순전파 출력에 대한 미분 값\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.2 덧셈 계층 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    # 상류에서 내려 오는 미분 값을 그대로 하류로 보냄 \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "덧셈 계층과 곱셈 계층으로 순전파 역전파 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dOrange: 3.3000000000000003\n",
      "dOrange_num: 165\n",
      "dTax: 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer() # 사과 수량 * 개수 \n",
    "mul_orange_layer = MulLayer()  # 오랜지 수량 * 개수 \n",
    "add_apple_orange_layer = AddLayer() # 사과가격 + 오랜지가격 \n",
    "mul_tax_layer = MulLayer() # 총 가격 * 수수료 \n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 활성화 함수 계층 구현하기 \n",
    "\n",
    "## 5.5.1 ReLU 계층\n",
    "\n",
    "y = x (x>0) <br>\n",
    "y = 0 (x<=0)\n",
    "\n",
    "x 에 대한 미분 \n",
    "\n",
    "$\\frac{dy}{dx}$ = 1 (x>0) <br>\n",
    "$\\frac{dy}{dx}$ = 0 (x<=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        sef.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask : True/False 로 구성된 넘파일 배열로 <br>\n",
    "    순전파의 입력인 x 의 원소값이 0 이하인 인덱스는 True 그외에는 False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "mask = (x<=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 Sigmoid 계층 \n",
    "\n",
    "$\\frac{1}{1+\\exp(-x)}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-1))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순전파의 출력을 인스턴스 변수 out 에 보관했다, 역전파 때 그 변수를 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6 Affine/Softmax 계층 구현\n",
    "\n",
    "## 5.6.1 Affine 계층 \n",
    "\n",
    "== Fully Connected\n",
    "\n",
    "Y = np.dot(X,W) +B 로 계산 \n",
    "\n",
    "X(2,) * W(2,3) = O(3,) 의 차원의 원소 개수를 일치 시켜야 함<br>\n",
    "(신경망의 순전파 때 수행하는 행렬의 내적 = 어파인 변환(Affine transformaton))\n",
    "\n",
    "$X \\cdot W = \\frac{\\partial L}{\\partial Y}$ 이기때문에\n",
    "\n",
    "$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y}\\cdot {W}^{t}$ <br>\n",
    "$\\frac{\\partial L}{\\partial W} =  {X}^{t}\\cdot\\frac{\\partial L}{\\partial Y}$\n",
    "\n",
    "역전파로 $\\frac{\\partial L}{\\partial X}$ 값과 $\\frac{\\partial L}{\\partial W}$ 값을 구함 \n",
    "\n",
    "편향을 더할때에도 각각의 N 개의 데이터 모두에게 적용\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "x_dot_w = np.array([[0,0,0], [10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "print(x_dot_w + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편향의 역전파는 N 개의 데이텅 대한 미분을 데이터 마다 더해서 구함.<br>\n",
    "np.sum() 에서 0 번째 축(데이터를 단위로 한 축)에 대해서 총합을 구함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1,2,3], [4,5,6]]) # 데이터 2개 \n",
    "dB = np.sum(dY, axis = 0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "softmax : 입력 값을 정규화하여 출력 \n",
    "\n",
    "신경망이 수행하는 작업 : 학습, 추론<br> \n",
    "    추론에서 주로 max 값을 사용<br>\n",
    "    학습에서 주로 softmax 값을 사용 \n",
    "    \n",
    "손실 함수인 교차 엔트로피 오차도 포함 = softmax-with-loss\n",
    "\n",
    "- 3 클래스 분류를 가정하고 이전 계층의 입력(점수)를 받아 softmax 계층은 (a1, a2, a3) 를 정규화 하여 (y1, y2, y3) 를 출력 \n",
    "- Cross Entropy Error 계층은 softmax 출력과 정답을 받고 손실을 L 을 출력\n",
    "- softmax 계층의 역전파는 (y1-t1, y2-t2, y3-t3) 오차가 앞계층에 전해짐 \n",
    "\n",
    "예) 정답이 (0, 1, 0)  일때 softmax 에서 (0.3, 0.2, 0.5) 를 출력 했을 때 <br>\n",
    "정답인 1번 인덱스 는 확률이 0.2(20%) 라서 softmaxk 의 역전파는 (0.3, -0.8, 0.5) 가 되어 커다란 오차를 전파 하게 됨 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실\n",
    "        self.y = None #softmax 출력\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(t)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.7 오차역전파법 구현하기\n",
    "\n",
    "\n",
    "## 5.7.1 신경망 학습의 전체 그림 \n",
    "\n",
    "* 전체 \n",
    "    - 가중치와 편향을 훈련 데이터에 적응하도록 저정하는 과정 (학습) 을 진행\n",
    "* 1단계 - 미니배치\n",
    "    - 훈련 데이터 중 일부를 가져와 손실 함수 값을 줄이는 것이 목표\n",
    "* 2단계 - 기울기 산출 \n",
    "    - 각 가중치 매개변수의 기울기를 구해 손실 함수의 값을 가장 작게 하는 방향을 제시 \n",
    "* 3단계 - 매개변수 갱신\n",
    "    - 가중치 매개변수를 기울기 방향으로 조금 갱신\n",
    "* 4단계 - 반복\n",
    "\n",
    "기울기 산출 에서 오차역전파법 사용 \n",
    "\n",
    "수치 미분은 구현하기 쉽지만 계산이 올래 걸리나 오차역전파법은 기울기를 효율적이고 빠르게 구할 수 있음 \n",
    "\n",
    "## 5.7.2 오차역전파법을 이용한 신경망 구현\n",
    "\n",
    "lastLayer : 신경망의 마지막 계층 Softmax With Loss 계층 이용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict() # 순서가 있는 dictionary\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.3 오차역전파법으로 구한 기울기 검증\n",
    "\n",
    "오차역전파의 복잡한 구현에 실수를 찾기 위해<br> \n",
    "수치미분을 사용하여 정확히 구현했는지 확인\n",
    "\n",
    "gradient check 기울기 확인 (두 기울기 구하는 방법을 비교) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:1.41856751671e-06\n",
      "b1:1.52452317963e-05\n",
      "W2:6.8434419004e-13\n",
      "b2:1.19904090823e-10\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오차역전파법을 사용한 학습 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.113083333333 0.1139\n",
      "0.90485 0.9051\n",
      "0.920133333333 0.9212\n",
      "0.93645 0.9371\n",
      "0.94565 0.9451\n",
      "0.9521 0.9494\n",
      "0.956683333333 0.9541\n",
      "0.961383333333 0.9586\n",
      "0.96565 0.962\n",
      "0.96645 0.9631\n",
      "0.970783333333 0.9662\n",
      "0.972383333333 0.9666\n",
      "0.974366666667 0.9687\n",
      "0.976816666667 0.9686\n",
      "0.976183333333 0.9688\n",
      "0.978216666667 0.9712\n",
      "0.979233333333 0.9728\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
