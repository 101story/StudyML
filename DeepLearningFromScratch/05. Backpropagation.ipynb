{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 오차역전파법 Backpropagation : 오차를 역으로 전파 하는 방법 \n",
    "\n",
    "# 5.1 계산 그래프 \n",
    "\n",
    "계산 과정을 그래프로 나타낸 것 : Node(노드)와 Edge(에지) 로 표현\n",
    "\n",
    "## 5.1.1 계산 그래프로 풀다 \n",
    "\n",
    "[사과 한개] --100원--> [*2 개]  --200원-->  [**1.1 수수료] --220원-->  결제 \n",
    "\n",
    "1. 계산 그래프를 구성한다. \n",
    "2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다. \n",
    "\n",
    "* forward propagation : 계산을 왼쪽에서 오른쪽으로 진행\n",
    "* backward propagation : 계산을 오른쪽에서 왼쪽으로 진행 \n",
    "\n",
    "## 5.1.2 국소적 계산 \n",
    "국소적 : 자신과 직접 관계된 작은 범위 \n",
    "<img width=50% src ='calculateGraph.png' ></img>\n",
    "\n",
    "\n",
    "## 5.1.3 왜 계산 그래프?\n",
    "사과 값이 조금 올랐다. 지불금액이 얼마나 증가 하는가? <br>\n",
    "--> 역전파를 이용해 구할 수 있다. \n",
    "\n",
    "> 사과 가격에 대한 지불금액의 미분 <br>\n",
    "사과 가격이 1원 오르면 최종 금액은 2.2원 오르게 된다. <br>\n",
    "오른쪽에서 왼쪽으로 1 -> 1.1 > 2.2 순으로 미분 값을 전달\n",
    "\n",
    "# 5.2 Chain rule (연쇄법칙)\n",
    "\n",
    "## 5.2.1 계산 그래프의 역전파 \n",
    "\n",
    "신호 E 에 순전파 때의 계산의 미분을 곱한 후 다음 노드로 전달한다. <br>\n",
    "f(x) 가 $x^2$ 라면 미분한 값은 2x 가 된다. \n",
    "\n",
    "## 5.2.2 연쇄법칙이란?\n",
    "* 합성 함수 : 여러 함수로 구성된 함수 \n",
    "    * 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타냄 <br>\n",
    "\n",
    "$z = (x+y)^2$   <br>\n",
    "        =><br>\n",
    "        $z = t^2$<br>\n",
    "        $t = (x+y)$\n",
    "\n",
    "각각에 대해서 편미분 하면 x 에 대한 z 의 미분 값은 = 2(x+y) 가 된다. \n",
    "\n",
    "# 5.3 역전파\n",
    "\n",
    "## 5.3.1 덧셈 노드의 역전파 \n",
    "\n",
    "$z=x+y$ 를 미분 \n",
    "\n",
    "덧셈 노드의 역전파는 1을 곱하기만 할뿐 입력되는 값을 그대로 다음 노드로 보냄\n",
    "\n",
    "입력신호를 다음 노드로 출력할 뿐 그대로 다음노드로 전달함 \n",
    "\n",
    "## 5.3.2 곱셈 노드의 역전파 \n",
    "\n",
    "$z=xy$ 를 미분\n",
    "\n",
    "곱셈 노드의 역전파는 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값' 을 곱해서 하류에 보냄\n",
    "\n",
    "<img  width=50% src ='calculateGraphBack.png'/>\n",
    "\n",
    "10, 5 x -> 50 이 됐으면 곱셈의 역전파에서는 \n",
    "\n",
    "1.3 이라는 임의의 값이 역으로 흘러오면 <br>\n",
    "1.3 x 5 = 6.5 다른 하나는 1.3 x 10 = 13 이 됨 (각각의 편미분 값)<br>\n",
    "6.5 는 10 이 온 방향으로 <br>\n",
    "13 은 5 가 온 방향으로 보내짐 \n",
    "\n",
    "\n",
    "결과의 오차를 반영하기 위해 각각의 노드에 하이퍼파라메터 값을 역으로 변경시켜 나간다. \n",
    "\n",
    "\n",
    "# 5.4 단순 계층 구현\n",
    "\n",
    "## 5.4.1 곱셈 계층 \n",
    "\n",
    "모든 계층이 forward() 와 backward() 라는 공통의 인터페이스를 갖도록 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    # 순전파에서 유지할 x,y 변수 초기화\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        return out\n",
    "    \n",
    "    # 상류에서 넘어온 미분 (dout) 에 순전파 떄의 값을 서로 바꿔 곱한 후 하류로 흘려 보냄 \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # x 와 y 를 바꾼다. \n",
    "        dy = dout * self.x\n",
    "        return dx, dy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "곱셈 계층 순전파 역전파 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 220\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dTax: 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1 \n",
    "# 순전파 출력에 대한 미분 값\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.2 덧셈 계층 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    # 상류에서 내려 오는 미분 값을 그대로 하류로 보냄 \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "덧셈 계층과 곱셈 계층으로 순전파 역전파 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dOrange: 3.3000000000000003\n",
      "dOrange_num: 165\n",
      "dTax: 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer() # 사과 수량 * 개수 \n",
    "mul_orange_layer = MulLayer()  # 오랜지 수량 * 개수 \n",
    "add_apple_orange_layer = AddLayer() # 사과가격 + 오랜지가격 \n",
    "mul_tax_layer = MulLayer() # 총 가격 * 수수료 \n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 활성화 함수 계층 구현하기 \n",
    "\n",
    "## 5.5.1 ReLU 계층\n",
    "\n",
    "y = x (x>0) <br>\n",
    "y = 0 (x<=0)\n",
    "\n",
    "x 에 대한 미분 \n",
    "\n",
    "$\\frac{dy}{dx}$ = 1 (x>0) <br>\n",
    "$\\frac{dy}{dx}$ = 0 (x<=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        sef.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask : True/False 로 구성된 넘파일 배열로 <br>\n",
    "    순전파의 입력인 x 의 원소값이 0 이하인 인덱스는 True 그외에는 False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "mask = (x<=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 Sigmoid 계층 \n",
    "\n",
    "$\\frac{1}{1+\\exp(-x)}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-1))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순전파의 출력을 인스턴스 변수 out 에 보관했다, 역전파 때 그 변수를 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6 Affine/Softmax 계층 구현\n",
    "\n",
    "## 5.6.1 Affine 계층\n",
    "\n",
    "Y = np.dot(X,W) +B 로 계산 \n",
    "\n",
    "X(2,) * W(2,3) = O(3,) 의 차원의 원소 개수를 일치 시켜야 함<br>\n",
    "(신경망의 순전파 때 수행하는 행렬의 내적 = 어파인 변환(Affine transformaton))\n",
    "\n",
    "$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y}\\cdot {W}^{t}$ <br>\n",
    "$\\frac{\\partial L}{\\partial W} =  {X}^{t}\\cdot\\frac{\\partial L}{\\partial Y}$\n",
    "\n",
    "역전파로 $\\frac{\\partial L}{\\partial X}$ 값과 $\\frac{\\partial L}{\\partial W}$ 값을 구함 \n",
    "\n",
    "편향을 더할때에도 각각의 N 개의 데이터 모두에게 적용\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "x_dot_w = np.array([[0,0,0], [10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "print(x_dot_w + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편향의 역전파는 N 개의 데이텅 대한 미분을 데이터 마다 더해서 구함.<br>\n",
    "np.sum() 에서 0 번째 축(데이터를 단위로 한 축)에 대해서 총합을 구함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1,2,3], [4,5,6]]) # 데이터 2개 \n",
    "dB = np.sum(dY, axis = 0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "softmax : 입력 값을 정규화하여 출력 \n",
    "\n",
    "신경망이 수행하는 작업 : 학습, 추론<br> \n",
    "    추론에서 주로 max 값을 사용<br>\n",
    "    학습에서 주로 softmax 값을 사용 \n",
    "    \n",
    "손실 함수인 교차 엔트로피 오차도 포함 = softmax-with-loss\n",
    "\n",
    "- 3 클래스 분류를 가정하고 이전 계층의 입력(점수)를 받아 softmax 계층은 (a1, a2, a3) 를 정규화 하여 (y1, y2, y3) 를 출력 \n",
    "- Cross Entropy Error 계층은 softmax 출력과 정답을 받고 손실을 L 을 출력\n",
    "- softmax 계층의 역전파는 (y1-t1, y2-t2, y3-t3) 오차가 앞계층에 전해짐 \n",
    "\n",
    "예) 정답이 (0, 1, 0)  일때 softmax 에서 (0.3, 0.2, 0.5) 를 출력 했을 때 <br>\n",
    "정답인 1번 인덱스 는 확률이 0.2(20%) 라서 softmaxk 의 역전파는 (0.3, -0.8, 0.5) 가 되어 커다란 오차를 전파 하게 됨 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실\n",
    "        self.y = None #softmax 출력\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(t)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.7 오차역전파법 구현하기\n",
    "\n",
    "\n",
    "## 5.7.1 신경망 학습의 전체 그림 \n",
    "\n",
    "* 전체 \n",
    "    - 가중치와 편향을 훈련 데이터에 적응하도록 저정하는 과정 (학습) 을 진행\n",
    "* 1단계 - 미니배치\n",
    "    - 훈련 데이터 중 일부를 가져와 손실 함수 값을 줄이는 것이 목표\n",
    "* 2단계 - 기울기 산출 \n",
    "    - 각 가중치 매개변수의 기울기를 구해 손실 함수의 값을 가장 작게 하는 방향을 제시 \n",
    "* 3단계 - 매개변수 갱신\n",
    "    - 가중치 매개변수를 기울기 방향으로 조금 갱신\n",
    "* 4단계 - 반복\n",
    "\n",
    "기울기 산출 에서 오차역전파법 사용 \n",
    "\n",
    "수치 미분은 구현하기 쉽지만 계산이 올래 걸리나 오차역전파법은 기울기를 효율적이고 빠르게 구할 수 있음 \n",
    "\n",
    "## 5.7.2 오차역전파법을 이용한 신경망 구현\n",
    "\n",
    "lastLayer : 신경망의 마지막 계층 Softmax With Loss 계층 이용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict() # 순서가 있는 dictionary\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.3 오차역전파법으로 구한 기울기 검증\n",
    "\n",
    "오차역전파의 복잡한 구현에 실수를 찾기 위해<br> \n",
    "수치미분을 사용하여 정확히 구현했는지 확인\n",
    "\n",
    "gradient check 기울기 확인 (두 기울기 구하는 방법을 비교) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:1.41856751671e-06\n",
      "b1:1.52452317963e-05\n",
      "W2:6.8434419004e-13\n",
      "b2:1.19904090823e-10\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오차역전파법을 사용한 학습 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.113083333333 0.1139\n",
      "0.90485 0.9051\n",
      "0.920133333333 0.9212\n",
      "0.93645 0.9371\n",
      "0.94565 0.9451\n",
      "0.9521 0.9494\n",
      "0.956683333333 0.9541\n",
      "0.961383333333 0.9586\n",
      "0.96565 0.962\n",
      "0.96645 0.9631\n",
      "0.970783333333 0.9662\n",
      "0.972383333333 0.9666\n",
      "0.974366666667 0.9687\n",
      "0.976816666667 0.9686\n",
      "0.976183333333 0.9688\n",
      "0.978216666667 0.9712\n",
      "0.979233333333 0.9728\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
