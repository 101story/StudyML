{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손실함수 : 결과값을 가장 작게 만드는 함수 가중치 매개변수를 찾을떄 사용 \n",
    "\n",
    "# 4.1 데이터 학습\n",
    "\n",
    "규칙을 ‘사람’이 만드는 방식 ‘기계’가 데이터로부터 규칙(특징)을 발견해 내는 방식으로 패러다임 전환\n",
    "\n",
    "## 4.1.1 데이터 주도학습\n",
    "특징을 추출하고 벡터가 가지는 가중치를 부여\n",
    "\n",
    "종단간 기계학습(end-to-end machine learning) : 처음부터 끝까지 입력에서 결과를 얻는다. \n",
    "\n",
    "## 4.1.2 훈련데이터와 시험데이터 \n",
    "범용능력을 평가하기 위해서 train, test 데이터 분리 \n",
    "\n",
    "# 4.2 손실함수 \n",
    "loss function : 최적의 매겨변수 값을 탐색 \n",
    "\n",
    "평균 제곱 오차, 교차 엔트로피 오차 사용\n",
    "\n",
    "\n",
    "## 4.2.1 평균 제곱 오차 \n",
    "\n",
    "mean squared error MSE : 각 원소의 출력(추정) 값과 정답 레이블의 차를 제곱한 후 그 총합 \n",
    "\n",
    "one hot encoding: 한원소만 1로 하고 그외는 0으로 나타내는 표기범\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.095\n",
      "0.415\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.2,0.7,0.2,0.1,0,0,0,0,0]\n",
    "\n",
    "print(mean_squared_error(np.array(y), np.array(t)))\n",
    "\n",
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.2,0.7,0.2,0.1,0,0.8,0,0,0]\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오답에 가까울 수록 결과가 크다.\n",
    "\n",
    "## 4.2.2 교차 엔트로피 \n",
    "cross entroypy CEE : 정답에 해당하는 인덱스만 1 log 를 곱해도 나머진 모두 0이므로 결과에 영향을 주지 않는다. 정답일떄의 출력이 전체 값을 정함\n",
    "\n",
    "정답에 해당하는 출력이 커질수록 0에 다가가다가 출력이 1일떄 0이 됨, 반대로 정답일때의 출력이 작아질수록 오차는 커짐\n",
    "\n",
    "- 정보엔트로피 : 현재 사건에 대한 정보량 (정보량이 많다. = 선택사항이 많다. = 불확실 하다. = 복잡성, 불확정성)\n",
    "- 교차엔트로피 (자연로그) : 두 확률 간의 엔트로피 (두확률간에 복잡도가 얼마나 높은지, 오차가 얼마나 많은지)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "# np.log(0) 이면 -inf 가 되어 계산을 진행할수 없음 0이 되지 않도록 해줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356674801082\n",
      "0.69314698056\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.2,0.7,0.2,0.1,0,0,0.1,0.2,0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "y = [0.1,0.2,0.5,0.2,0.1,0,0.8,0,0,0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차가 작은 처번째 추정이 정답일 가능성이 높다고 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 미니배치 학습\n",
    "\n",
    "모두에 대한 손실 함수의 합을 구함 평균 손실 함수 \n",
    "\n",
    "일부만 골라서 학습 미니배치 학습 \n",
    "\n",
    "test 한건당 평균 오차 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 \n",
    "import numpy as np\n",
    "import pickle\n",
    "from tmp.dataset.mnist import load_mnist\n",
    "from common.functions import sigmoid, softmax\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize =True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터에서 무작위로 10장만 뺌 np.random.choice 이용\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "x_batch[0:10]\n",
    "t_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35983, 53085, 12898, 30921,  9693,  2657, 45665,  6457,   993, 43959])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4 (배치용) 교차 엔트로피 오차 구현하기 \n",
    "\n",
    "데이터 하나당 교차 엔트로피 오차를 구하는 경우 reshape 함수로 데이터 shape 을 바꿈\n",
    "배치크기로 나눠 정규화를 하고 이미지 1장단 평균의 교차 엔트로피 오차를 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21815200513228775"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmp.ch03 import neuralnet_mnist\n",
    "network = neuralnet_mnist.init_network()\n",
    "y = neuralnet_mnist.predict(network, x_batch)\n",
    "cross_entropy_error(y, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot 이 아닌 숫자 레이블일 경우 \n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t]))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([  5.24714182e-04,   1.59779377e-03,   7.48113889e-05,\n",
       "         6.39280770e-04,   9.94004071e-01,   4.35634895e-04,\n",
       "         1.87226001e-03,   9.97999232e-05,   7.29477335e-07,\n",
       "         2.27661990e-02], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size\n",
    "t\n",
    "np.arange(batch_size)\n",
    "y[np.arange(batch_size), t]\n",
    "# np.arange(batch_size) row / t column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* np.log(y[arange(batch_size), t])  설명<br>\n",
    "0 부터 batch_size-1 까지 배열을 생성 <br>\n",
    "각 데이터 정답 레이블에 해당하는 신경망의 출력을 추출 <br>\n",
    "예) [y[0,2],y[1,7],y[3,9],y[4,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232.84817245588638"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y, [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 두가지를 한번에\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원핫벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t]))/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.5 왜 손실 함수를 설정하는가?\n",
    "\n",
    "손실함수의 미분 : 가중치 매개변수의 값을 아주 조금 변화시켰을때, 손실함수가 어떻게 변하냐 <br>\n",
    "음수면 가중치 매개변수를 양으로 양이면 음수로 변화 시킴 <br>\n",
    "미분 값이 0 이면 멈춤 <br>\n",
    "\n",
    "계단 함수의 경우 미분한 값이 대부분 0 시그모이드는 연속적으로 변함 <br>\n",
    "기울기가 0이 되지 않아 올바르게 학습할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 수치 미분\n",
    "\n",
    "경사법 기울기(경사) 값을 기준으로 나아가야 하는 방향 \n",
    "\n",
    "## 4.3.1 미분\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=10e-50\n",
    "    return (f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제점1) 반올림 오차 소숫점 8자리 이하는 생략되어 0이 되버림 (float32 0이되버림) > 1e-4 사용 <br>\n",
    "문제점2) 차분 x+h 와 x 사이의 기울기에 해당됨 x 의 진정한 접선이 아니다. \n",
    "\n",
    "x+h 와 x-h 를 사용하여 x 를 중심으로 전후 차분을 계산 : **중심차분(중앙차분) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=1e-4 #0.001\n",
    "    return (f(x+h)-f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 수치미분의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tangent_line(f, x):\n",
    "    a = numerical_diff(f,x)\n",
    "    b = f(x)-a*x\n",
    "    return lambda t:a*t+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25, -0.23, -0.21, -0.19, -0.17, -0.15, -0.13, -0.11, -0.09,\n",
       "       -0.07, -0.05, -0.03, -0.01,  0.01,  0.03,  0.05,  0.07,  0.09,\n",
       "        0.11,  0.13,  0.15,  0.17,  0.19,  0.21,  0.23,  0.25,  0.27,\n",
       "        0.29,  0.31,  0.33,  0.35,  0.37,  0.39,  0.41,  0.43,  0.45,\n",
       "        0.47,  0.49,  0.51,  0.53,  0.55,  0.57,  0.59,  0.61,  0.63,\n",
       "        0.65,  0.67,  0.69,  0.71,  0.73,  0.75,  0.77,  0.79,  0.81,\n",
       "        0.83,  0.85,  0.87,  0.89,  0.91,  0.93,  0.95,  0.97,  0.99,\n",
       "        1.01,  1.03,  1.05,  1.07,  1.09,  1.11,  1.13,  1.15,  1.17,\n",
       "        1.19,  1.21,  1.23,  1.25,  1.27,  1.29,  1.31,  1.33,  1.35,\n",
       "        1.37,  1.39,  1.41,  1.43,  1.45,  1.47,  1.49,  1.51,  1.53,\n",
       "        1.55,  1.57,  1.59,  1.61,  1.63,  1.65,  1.67,  1.69,  1.71,\n",
       "        1.73,  1.75,  1.77,  1.79,  1.81,  1.83,  1.85,  1.87,  1.89,\n",
       "        1.91,  1.93,  1.95,  1.97,  1.99,  2.01,  2.03,  2.05,  2.07,\n",
       "        2.09,  2.11,  2.13,  2.15,  2.17,  2.19,  2.21,  2.23,  2.25,\n",
       "        2.27,  2.29,  2.31,  2.33,  2.35,  2.37,  2.39,  2.41,  2.43,\n",
       "        2.45,  2.47,  2.49,  2.51,  2.53,  2.55,  2.57,  2.59,  2.61,\n",
       "        2.63,  2.65,  2.67,  2.69,  2.71,  2.73,  2.75,  2.77,  2.79,\n",
       "        2.81,  2.83,  2.85,  2.87,  2.89,  2.91,  2.93,  2.95,  2.97,\n",
       "        2.99,  3.01,  3.03,  3.05,  3.07,  3.09,  3.11,  3.13,  3.15,\n",
       "        3.17,  3.19,  3.21,  3.23,  3.25,  3.27,  3.29,  3.31,  3.33,\n",
       "        3.35,  3.37,  3.39,  3.41,  3.43,  3.45,  3.47,  3.49,  3.51,\n",
       "        3.53,  3.55,  3.57,  3.59,  3.61,  3.63,  3.65,  3.67,  3.69,\n",
       "        3.71,  3.73])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x82430f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x82515f8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x82b14a8>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x82b1ef0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ec8c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "\n",
    "tangent = tangent_line(function_1, 5)\n",
    "tl = tangent(x)\n",
    "tl\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,tl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5일떄 변화량\n",
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10일떄 변화량\n",
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3 편미분\n",
    "partial differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 #np.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0=3 x1=4 x0 에 대한 편미분\n",
    "def function_tmp1(x):\n",
    "    return x*x + 4.0**2.0\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0=3 x1=4 x1 에 대한 편미분\n",
    "def function_tmp1(x):\n",
    "    return  3.0**2.0+x*x\n",
    "numerical_diff(function_tmp1, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 기울기 (gradient)\n",
    "\n",
    "모든 변수의 평미분을 벡터로 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모든 변수의 편미분 (기울기) 값 구하기 \n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # k 와 shape 이 같은 0배열을 생성 \n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  8.]\n",
      "[ 0.  4.]\n",
      "[ 6.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "기울기가 가리키는 쪽: 각 장소에서 함수의 출력 값을 가장 줄이는 방향 (최소값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.4.1 경사 하강법\n",
    "최적의 매개변수 찾기 \n",
    "\n",
    "* 극소값, 최소값, 안장점 (sddle point) 이 되는 장소에서는 기울기가 0 \n",
    "    * 복잡하고 찌그러진 모양의 함수라면 평평한곳으로 파고들면 고원(plateau) 라는 학습이 진행되지 않는 정체기에 빠질수 있다.\n",
    "    \n",
    "* 경사법 : 기울기를 구하고 -> 기울어진 방향으로 일정 거리만큼 이동 반복 \n",
    "    * 학습률 : 매개변수 값을 얼마나 갱신할지 갱신하는 양 \n",
    "    \n",
    "현재 상태 = 이전 상태 - 경사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.11110793e-10,   8.14814391e-10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.58983747e+13,  -1.29524862e+12])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 경우 \n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "큰 값으로 발산해버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 작은 경우 \n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "거의 갱신되지 않은 채로 끝남 \n",
    "\n",
    "* 하이퍼파라미터 : 학습률 같은 매개변수<br> \n",
    "    가중치, 편향 같은 매개변수는 훈련 데이터와 학습 알고리즘에 의해 자동으로 획득<br> \n",
    "    하이퍼파라미터는 직접 설정해줘야함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list, numpy 의 array 차이로 [:] 복제가 x = init_x  아닌 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2 신경망의 기울기 \n",
    "\n",
    "가중치 매개변수에 관한 손실함수의 기울기 <br>\n",
    "W 가 조금 변경했을 떄 손실함수 L 이 얼마나 변화하느냐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 신경망 기울기 구하기 \n",
    "# x 입력 데이터, t 정답 레이블 \n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    # 2 * 3 가중치 매개변수\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    # 예측함수\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    #손실함수\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48310344  1.90941639 -0.10516501]\n",
      " [ 0.68532831  0.45945631 -0.85851639]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90665754  1.55916052 -0.83576375]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8723504888714944"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0,0,1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19383368  0.37222683 -0.56606051]\n",
      " [ 0.29075052  0.55834025 -0.84909077]]\n"
     ]
    }
   ],
   "source": [
    "# numerical_gradient 내부에서 f(x) 를 실행 -> 일관성을 위해서 f(W) 를 정의\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "* 전제 <br>\n",
    "    가중치와 편향을 훈련데이터에 적응하는 과정 : 학습\n",
    "* 1단계-미니배치 <br>\n",
    "    훈련데이터중에 일부 선별 \n",
    "* 2단계-기울기 산출 <br>\n",
    "    손실함수 값을 줄이기 위해 가중치 매개변수의 기울기 구함 손실 함수를 작게하는 방향을 구함\n",
    "* 3단계-매개변수 갱신 <br>\n",
    "    가중치 매개변수를 기울기 방향으로 갱신 \n",
    "* 4단계-반복\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **SGD** 확률적 경사 하강법 (stochastic gradient descent) <br>\n",
    "    확률 적으로 무작위로 골라낸 데이터 로 하는 경사하강법\n",
    "    \n",
    "## 4.5.1 2층 신경망 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    # 입력충의 뉴런수, 은닉층의 뉴런수, 출력층의 뉴런수\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        # 1층의 가중치 \n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        # 2층의 가중치 \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    # 예측 : 이미지 데이터\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # 손실함수값 x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    # 예측 정확도\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # 가중치 매개변수 기울기 x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        # 1층 가중치의 기울기\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        # 2층 가중치의 기울기\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # numerical_gradient 개선판 \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# 신경망에 필요한 매개변수 저장\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09661801,  0.09823199,  0.10047899,  0.10625221,  0.09984452,\n",
       "         0.10113933,  0.09909032,  0.10163015,  0.10220259,  0.09451189],\n",
       "       [ 0.09702084,  0.09876809,  0.10043464,  0.10595654,  0.09913132,\n",
       "         0.1010357 ,  0.09889177,  0.10200017,  0.10205234,  0.0947086 ],\n",
       "       [ 0.09642484,  0.09856958,  0.10062087,  0.10651354,  0.09919013,\n",
       "         0.10059105,  0.09887902,  0.10198866,  0.10233317,  0.09488912],\n",
       "       [ 0.09639613,  0.09853308,  0.10061959,  0.1060782 ,  0.09941427,\n",
       "         0.10076057,  0.09907962,  0.10190778,  0.10207527,  0.09513549],\n",
       "       [ 0.096157  ,  0.09864748,  0.10077103,  0.10624788,  0.09990382,\n",
       "         0.10077861,  0.09904095,  0.10149305,  0.10207702,  0.09488317],\n",
       "       [ 0.0965733 ,  0.09866743,  0.10049966,  0.10631646,  0.09950652,\n",
       "         0.100639  ,  0.09905209,  0.10174237,  0.10210188,  0.0949013 ],\n",
       "       [ 0.0964438 ,  0.09849396,  0.10067279,  0.1059192 ,  0.09964911,\n",
       "         0.10096882,  0.09899076,  0.10175873,  0.10223044,  0.09487239],\n",
       "       [ 0.09657587,  0.09866106,  0.10039193,  0.10638895,  0.09922   ,\n",
       "         0.1005554 ,  0.09919081,  0.10173015,  0.102276  ,  0.09500982],\n",
       "       [ 0.09626664,  0.0986377 ,  0.10033836,  0.10638344,  0.09934022,\n",
       "         0.10069705,  0.09888089,  0.10200228,  0.10245828,  0.09499512],\n",
       "       [ 0.09645758,  0.09878853,  0.10018838,  0.10609402,  0.09948069,\n",
       "         0.10090117,  0.09893227,  0.10202529,  0.10230728,  0.09482479],\n",
       "       [ 0.09625684,  0.09836252,  0.10055615,  0.10627081,  0.09987768,\n",
       "         0.10015035,  0.0992955 ,  0.10211087,  0.10219164,  0.09492766],\n",
       "       [ 0.09645473,  0.09828676,  0.10043809,  0.10627269,  0.09968265,\n",
       "         0.1006638 ,  0.09898532,  0.10202748,  0.10220489,  0.09498359],\n",
       "       [ 0.09621455,  0.09832933,  0.10041787,  0.10653199,  0.09962813,\n",
       "         0.10094151,  0.09910892,  0.10180584,  0.10222619,  0.09479567],\n",
       "       [ 0.0965126 ,  0.09871691,  0.10024461,  0.10626239,  0.09952332,\n",
       "         0.10104197,  0.09896554,  0.10162985,  0.10228587,  0.09481694],\n",
       "       [ 0.09661367,  0.09873086,  0.1003321 ,  0.10635276,  0.09958106,\n",
       "         0.1009093 ,  0.09921093,  0.10164345,  0.10199523,  0.09463064],\n",
       "       [ 0.09639374,  0.09888914,  0.10040327,  0.10646484,  0.09919105,\n",
       "         0.10091612,  0.09919126,  0.10187916,  0.10222371,  0.09444772],\n",
       "       [ 0.09620807,  0.09872265,  0.10019579,  0.10651604,  0.09942062,\n",
       "         0.10075857,  0.09907399,  0.10174445,  0.10219115,  0.09516867],\n",
       "       [ 0.09648968,  0.09827297,  0.10056828,  0.10628599,  0.09967929,\n",
       "         0.10076894,  0.09918457,  0.10172367,  0.1021824 ,  0.09484421],\n",
       "       [ 0.0964806 ,  0.09855646,  0.10003011,  0.10634274,  0.09984345,\n",
       "         0.1008231 ,  0.0988445 ,  0.10172835,  0.10216154,  0.09518916],\n",
       "       [ 0.09633662,  0.09854292,  0.10041067,  0.10614175,  0.09964619,\n",
       "         0.10066264,  0.09947008,  0.10168374,  0.10252652,  0.09457887],\n",
       "       [ 0.09636242,  0.09881579,  0.10032797,  0.10624864,  0.09954995,\n",
       "         0.10080709,  0.09900423,  0.10175114,  0.10248303,  0.09464977],\n",
       "       [ 0.09660505,  0.09838743,  0.10035023,  0.10597219,  0.0994768 ,\n",
       "         0.10079491,  0.09977302,  0.10190479,  0.10214646,  0.09458913],\n",
       "       [ 0.09620534,  0.09895214,  0.10060463,  0.10639854,  0.09919797,\n",
       "         0.10101504,  0.09882229,  0.10162335,  0.10216894,  0.09501174],\n",
       "       [ 0.09652111,  0.09830338,  0.10049695,  0.10616254,  0.09981816,\n",
       "         0.10081002,  0.09900986,  0.10186649,  0.10234553,  0.09466596],\n",
       "       [ 0.09644955,  0.09884078,  0.10029594,  0.10602393,  0.09990746,\n",
       "         0.1005583 ,  0.09905926,  0.10178403,  0.1022857 ,  0.09479505],\n",
       "       [ 0.09622067,  0.09824835,  0.10063223,  0.10639181,  0.09957093,\n",
       "         0.10098481,  0.09894833,  0.10161041,  0.10228871,  0.09510376],\n",
       "       [ 0.09642763,  0.09827951,  0.10078432,  0.10622419,  0.09966981,\n",
       "         0.10039943,  0.09958859,  0.10174155,  0.10215368,  0.0947313 ],\n",
       "       [ 0.0966106 ,  0.09853792,  0.10084355,  0.1062928 ,  0.09948356,\n",
       "         0.10058731,  0.0989475 ,  0.10169125,  0.10215264,  0.09485287],\n",
       "       [ 0.09686993,  0.09856873,  0.10055861,  0.1061739 ,  0.09939444,\n",
       "         0.1007276 ,  0.09912766,  0.10185342,  0.10230488,  0.09442082],\n",
       "       [ 0.09651871,  0.09847943,  0.10032398,  0.10621693,  0.09940601,\n",
       "         0.10104754,  0.09917987,  0.101697  ,  0.10219454,  0.09493598],\n",
       "       [ 0.09647787,  0.09848258,  0.10060122,  0.10617802,  0.0992415 ,\n",
       "         0.10100586,  0.09895606,  0.10183974,  0.10190135,  0.0953158 ],\n",
       "       [ 0.09653268,  0.098442  ,  0.10078751,  0.10594655,  0.09958113,\n",
       "         0.10086027,  0.09888634,  0.10195982,  0.10204868,  0.09495502],\n",
       "       [ 0.09664566,  0.0986948 ,  0.10029758,  0.1059169 ,  0.09975832,\n",
       "         0.10054159,  0.0990213 ,  0.10211741,  0.10205941,  0.09494703],\n",
       "       [ 0.09659982,  0.09863483,  0.10048801,  0.10631511,  0.09971997,\n",
       "         0.10043024,  0.09895579,  0.10187114,  0.10206033,  0.09492475],\n",
       "       [ 0.09623993,  0.09873546,  0.10001258,  0.10632841,  0.0994163 ,\n",
       "         0.1010685 ,  0.09955853,  0.10159509,  0.10265519,  0.09439002],\n",
       "       [ 0.09621265,  0.09843948,  0.1005809 ,  0.10660857,  0.0998337 ,\n",
       "         0.10034515,  0.09918182,  0.10191833,  0.10206787,  0.09481154],\n",
       "       [ 0.09626599,  0.09857668,  0.10042009,  0.10624516,  0.09970811,\n",
       "         0.10106869,  0.09874704,  0.10199224,  0.10209225,  0.09488377],\n",
       "       [ 0.09677298,  0.09807155,  0.10026605,  0.10643408,  0.09950761,\n",
       "         0.10057086,  0.09917555,  0.10183786,  0.10250606,  0.09485739],\n",
       "       [ 0.09663224,  0.09843636,  0.10031698,  0.10597171,  0.09958135,\n",
       "         0.10109976,  0.0988094 ,  0.10188088,  0.10212975,  0.09514158],\n",
       "       [ 0.09660555,  0.09881413,  0.10068267,  0.1063568 ,  0.09923195,\n",
       "         0.10041315,  0.09902675,  0.10179832,  0.10203423,  0.09503646],\n",
       "       [ 0.09649538,  0.09885694,  0.10063218,  0.10602167,  0.09967426,\n",
       "         0.10061773,  0.09903412,  0.10177503,  0.10248826,  0.09440445],\n",
       "       [ 0.09649002,  0.09835974,  0.10092072,  0.10645249,  0.09947528,\n",
       "         0.10065358,  0.09889775,  0.10181901,  0.10223151,  0.0946999 ],\n",
       "       [ 0.0963076 ,  0.09894211,  0.10046609,  0.10605027,  0.09953539,\n",
       "         0.1009455 ,  0.09934129,  0.10160852,  0.10209185,  0.09471136],\n",
       "       [ 0.09633035,  0.09869477,  0.10041873,  0.10644907,  0.09939756,\n",
       "         0.10089926,  0.09888049,  0.10162917,  0.10235918,  0.09494142],\n",
       "       [ 0.0963292 ,  0.09839838,  0.10073536,  0.10598817,  0.10013743,\n",
       "         0.1004674 ,  0.09905974,  0.10182286,  0.10260484,  0.09445662],\n",
       "       [ 0.09646699,  0.09877864,  0.10051423,  0.10625241,  0.09956363,\n",
       "         0.10060954,  0.09886484,  0.10160048,  0.10252585,  0.09482337],\n",
       "       [ 0.09663028,  0.09854492,  0.10028905,  0.10623261,  0.09962413,\n",
       "         0.10025695,  0.09932992,  0.10196498,  0.10195127,  0.09517589],\n",
       "       [ 0.09628053,  0.09851359,  0.10023841,  0.10599306,  0.09970814,\n",
       "         0.10088936,  0.09886041,  0.10203728,  0.10254096,  0.09493825],\n",
       "       [ 0.09628607,  0.0985301 ,  0.10048885,  0.10645503,  0.09956688,\n",
       "         0.10056496,  0.09921293,  0.10177158,  0.10196625,  0.09515735],\n",
       "       [ 0.09634938,  0.09847679,  0.10082617,  0.10656433,  0.09930166,\n",
       "         0.10085483,  0.09869019,  0.10166297,  0.10229132,  0.09498238],\n",
       "       [ 0.09637767,  0.09904156,  0.10028704,  0.10596573,  0.0997201 ,\n",
       "         0.10130636,  0.09910533,  0.10164643,  0.10190478,  0.09464501],\n",
       "       [ 0.09635611,  0.0986787 ,  0.10036898,  0.10630559,  0.09977721,\n",
       "         0.10077741,  0.09907931,  0.10159237,  0.10232824,  0.09473607],\n",
       "       [ 0.09640283,  0.09878067,  0.10068182,  0.10656159,  0.09949276,\n",
       "         0.10060918,  0.09908063,  0.10149474,  0.1021032 ,  0.09479258],\n",
       "       [ 0.09635112,  0.09883368,  0.10048091,  0.10601376,  0.09946366,\n",
       "         0.10068361,  0.09900434,  0.10174399,  0.10275674,  0.0946682 ],\n",
       "       [ 0.09630617,  0.09853031,  0.10087493,  0.106362  ,  0.09981856,\n",
       "         0.10020512,  0.09899807,  0.10179798,  0.10230314,  0.09480373],\n",
       "       [ 0.09628468,  0.0983843 ,  0.10076323,  0.10654455,  0.09966309,\n",
       "         0.10057525,  0.09885953,  0.10178146,  0.10230664,  0.09483728],\n",
       "       [ 0.09660009,  0.09839684,  0.10054187,  0.1060627 ,  0.0993197 ,\n",
       "         0.10076486,  0.09912791,  0.10185192,  0.10248016,  0.09485396],\n",
       "       [ 0.09674174,  0.09817201,  0.09997707,  0.10641057,  0.09966651,\n",
       "         0.10101401,  0.09890428,  0.10141658,  0.10251961,  0.09517762],\n",
       "       [ 0.09622865,  0.09852195,  0.10079677,  0.10631497,  0.09944425,\n",
       "         0.10045487,  0.09918821,  0.10186751,  0.10253579,  0.09464702],\n",
       "       [ 0.09649211,  0.09844462,  0.10073001,  0.10658561,  0.09959358,\n",
       "         0.10042348,  0.09897708,  0.10172163,  0.10213676,  0.09489513],\n",
       "       [ 0.09652482,  0.0986798 ,  0.10077047,  0.10615806,  0.0994321 ,\n",
       "         0.10093175,  0.09892278,  0.10156406,  0.10223491,  0.09478126],\n",
       "       [ 0.0963656 ,  0.09881427,  0.10041037,  0.10630376,  0.09951568,\n",
       "         0.10040657,  0.09917291,  0.10206599,  0.10215348,  0.09479138],\n",
       "       [ 0.09648829,  0.09875259,  0.10050019,  0.10609972,  0.09947661,\n",
       "         0.10102667,  0.0991057 ,  0.10155282,  0.10239138,  0.09460602],\n",
       "       [ 0.09619867,  0.09837961,  0.10019036,  0.10612816,  0.09979305,\n",
       "         0.10073895,  0.09928455,  0.10200891,  0.10239418,  0.09488358],\n",
       "       [ 0.09642324,  0.09862681,  0.10091708,  0.10627031,  0.09949689,\n",
       "         0.10051648,  0.09916666,  0.10182589,  0.10222545,  0.0945312 ],\n",
       "       [ 0.09628633,  0.09840678,  0.10043833,  0.10608935,  0.09953134,\n",
       "         0.1007764 ,  0.09914928,  0.10194137,  0.10262519,  0.09475565],\n",
       "       [ 0.09669801,  0.09867291,  0.1004611 ,  0.10614112,  0.09966688,\n",
       "         0.10066005,  0.09892543,  0.10171204,  0.10274463,  0.09431784],\n",
       "       [ 0.09648934,  0.09866885,  0.10053832,  0.10606092,  0.09961338,\n",
       "         0.10085572,  0.09901894,  0.10166857,  0.1021763 ,  0.09490967],\n",
       "       [ 0.09621265,  0.09856165,  0.10044919,  0.10634757,  0.09934844,\n",
       "         0.10066675,  0.0991516 ,  0.101709  ,  0.10268105,  0.0948721 ],\n",
       "       [ 0.09637697,  0.09846975,  0.10080892,  0.10619858,  0.09926895,\n",
       "         0.10054597,  0.09931268,  0.1021157 ,  0.10209299,  0.09480949],\n",
       "       [ 0.09637753,  0.09866195,  0.10051715,  0.10623075,  0.09961596,\n",
       "         0.10074561,  0.09874608,  0.10192066,  0.10236443,  0.09481987],\n",
       "       [ 0.09631584,  0.09859437,  0.10051469,  0.10631948,  0.09954773,\n",
       "         0.10040753,  0.09943023,  0.10168654,  0.10250511,  0.09467848],\n",
       "       [ 0.09613652,  0.09852312,  0.10022512,  0.1061309 ,  0.09976632,\n",
       "         0.10073235,  0.09914596,  0.10194034,  0.10219666,  0.09520269],\n",
       "       [ 0.09640448,  0.09828358,  0.1005201 ,  0.10658389,  0.09988702,\n",
       "         0.10045124,  0.09906002,  0.10183054,  0.10225826,  0.09472087],\n",
       "       [ 0.09611892,  0.09833847,  0.10100163,  0.10602041,  0.09969897,\n",
       "         0.1005861 ,  0.09917961,  0.10178775,  0.10239015,  0.094878  ],\n",
       "       [ 0.09635108,  0.09862377,  0.10081765,  0.10644024,  0.09957707,\n",
       "         0.10064631,  0.0985717 ,  0.10164191,  0.10209917,  0.0952311 ],\n",
       "       [ 0.09627547,  0.0984028 ,  0.10064462,  0.10630389,  0.09979048,\n",
       "         0.10090512,  0.09874539,  0.10182606,  0.10217356,  0.09493261],\n",
       "       [ 0.09637885,  0.0985369 ,  0.10077491,  0.10646452,  0.09949727,\n",
       "         0.10081193,  0.09902306,  0.10167067,  0.10219913,  0.09464277],\n",
       "       [ 0.09633563,  0.09866652,  0.10014644,  0.10638807,  0.0993632 ,\n",
       "         0.10073873,  0.09905788,  0.10197696,  0.10230875,  0.09501783],\n",
       "       [ 0.09646646,  0.09843276,  0.10030958,  0.10602861,  0.09965481,\n",
       "         0.1007779 ,  0.09928819,  0.10163404,  0.10243808,  0.09496956],\n",
       "       [ 0.09628436,  0.09826105,  0.10059347,  0.1064151 ,  0.0994266 ,\n",
       "         0.10083155,  0.09911202,  0.10163478,  0.10241918,  0.09502189],\n",
       "       [ 0.09655696,  0.09887667,  0.10036792,  0.10624158,  0.09983946,\n",
       "         0.10071233,  0.09923676,  0.1017277 ,  0.10210266,  0.09433794],\n",
       "       [ 0.09613343,  0.09856248,  0.10050413,  0.10619565,  0.09989907,\n",
       "         0.10058901,  0.09927966,  0.1016794 ,  0.10225685,  0.09490033],\n",
       "       [ 0.09666495,  0.09852853,  0.1006357 ,  0.10617252,  0.09978856,\n",
       "         0.1008267 ,  0.09881619,  0.10170544,  0.10203038,  0.09483102],\n",
       "       [ 0.09650636,  0.09820383,  0.10028766,  0.1066188 ,  0.09969746,\n",
       "         0.10077086,  0.09921111,  0.10182634,  0.10208067,  0.09479693],\n",
       "       [ 0.09621437,  0.0986122 ,  0.10047169,  0.10659315,  0.09943306,\n",
       "         0.1005242 ,  0.09879891,  0.10179342,  0.10232309,  0.09523591],\n",
       "       [ 0.09648793,  0.09862642,  0.1005897 ,  0.10617888,  0.09972961,\n",
       "         0.10071001,  0.09896318,  0.10170398,  0.10220449,  0.09480581],\n",
       "       [ 0.09638863,  0.09855577,  0.10059523,  0.10659494,  0.09967025,\n",
       "         0.10049333,  0.09916188,  0.1016168 ,  0.10228235,  0.09464082],\n",
       "       [ 0.09677321,  0.09848167,  0.10005722,  0.10625161,  0.09989044,\n",
       "         0.10070466,  0.09899286,  0.10191584,  0.10219026,  0.09474223],\n",
       "       [ 0.09631617,  0.0983308 ,  0.10060156,  0.1062401 ,  0.09953637,\n",
       "         0.10085652,  0.09916846,  0.10150613,  0.10235183,  0.09509204],\n",
       "       [ 0.09613628,  0.09854443,  0.10015691,  0.10632129,  0.09994251,\n",
       "         0.10056401,  0.09908534,  0.10178641,  0.10258654,  0.09487628],\n",
       "       [ 0.0964603 ,  0.09844357,  0.10082095,  0.10610306,  0.09936512,\n",
       "         0.10064505,  0.09926515,  0.10189936,  0.10194274,  0.0950547 ],\n",
       "       [ 0.09622286,  0.09826419,  0.10069852,  0.1063878 ,  0.09965816,\n",
       "         0.10059179,  0.09916881,  0.10173153,  0.10256068,  0.09471565],\n",
       "       [ 0.09651738,  0.09846084,  0.10048527,  0.10607169,  0.09950862,\n",
       "         0.1005772 ,  0.09931038,  0.10176741,  0.10265538,  0.09464582],\n",
       "       [ 0.09651673,  0.09849357,  0.10088443,  0.10641298,  0.09948639,\n",
       "         0.10038318,  0.09920006,  0.10184422,  0.10209089,  0.09468756],\n",
       "       [ 0.09667531,  0.09874435,  0.10049837,  0.10607206,  0.09932818,\n",
       "         0.10029888,  0.09931086,  0.10200083,  0.10229793,  0.09477324],\n",
       "       [ 0.09664365,  0.09840231,  0.10067094,  0.10630684,  0.09940842,\n",
       "         0.10054756,  0.09875886,  0.10174427,  0.10244899,  0.09506817],\n",
       "       [ 0.09655409,  0.09851237,  0.10032468,  0.10633219,  0.09948629,\n",
       "         0.10063653,  0.09917944,  0.10180471,  0.10208946,  0.09508024],\n",
       "       [ 0.09661433,  0.09799781,  0.10073929,  0.10647987,  0.09985843,\n",
       "         0.10069046,  0.09907702,  0.10181122,  0.10183068,  0.09490089],\n",
       "       [ 0.09643124,  0.09879417,  0.10043252,  0.10602886,  0.09957447,\n",
       "         0.10068628,  0.09885045,  0.10180514,  0.10249931,  0.09489755]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 매개변수 예측 처리 (순방향)\n",
    "x = np.random.rand(100, 784) # 더미 입력 데이터 100장 \n",
    "y = net.predict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# 기울기 계산\n",
    "x = np.random.rand(100, 784) # 더미 입력 데이터 (100장)\n",
    "t = np.random.rand(100, 10) # 더미 정답 데이터 (100장)\n",
    "\n",
    "grads = net.numerical_gradient(x,t)\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2 미니배치 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TwoLayerNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-420db5fbb913>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TwoLayerNet' is not defined"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tmp.dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60,000 개의 훈련데이터에서 임의 100개 데이터 추림 (이미지, 정답 데이터) <br>\n",
    "확률 적으로 경사 하강법 수행으로 매개변수 갱신<br>\n",
    "갱신 횟수 10,000 번 (반복횟수) <br>\n",
    "갱신 시, 훈련데이터에 대한 손실함수계산, 배열에 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.0986333333333, 0.0958\n",
      "train acc, test acc | 0.788166666667, 0.7918\n",
      "train acc, test acc | 0.879333333333, 0.8826\n",
      "train acc, test acc | 0.897816666667, 0.9012\n",
      "train acc, test acc | 0.908966666667, 0.9118\n",
      "train acc, test acc | 0.914733333333, 0.916\n",
      "train acc, test acc | 0.92025, 0.9222\n",
      "train acc, test acc | 0.924416666667, 0.9248\n",
      "train acc, test acc | 0.928466666667, 0.9279\n",
      "train acc, test acc | 0.93235, 0.9316\n",
      "train acc, test acc | 0.934816666667, 0.935\n",
      "train acc, test acc | 0.937483333333, 0.9376\n",
      "train acc, test acc | 0.940166666667, 0.9389\n",
      "train acc, test acc | 0.941966666667, 0.9399\n",
      "train acc, test acc | 0.94375, 0.9419\n",
      "train acc, test acc | 0.945783333333, 0.9448\n",
      "train acc, test acc | 0.947166666667, 0.9456\n"
     ]
    }
   ],
   "source": [
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.3 시험 데이터로 평가 \n",
    "\n",
    "* epoch : 학습에서 훈련 데이터를 모두 소진했을 떄의 횟수 (10,000개를 100개의 미지배치로 학습한 경우 100회 1epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8MhUs5YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d576737828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 정확도 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
