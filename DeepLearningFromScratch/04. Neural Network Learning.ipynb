{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손실함수 : 결과값을 가장 작게 만드는 함수 가중치 매개변수를 찾을떄 사용 \n",
    "\n",
    "# 4.1 데이터 학습\n",
    "\n",
    "규칙을 ‘사람’이 만드는 방식 ‘기계’가 데이터로부터 규칙(특징)을 발견해 내는 방식으로 패러다임 전환\n",
    "\n",
    "## 4.1.1 데이터 주도학습\n",
    "특징을 추출하고 벡터가 가지는 가중치를 부여\n",
    "\n",
    "종단간 기계학습(end-to-end machine learning) : 처음부터 끝까지 입력에서 결과를 얻는다. \n",
    "\n",
    "## 4.1.2 훈련데이터와 시험데이터 \n",
    "범용능력을 평가하기 위해서 train, test 데이터 분리 \n",
    "\n",
    "# 4.2 손실함수 \n",
    "loss function : 최적의 매겨변수 값을 탐색 \n",
    "\n",
    "평균 제곱 오차, 교차 엔트로피 오차 사용\n",
    "\n",
    "\n",
    "## 4.2.1 평균 제곱 오차 \n",
    "\n",
    "mean squared error MSE : 각 원소의 출력(추정) 값과 정답 레이블의 차를 제곱한 후 그 총합 \n",
    "\n",
    "one hot encoding: 한원소만 1로 하고 그외는 0으로 나타내는 표기범\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.095\n",
      "0.415\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.2,0.7,0.2,0.1,0,0,0,0,0]\n",
    "\n",
    "print(mean_squared_error(np.array(y), np.array(t)))\n",
    "\n",
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.2,0.7,0.2,0.1,0,0.8,0,0,0]\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오답에 가까울 수록 결과가 크다.\n",
    "\n",
    "## 4.2.2 교차 엔트로피 \n",
    "cross entroypy CEE : 정답에 해당하는 인덱스만 1 log 를 곱해도 나머진 모두 0이므로 결과에 영향을 주지 않는다. 정답일떄의 출력이 전체 값을 정함\n",
    "\n",
    "정답에 해당하는 출력이 커질수록 0에 다가가다가 출력이 1일떄 0이 됨, 반대로 정답일때의 출력이 작아질수록 오차는 커짐\n",
    "\n",
    "- 정보엔트로피 : 현재 사건에 대한 정보량 (정보량이 많다. = 선택사항이 많다. = 불확실 하다. = 복잡성, 불확정성)\n",
    "- 교차엔트로피 (자연로그) : 두 확률 간의 엔트로피 (두확률간에 복잡도가 얼마나 높은지, 오차가 얼마나 많은지)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "# np.log(0) 이면 -inf 가 되어 계산을 진행할수 없음 0이 되지 않도록 해줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356674801082\n",
      "0.69314698056\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.2,0.7,0.2,0.1,0,0,0.1,0.2,0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "y = [0.1,0.2,0.5,0.2,0.1,0,0.8,0,0,0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차가 작은 처번째 추정이 정답일 가능성이 높다고 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 미니배치 학습\n",
    "\n",
    "모두에 대한 손실 함수의 합을 구함 평균 손실 함수 \n",
    "\n",
    "일부만 골라서 학습 미니배치 학습 \n",
    "\n",
    "test 한건당 평균 오차 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 \n",
    "import numpy as np\n",
    "import pickle\n",
    "from tmp.dataset.mnist import load_mnist\n",
    "from common.functions import sigmoid, softmax\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize =True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터에서 무작위로 10장만 뺌 np.random.choice 이용\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "x_batch[0:10]\n",
    "t_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35983, 53085, 12898, 30921,  9693,  2657, 45665,  6457,   993, 43959])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4 (배치용) 교차 엔트로피 오차 구현하기 \n",
    "\n",
    "데이터 하나당 교차 엔트로피 오차를 구하는 경우 reshape 함수로 데이터 shape 을 바꿈\n",
    "배치크기로 나눠 정규화를 하고 이미지 1장단 평균의 교차 엔트로피 오차를 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21815200513228775"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmp.ch03 import neuralnet_mnist\n",
    "network = neuralnet_mnist.init_network()\n",
    "y = neuralnet_mnist.predict(network, x_batch)\n",
    "cross_entropy_error(y, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot 이 아닌 숫자 레이블일 경우 \n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t]))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([  5.24714182e-04,   1.59779377e-03,   7.48113889e-05,\n",
       "         6.39280770e-04,   9.94004071e-01,   4.35634895e-04,\n",
       "         1.87226001e-03,   9.97999232e-05,   7.29477335e-07,\n",
       "         2.27661990e-02], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size\n",
    "t\n",
    "np.arange(batch_size)\n",
    "y[np.arange(batch_size), t]\n",
    "# np.arange(batch_size) row / t column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* np.log(y[arange(batch_size), t])  설명<br>\n",
    "0 부터 batch_size-1 까지 배열을 생성 <br>\n",
    "각 데이터 정답 레이블에 해당하는 신경망의 출력을 추출 <br>\n",
    "예) [y[0,2],y[1,7],y[3,9],y[4,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232.84817245588638"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y, [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 두가지를 한번에\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원핫벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t]))/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.5 왜 손실 함수를 설정하는가?\n",
    "\n",
    "손실함수의 미분 : 가중치 매개변수의 값을 아주 조금 변화시켰을때, 손실함수가 어떻게 변하냐 <br>\n",
    "음수면 가중치 매개변수를 양으로 양이면 음수로 변화 시킴 <br>\n",
    "미분 값이 0 이면 멈춤 <br>\n",
    "\n",
    "계단 함수의 경우 미분한 값이 대부분 0 시그모이드는 연속적으로 변함 <br>\n",
    "기울기가 0이 되지 않아 올바르게 학습할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 수치 미분\n",
    "\n",
    "경사법 기울기(경사) 값을 기준으로 나아가야 하는 방향 \n",
    "\n",
    "## 4.3.1 미분\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=10e-50\n",
    "    return (f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제점1) 반올림 오차 소숫점 8자리 이하는 생략되어 0이 되버림 (float32 0이되버림) > 1e-4 사용 <br>\n",
    "문제점2) 차분 x+h 와 x 사이의 기울기에 해당됨 x 의 진정한 접선이 아니다. \n",
    "\n",
    "x+h 와 x-h 를 사용하여 x 를 중심으로 전후 차분을 계산 : **중심차분(중앙차분) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=1e-4 #0.001\n",
    "    return (f(x+h)-f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 수치미분의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tangent_line(f, x):\n",
    "    a = numerical_diff(f,x)\n",
    "    b = f(x)-a*x\n",
    "    return lambda t:a*t+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25, -0.23, -0.21, -0.19, -0.17, -0.15, -0.13, -0.11, -0.09,\n",
       "       -0.07, -0.05, -0.03, -0.01,  0.01,  0.03,  0.05,  0.07,  0.09,\n",
       "        0.11,  0.13,  0.15,  0.17,  0.19,  0.21,  0.23,  0.25,  0.27,\n",
       "        0.29,  0.31,  0.33,  0.35,  0.37,  0.39,  0.41,  0.43,  0.45,\n",
       "        0.47,  0.49,  0.51,  0.53,  0.55,  0.57,  0.59,  0.61,  0.63,\n",
       "        0.65,  0.67,  0.69,  0.71,  0.73,  0.75,  0.77,  0.79,  0.81,\n",
       "        0.83,  0.85,  0.87,  0.89,  0.91,  0.93,  0.95,  0.97,  0.99,\n",
       "        1.01,  1.03,  1.05,  1.07,  1.09,  1.11,  1.13,  1.15,  1.17,\n",
       "        1.19,  1.21,  1.23,  1.25,  1.27,  1.29,  1.31,  1.33,  1.35,\n",
       "        1.37,  1.39,  1.41,  1.43,  1.45,  1.47,  1.49,  1.51,  1.53,\n",
       "        1.55,  1.57,  1.59,  1.61,  1.63,  1.65,  1.67,  1.69,  1.71,\n",
       "        1.73,  1.75,  1.77,  1.79,  1.81,  1.83,  1.85,  1.87,  1.89,\n",
       "        1.91,  1.93,  1.95,  1.97,  1.99,  2.01,  2.03,  2.05,  2.07,\n",
       "        2.09,  2.11,  2.13,  2.15,  2.17,  2.19,  2.21,  2.23,  2.25,\n",
       "        2.27,  2.29,  2.31,  2.33,  2.35,  2.37,  2.39,  2.41,  2.43,\n",
       "        2.45,  2.47,  2.49,  2.51,  2.53,  2.55,  2.57,  2.59,  2.61,\n",
       "        2.63,  2.65,  2.67,  2.69,  2.71,  2.73,  2.75,  2.77,  2.79,\n",
       "        2.81,  2.83,  2.85,  2.87,  2.89,  2.91,  2.93,  2.95,  2.97,\n",
       "        2.99,  3.01,  3.03,  3.05,  3.07,  3.09,  3.11,  3.13,  3.15,\n",
       "        3.17,  3.19,  3.21,  3.23,  3.25,  3.27,  3.29,  3.31,  3.33,\n",
       "        3.35,  3.37,  3.39,  3.41,  3.43,  3.45,  3.47,  3.49,  3.51,\n",
       "        3.53,  3.55,  3.57,  3.59,  3.61,  3.63,  3.65,  3.67,  3.69,\n",
       "        3.71,  3.73])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x82430f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x82515f8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x82b14a8>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x82b1ef0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ec8c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "\n",
    "tangent = tangent_line(function_1, 5)\n",
    "tl = tangent(x)\n",
    "tl\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,tl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5일떄 변화량\n",
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10일떄 변화량\n",
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3 편미분\n",
    "partial differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 #np.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0=3 x1=4 x0 에 대한 편미분\n",
    "def function_tmp1(x):\n",
    "    return x*x + 4.0**2.0\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0=3 x1=4 x1 에 대한 편미분\n",
    "def function_tmp1(x):\n",
    "    return  3.0**2.0+x*x\n",
    "numerical_diff(function_tmp1, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 기울기 (gradient)\n",
    "\n",
    "모든 변수의 평미분을 벡터로 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모든 변수의 편미분 (기울기) 값 구하기 \n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # k 와 shape 이 같은 0배열을 생성 \n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  8.]\n",
      "[ 0.  4.]\n",
      "[ 6.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "기울기가 가리키는 쪽: 각 장소에서 함수의 출력 값을 가장 줄이는 방향 (최소값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.4.1 경사 하강법\n",
    "최적의 매개변수 찾기 \n",
    "\n",
    "* 극소값, 최소값, 안장점 (sddle point) 이 되는 장소에서는 기울기가 0 \n",
    "    * 복잡하고 찌그러진 모양의 함수라면 평평한곳으로 파고들면 고원(plateau) 라는 학습이 진행되지 않는 정체기에 빠질수 있다.\n",
    "    \n",
    "* 경사법 : 기울기를 구하고 -> 기울어진 방향으로 일정 거리만큼 이동 반복 \n",
    "    * 학습률 : 매개변수 값을 얼마나 갱신할지 갱신하는 양 \n",
    "    \n",
    "현재 상태 = 이전 상태 - 경사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.11110793e-10,   8.14814391e-10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.58983747e+13,  -1.29524862e+12])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 경우 \n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "큰 값으로 발산해버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 작은 경우 \n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "거의 갱신되지 않은 채로 끝남 \n",
    "\n",
    "numpy 의 array 는 list와 다르게 [:] 복제가 x = init_x  아닌 init_x.copy() 로 해야 init_x 값이 변하지 않는다. \n",
    "\n",
    "* 하이퍼파라미터 : 학습률 같은 매개변수<br> \n",
    "    가중치, 편향 같은 매개변수는 훈련 데이터와 학습 알고리즘에 의해 자동으로 획득<br> \n",
    "    하이퍼파라미터는 직접 설정해줘야함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2 신경망의 기울기 \n",
    "\n",
    "가중치 매개변수에 관한 손실함수의 기울기 <br>\n",
    "W 가 조금 변경했을 떄 손실함수 L 이 얼마나 변화하느냐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 신경망 기울기 구하기 \n",
    "# x 입력 데이터, t 정답 레이블 \n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    # 2 * 3 가중치 매개변수\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    # 예측함수\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    #손실함수\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48310344  1.90941639 -0.10516501]\n",
      " [ 0.68532831  0.45945631 -0.85851639]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90665754  1.55916052 -0.83576375]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8723504888714944"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0,0,1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19383368  0.37222683 -0.56606051]\n",
      " [ 0.29075052  0.55834025 -0.84909077]]\n"
     ]
    }
   ],
   "source": [
    "# numerical_gradient 내부에서 f(x) 를 실행 -> 일관성을 위해서 f(W) 를 정의\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "* 전제 <br>\n",
    "    가중치와 편향을 훈련데이터에 적응하는 과정 : 학습\n",
    "    \n",
    "* 1단계-미니배치 <br>\n",
    "    훈련데이터중에 일부 무작위로 선별 \n",
    "    \n",
    "* 2단계-기울기 산출 <br>\n",
    "    손실함수 값을 줄이기 위해 가중치 매개변수의 기울기 구함 손실 함수를 작게하는 방향을 구함\n",
    "    \n",
    "* 3단계-매개변수 갱신 <br>\n",
    "    가중치 매개변수를 기울기 방향으로 갱신 \n",
    "    \n",
    "* 4단계-반복\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **SGD** 확률적 경사 하강법 (stochastic gradient descent) <br>\n",
    "    확률 적으로 무작위로 골라낸 데이터 로 하는 경사하강법\n",
    "    \n",
    "## 4.5.1 2층 신경망 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    # 입력충의 뉴런수, 은닉층의 뉴런수, 출력층의 뉴런수\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        # 1층의 가중치 \n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        # 2층의 가중치 \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    # 예측 : 이미지 데이터\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # 손실함수값 x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    # 예측 정확도\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # 가중치 매개변수 기울기 x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        # 1층 가중치의 기울기\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        # 2층 가중치의 기울기\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # numerical_gradient 개선판 \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# 신경망에 필요한 매개변수 저장\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09707189,  0.10129189,  0.091523  ,  0.10397075,  0.10806044,\n",
       "         0.10069367,  0.09483567,  0.10361215,  0.10201987,  0.09692066],\n",
       "       [ 0.09649269,  0.10144229,  0.09199072,  0.10416948,  0.10794061,\n",
       "         0.10037948,  0.0949145 ,  0.10356521,  0.10195219,  0.09715284],\n",
       "       [ 0.09674049,  0.10138959,  0.09156471,  0.10402363,  0.10807387,\n",
       "         0.10040011,  0.09511323,  0.10378822,  0.10188773,  0.09701841],\n",
       "       [ 0.09702121,  0.10136123,  0.09162577,  0.1037925 ,  0.10793477,\n",
       "         0.10064575,  0.09508692,  0.10405596,  0.10167748,  0.09679841],\n",
       "       [ 0.09684844,  0.10175652,  0.09155922,  0.10401971,  0.10781472,\n",
       "         0.10028806,  0.09480651,  0.10373731,  0.10210824,  0.09706128],\n",
       "       [ 0.09699031,  0.10150254,  0.09134974,  0.10407549,  0.10833824,\n",
       "         0.10052274,  0.09488384,  0.10347607,  0.10185495,  0.09700608],\n",
       "       [ 0.09716164,  0.10132076,  0.09149925,  0.10379206,  0.10817238,\n",
       "         0.10027034,  0.09480523,  0.10376656,  0.10208513,  0.09712667],\n",
       "       [ 0.09685679,  0.10151828,  0.09188555,  0.10378359,  0.10802314,\n",
       "         0.10012556,  0.09475671,  0.10400177,  0.10195177,  0.09709683],\n",
       "       [ 0.0962814 ,  0.101346  ,  0.09173011,  0.10407639,  0.10816309,\n",
       "         0.10042889,  0.09513101,  0.10357789,  0.10213314,  0.09713208],\n",
       "       [ 0.0971574 ,  0.10101625,  0.09145976,  0.1035909 ,  0.10834215,\n",
       "         0.10030269,  0.09487864,  0.10371095,  0.10240305,  0.09713821],\n",
       "       [ 0.09656167,  0.1013377 ,  0.09168391,  0.10411083,  0.10824965,\n",
       "         0.10062302,  0.09464054,  0.10377745,  0.1017203 ,  0.09729494],\n",
       "       [ 0.09703014,  0.10107526,  0.09193311,  0.10388093,  0.10849612,\n",
       "         0.10027851,  0.09483188,  0.10339568,  0.10186058,  0.09721779],\n",
       "       [ 0.09720511,  0.10127585,  0.09142629,  0.10398596,  0.10806616,\n",
       "         0.10078437,  0.09490461,  0.10370908,  0.10178898,  0.09685359],\n",
       "       [ 0.09707482,  0.1014372 ,  0.09149762,  0.1039104 ,  0.1082085 ,\n",
       "         0.10029202,  0.09505429,  0.1034663 ,  0.10197944,  0.0970794 ],\n",
       "       [ 0.09667044,  0.10135878,  0.09176366,  0.10369528,  0.10821266,\n",
       "         0.10073559,  0.09478259,  0.10375789,  0.10190645,  0.09711665],\n",
       "       [ 0.09715821,  0.10092771,  0.09184422,  0.10417634,  0.10807183,\n",
       "         0.10028198,  0.09516495,  0.10373385,  0.10199487,  0.09664605],\n",
       "       [ 0.09676151,  0.10128943,  0.09193166,  0.1041921 ,  0.10780899,\n",
       "         0.10040578,  0.09524807,  0.1033509 ,  0.10199302,  0.09701854],\n",
       "       [ 0.09658923,  0.10151008,  0.09188949,  0.10430937,  0.10794346,\n",
       "         0.10011106,  0.09474682,  0.10332634,  0.10197588,  0.09759827],\n",
       "       [ 0.09651727,  0.10129842,  0.09168962,  0.10435939,  0.10841634,\n",
       "         0.09999496,  0.09458818,  0.10397695,  0.10190835,  0.09725054],\n",
       "       [ 0.0968371 ,  0.10159249,  0.0918496 ,  0.10423508,  0.10755499,\n",
       "         0.10006028,  0.09496849,  0.10387411,  0.10182168,  0.09720617],\n",
       "       [ 0.0968033 ,  0.10097938,  0.0917144 ,  0.10392256,  0.10813067,\n",
       "         0.1003029 ,  0.09474389,  0.10379938,  0.102183  ,  0.09742051],\n",
       "       [ 0.09694342,  0.10088878,  0.09156915,  0.10380321,  0.10856155,\n",
       "         0.10044745,  0.09481769,  0.1037618 ,  0.10200548,  0.09720146],\n",
       "       [ 0.09661786,  0.10126103,  0.09151541,  0.10383711,  0.10803211,\n",
       "         0.10055172,  0.09518797,  0.10381093,  0.10200852,  0.09717733],\n",
       "       [ 0.09693419,  0.10161297,  0.09195853,  0.10370348,  0.1076963 ,\n",
       "         0.10017699,  0.09493229,  0.10349489,  0.102137  ,  0.09735336],\n",
       "       [ 0.09679575,  0.1014647 ,  0.09169293,  0.10399047,  0.10814269,\n",
       "         0.10043325,  0.09524086,  0.10390773,  0.10169262,  0.09663901],\n",
       "       [ 0.09647607,  0.10167006,  0.09196687,  0.10381625,  0.10813746,\n",
       "         0.10018198,  0.09487441,  0.10409527,  0.10161585,  0.09716577],\n",
       "       [ 0.09713322,  0.10123407,  0.09149845,  0.10378872,  0.10810958,\n",
       "         0.10053316,  0.0948211 ,  0.10383596,  0.10196366,  0.09708209],\n",
       "       [ 0.09686722,  0.10161537,  0.09186215,  0.1040989 ,  0.10799965,\n",
       "         0.10034991,  0.09504006,  0.10357017,  0.10158598,  0.09701059],\n",
       "       [ 0.09689931,  0.10123987,  0.09191427,  0.10387052,  0.10773274,\n",
       "         0.10012815,  0.09486659,  0.10389599,  0.10181399,  0.09763856],\n",
       "       [ 0.09660515,  0.10154267,  0.09154145,  0.10413173,  0.10805712,\n",
       "         0.10055489,  0.0947979 ,  0.10396197,  0.10175966,  0.09704746],\n",
       "       [ 0.09686574,  0.10112124,  0.09161286,  0.10337739,  0.10819788,\n",
       "         0.10038231,  0.09495554,  0.10407582,  0.1020663 ,  0.09734492],\n",
       "       [ 0.09635123,  0.10153436,  0.09192793,  0.10397129,  0.1078183 ,\n",
       "         0.10028806,  0.09487394,  0.10375698,  0.10201418,  0.09746371],\n",
       "       [ 0.0966795 ,  0.101421  ,  0.09172865,  0.10379083,  0.10804699,\n",
       "         0.10026592,  0.09474994,  0.10399512,  0.10205424,  0.0972678 ],\n",
       "       [ 0.09686284,  0.10155454,  0.09202151,  0.1038931 ,  0.10761587,\n",
       "         0.10066645,  0.09501653,  0.10349296,  0.10171586,  0.09716033],\n",
       "       [ 0.09647154,  0.10143263,  0.09190379,  0.10393978,  0.10841063,\n",
       "         0.10058647,  0.09475974,  0.10369894,  0.10172637,  0.09707011],\n",
       "       [ 0.0968984 ,  0.10164463,  0.09132417,  0.10432068,  0.10812077,\n",
       "         0.10045004,  0.09480511,  0.10327108,  0.10202899,  0.09713612],\n",
       "       [ 0.0968115 ,  0.10122087,  0.09147351,  0.10406418,  0.10802238,\n",
       "         0.10037978,  0.0948109 ,  0.10391697,  0.10199578,  0.09730413],\n",
       "       [ 0.09660399,  0.10130498,  0.09159505,  0.10413278,  0.10804086,\n",
       "         0.10080295,  0.09501134,  0.10353965,  0.10182472,  0.09714368],\n",
       "       [ 0.09676453,  0.10127841,  0.09160737,  0.1041811 ,  0.1080034 ,\n",
       "         0.10033501,  0.09493416,  0.10355484,  0.10204739,  0.09729379],\n",
       "       [ 0.09689778,  0.10133189,  0.09183408,  0.1036889 ,  0.10835914,\n",
       "         0.10026628,  0.09510565,  0.10383822,  0.10149975,  0.09717831],\n",
       "       [ 0.09709409,  0.10107507,  0.09149789,  0.10363869,  0.10822279,\n",
       "         0.10046122,  0.09491825,  0.10388779,  0.10209447,  0.09710975],\n",
       "       [ 0.09689946,  0.10137065,  0.09152523,  0.10387989,  0.10820074,\n",
       "         0.10059342,  0.09495431,  0.10348927,  0.10178749,  0.09729954],\n",
       "       [ 0.0964883 ,  0.10121328,  0.09196202,  0.10418178,  0.10837407,\n",
       "         0.10030046,  0.0949673 ,  0.10362029,  0.10186102,  0.09703148],\n",
       "       [ 0.0965423 ,  0.10184751,  0.09169111,  0.10368638,  0.1081588 ,\n",
       "         0.1003718 ,  0.09498371,  0.10338277,  0.10193652,  0.0973991 ],\n",
       "       [ 0.09728195,  0.10123039,  0.09150057,  0.10410699,  0.1081192 ,\n",
       "         0.10045249,  0.09446141,  0.1037435 ,  0.10211984,  0.09698366],\n",
       "       [ 0.0967725 ,  0.10107379,  0.092168  ,  0.10442304,  0.10799658,\n",
       "         0.10076309,  0.09483263,  0.10328758,  0.10172621,  0.09695658],\n",
       "       [ 0.0968181 ,  0.10140732,  0.09189641,  0.10365996,  0.10801202,\n",
       "         0.10054808,  0.09488331,  0.10388584,  0.10165751,  0.09723145],\n",
       "       [ 0.09689653,  0.10146692,  0.09155221,  0.10426084,  0.1082624 ,\n",
       "         0.10011397,  0.09492926,  0.10362731,  0.10179368,  0.09709688],\n",
       "       [ 0.09667453,  0.10160807,  0.09191214,  0.10416768,  0.10786469,\n",
       "         0.10056571,  0.0943953 ,  0.10395104,  0.10146225,  0.09739859],\n",
       "       [ 0.09710806,  0.10132659,  0.09168894,  0.10348237,  0.10798173,\n",
       "         0.10055338,  0.09493816,  0.10374997,  0.10183144,  0.09733936],\n",
       "       [ 0.09668002,  0.10163708,  0.09130916,  0.10397286,  0.10796611,\n",
       "         0.10021394,  0.09494043,  0.10412354,  0.10218949,  0.09696738],\n",
       "       [ 0.09685191,  0.10121651,  0.09136823,  0.1040358 ,  0.10841077,\n",
       "         0.10070658,  0.09470496,  0.1035965 ,  0.10180733,  0.0973014 ],\n",
       "       [ 0.09685872,  0.1009426 ,  0.09161788,  0.10409116,  0.10829556,\n",
       "         0.10040439,  0.09491254,  0.10347445,  0.10199436,  0.09740834],\n",
       "       [ 0.09666018,  0.10152775,  0.09197368,  0.10359714,  0.10795278,\n",
       "         0.10068155,  0.09511078,  0.10386216,  0.10149395,  0.09714004],\n",
       "       [ 0.09674444,  0.10151226,  0.09178601,  0.10399459,  0.10764488,\n",
       "         0.10057763,  0.09487116,  0.10358393,  0.10205789,  0.0972272 ],\n",
       "       [ 0.09698178,  0.10133953,  0.09191811,  0.10379441,  0.10807151,\n",
       "         0.1003898 ,  0.09494542,  0.10397455,  0.10157802,  0.09700686],\n",
       "       [ 0.09710302,  0.1013108 ,  0.09162584,  0.10374038,  0.10824864,\n",
       "         0.10051115,  0.09480086,  0.10365051,  0.10181925,  0.09718955],\n",
       "       [ 0.09703962,  0.10104059,  0.09140599,  0.10401637,  0.10806514,\n",
       "         0.10059992,  0.09501111,  0.10347111,  0.10214054,  0.09720962],\n",
       "       [ 0.0974244 ,  0.10102839,  0.09168331,  0.10408063,  0.10775957,\n",
       "         0.10058584,  0.09489213,  0.10365409,  0.101794  ,  0.09709765],\n",
       "       [ 0.09659047,  0.1012526 ,  0.09196053,  0.10405221,  0.10780351,\n",
       "         0.10032179,  0.09501014,  0.10360782,  0.10190157,  0.09749936],\n",
       "       [ 0.09712759,  0.10145649,  0.09164114,  0.10359877,  0.10772263,\n",
       "         0.10053001,  0.09487732,  0.10378056,  0.1019788 ,  0.09728669],\n",
       "       [ 0.09706471,  0.10134616,  0.09183893,  0.10373831,  0.10818679,\n",
       "         0.10049364,  0.09475923,  0.10388235,  0.10175726,  0.09693263],\n",
       "       [ 0.09689537,  0.10088665,  0.09162451,  0.10425832,  0.10804298,\n",
       "         0.10044543,  0.09497307,  0.10401998,  0.10199662,  0.09685707],\n",
       "       [ 0.0967298 ,  0.10121402,  0.0917742 ,  0.10419359,  0.10794466,\n",
       "         0.10038146,  0.09498753,  0.10347919,  0.10188541,  0.09741012],\n",
       "       [ 0.09696241,  0.10117113,  0.09159997,  0.10414325,  0.10812211,\n",
       "         0.10024759,  0.09509282,  0.10388927,  0.10179937,  0.09697208],\n",
       "       [ 0.09655291,  0.10134214,  0.09175409,  0.10371748,  0.10808753,\n",
       "         0.10044001,  0.09501641,  0.10374091,  0.10205306,  0.09729546],\n",
       "       [ 0.09671061,  0.10165359,  0.09161174,  0.10415038,  0.10800816,\n",
       "         0.10002099,  0.09507147,  0.10334479,  0.10200124,  0.09742702],\n",
       "       [ 0.09675803,  0.1009941 ,  0.09150307,  0.10419352,  0.10864963,\n",
       "         0.10050382,  0.0951304 ,  0.10340805,  0.10177353,  0.09708585],\n",
       "       [ 0.09687548,  0.10158509,  0.09163822,  0.10412602,  0.10791164,\n",
       "         0.10016204,  0.09496174,  0.10388292,  0.10189268,  0.09696417],\n",
       "       [ 0.09662576,  0.10143066,  0.09195226,  0.10399057,  0.10798417,\n",
       "         0.10024343,  0.09518089,  0.10355263,  0.10182104,  0.09721858],\n",
       "       [ 0.0966798 ,  0.10097384,  0.09168713,  0.10426503,  0.10859166,\n",
       "         0.09993575,  0.09497149,  0.1038719 ,  0.10182397,  0.09719943],\n",
       "       [ 0.09674977,  0.10139128,  0.09169363,  0.10396416,  0.10814496,\n",
       "         0.10062398,  0.09496933,  0.10351554,  0.10196179,  0.09698556],\n",
       "       [ 0.09661556,  0.10107976,  0.09189727,  0.10417328,  0.10834603,\n",
       "         0.10048686,  0.09494674,  0.10349057,  0.10207012,  0.0968938 ],\n",
       "       [ 0.09666972,  0.10103703,  0.09183306,  0.1039156 ,  0.10825211,\n",
       "         0.10060894,  0.09487216,  0.10391331,  0.10177677,  0.09712129],\n",
       "       [ 0.09665726,  0.101386  ,  0.09180424,  0.10372342,  0.10791259,\n",
       "         0.10056479,  0.09490315,  0.10355788,  0.10215347,  0.09733721],\n",
       "       [ 0.09642724,  0.10136079,  0.09168771,  0.10452403,  0.10810418,\n",
       "         0.10029261,  0.09473571,  0.10367975,  0.1021891 ,  0.09699889],\n",
       "       [ 0.09672053,  0.10112519,  0.09138114,  0.1042706 ,  0.1082858 ,\n",
       "         0.10019948,  0.09489903,  0.10376175,  0.10201574,  0.09734072],\n",
       "       [ 0.09680516,  0.10152374,  0.0919153 ,  0.1039383 ,  0.10818491,\n",
       "         0.10038123,  0.09467276,  0.10357233,  0.10196676,  0.0970395 ],\n",
       "       [ 0.0970437 ,  0.10106388,  0.09163805,  0.10414148,  0.10838468,\n",
       "         0.10028392,  0.09485299,  0.10378497,  0.10188325,  0.09692308],\n",
       "       [ 0.09661337,  0.10112304,  0.0919303 ,  0.10416001,  0.10794839,\n",
       "         0.1003001 ,  0.09511246,  0.10381925,  0.10212831,  0.09686477],\n",
       "       [ 0.09705521,  0.10120554,  0.09154107,  0.10376409,  0.10808102,\n",
       "         0.10069146,  0.09532967,  0.10387351,  0.10158513,  0.0968733 ],\n",
       "       [ 0.09687516,  0.10102101,  0.09181158,  0.10397861,  0.10836969,\n",
       "         0.10061201,  0.09489977,  0.1035761 ,  0.10162138,  0.09723469],\n",
       "       [ 0.09663922,  0.10152438,  0.09189173,  0.10397152,  0.10781228,\n",
       "         0.10016417,  0.09494966,  0.10382311,  0.10186429,  0.09735964],\n",
       "       [ 0.09664584,  0.10089919,  0.09156985,  0.10429182,  0.10833995,\n",
       "         0.10042296,  0.09471814,  0.10389017,  0.10191693,  0.09730516],\n",
       "       [ 0.09652201,  0.10124554,  0.09191083,  0.10410539,  0.10823739,\n",
       "         0.10039389,  0.09453912,  0.10356229,  0.1021389 ,  0.09734464],\n",
       "       [ 0.09690776,  0.10140799,  0.09170888,  0.10343917,  0.10827054,\n",
       "         0.10024932,  0.09501923,  0.10382928,  0.10186741,  0.09730042],\n",
       "       [ 0.09643323,  0.10124238,  0.0917195 ,  0.10441664,  0.10811428,\n",
       "         0.10009769,  0.09486534,  0.10379527,  0.10217633,  0.09713935],\n",
       "       [ 0.09701395,  0.10119279,  0.09173946,  0.10398798,  0.10819722,\n",
       "         0.10046066,  0.09510517,  0.10360149,  0.10171946,  0.09698183],\n",
       "       [ 0.09661361,  0.10064121,  0.09181709,  0.10412816,  0.10851213,\n",
       "         0.10028816,  0.09498891,  0.10355376,  0.10221467,  0.0972423 ],\n",
       "       [ 0.09671022,  0.10127744,  0.09196367,  0.10443642,  0.1076503 ,\n",
       "         0.10041881,  0.09506658,  0.10357566,  0.10174339,  0.09715752],\n",
       "       [ 0.09673802,  0.10083666,  0.09143008,  0.10392226,  0.10866494,\n",
       "         0.10055757,  0.0947733 ,  0.10392024,  0.1021735 ,  0.09698343],\n",
       "       [ 0.09624274,  0.10153946,  0.09150342,  0.10412052,  0.10813028,\n",
       "         0.10051909,  0.09485126,  0.10390104,  0.10160948,  0.09758272],\n",
       "       [ 0.09668812,  0.10167421,  0.09186309,  0.10406734,  0.10798976,\n",
       "         0.10052917,  0.09491487,  0.10351646,  0.10137744,  0.09737954],\n",
       "       [ 0.09684374,  0.10144634,  0.09188389,  0.10380451,  0.10823802,\n",
       "         0.10034086,  0.09532061,  0.10348269,  0.10170377,  0.09693557],\n",
       "       [ 0.09688947,  0.10113445,  0.09157643,  0.10405061,  0.10847166,\n",
       "         0.10033978,  0.09486038,  0.10353379,  0.10202885,  0.09711457],\n",
       "       [ 0.09679144,  0.10122626,  0.09187104,  0.1038974 ,  0.10827861,\n",
       "         0.10036343,  0.09520574,  0.10366133,  0.10172395,  0.0969808 ],\n",
       "       [ 0.09663796,  0.10155546,  0.09176266,  0.1044892 ,  0.1079961 ,\n",
       "         0.1000995 ,  0.09461442,  0.10377605,  0.10190531,  0.09716333],\n",
       "       [ 0.09672527,  0.10102281,  0.09166388,  0.1041605 ,  0.10827165,\n",
       "         0.10037079,  0.09488574,  0.10395333,  0.10187882,  0.0970672 ],\n",
       "       [ 0.09703635,  0.10134354,  0.09168644,  0.1041037 ,  0.10805582,\n",
       "         0.10036163,  0.0948374 ,  0.10339334,  0.10183541,  0.09734636],\n",
       "       [ 0.09666161,  0.10116036,  0.09164252,  0.10432609,  0.1084555 ,\n",
       "         0.09998171,  0.09476336,  0.1037104 ,  0.10216897,  0.09712947]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 매개변수 예측 처리 (순방향)\n",
    "x = np.random.rand(100, 784) # 더미 입력 데이터 100장 \n",
    "y = net.predict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# 기울기 계산\n",
    "x = np.random.rand(100, 784) # 더미 입력 데이터 (100장)\n",
    "t = np.random.rand(100, 10) # 더미 정답 데이터 (100장)\n",
    "\n",
    "grads = net.numerical_gradient(x,t)\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2 미니배치 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tmp.dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60,000 개의 훈련데이터에서 임의 100개 데이터 추림 (이미지, 정답 데이터) <br>\n",
    "확률 적으로 경사 하강법 수행으로 매개변수 갱신<br>\n",
    "갱신 횟수 10,000 번 (반복횟수) <br>\n",
    "갱신 시, 훈련데이터에 대한 손실함수계산, 배열에 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.0986333333333, 0.0958\n",
      "train acc, test acc | 0.788166666667, 0.7918\n",
      "train acc, test acc | 0.879333333333, 0.8826\n",
      "train acc, test acc | 0.897816666667, 0.9012\n",
      "train acc, test acc | 0.908966666667, 0.9118\n",
      "train acc, test acc | 0.914733333333, 0.916\n",
      "train acc, test acc | 0.92025, 0.9222\n",
      "train acc, test acc | 0.924416666667, 0.9248\n",
      "train acc, test acc | 0.928466666667, 0.9279\n",
      "train acc, test acc | 0.93235, 0.9316\n",
      "train acc, test acc | 0.934816666667, 0.935\n",
      "train acc, test acc | 0.937483333333, 0.9376\n",
      "train acc, test acc | 0.940166666667, 0.9389\n",
      "train acc, test acc | 0.941966666667, 0.9399\n",
      "train acc, test acc | 0.94375, 0.9419\n",
      "train acc, test acc | 0.945783333333, 0.9448\n",
      "train acc, test acc | 0.947166666667, 0.9456\n"
     ]
    }
   ],
   "source": [
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.3 시험 데이터로 평가 \n",
    "\n",
    "* epoch : 학습에서 훈련 데이터를 모두 소진했을 떄의 횟수 (10,000개를 100개의 미지배치로 학습한 경우 100회 1epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8MhUs5YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d576737828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 정확도 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
