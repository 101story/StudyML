{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Classifying with Bayesian decision theory\n",
    "\n",
    "* 이전까지는 정보를 가지고 분류 하다가가 확률을 가지고 분류함 \n",
    "\n",
    "- 장점 : 소량의 데이터, 여러 개의 분류 항목을 다룸\n",
    "- 단점 : 입력데이터에 따라 민감함\n",
    "- 적용 : 명목형 값\n",
    "\n",
    "\n",
    "* 어디에 속할 확률이 높은가 여부로 판별함\n",
    "    - if p1(x, y) > p2(x, y), then the class is 1.\n",
    "    - if p2(x, y) > p1(x, y), then the class is 2.\n",
    "\n",
    "\n",
    "* Byes?\n",
    "    * 베이지안확률 이라고하는 분류 사용하기 편하고 인기가 있음 \n",
    "    * 확실하지 않은 상태의 사전지식과 논리를 적용하는 것이 가능 \n",
    "    * 빈발 확률 \n",
    "    * 데이터로부터 결론을 도출해 내고 논리와 사전 지식을 허용하지 않음\n",
    "    \n",
    "    \n",
    "# 4.2 Conditional probability\n",
    "조건부확률\n",
    "\n",
    "P(A|B) = p(A∩B)/p(B)<br>\n",
    "p(A∩B) = p(B|A)*p(A)\n",
    "\n",
    "    -> P(A|B) = p(B|A)*p(A) / p(B)\n",
    "    \n",
    "    \n",
    "# 4.3 Classifying with conditional probabilities\\\n",
    "\n",
    "If P(c1|x, y) > P(c2|x, y), the class is c1. <br>\n",
    "If P(c1|x, y) < P(c2|x, y), the class is c2.\n",
    "\n",
    "\n",
    "\n",
    "# 4.4 Document classification with naïve Bayes\n",
    "\n",
    "\n",
    "* General approach to naïve Bayes\n",
    "    1. Collect: Any method. We’ll use RSS feeds in this chapter.\n",
    "    2. Prepare: Numeric or Boolean values are needed.\n",
    "    3. Analyze: With many features, plotting features isn’t helpful. Looking at histograms is a better idea.\n",
    "    4. Train: Calculate the conditional probabilities of the independent features.\n",
    "    5. Test: Calculate the error rate.\n",
    "    6. Use: One common application of naïve Bayes is document classification. You can use naïve Bayes in any classification setting. It doesn’t have to be text.\n",
    "\n",
    "\n",
    "\n",
    "* 가정\n",
    "    - 모든 feature는 독립적이다. (독립적) \n",
    "    - 모든 feature는 중요도가 같다. \n",
    "\n",
    "\n",
    "\n",
    "# 4.5 Classifying text with Python\n",
    "\n",
    "- token : 문자의 조합, URL, IP ..  (feature : 각 단어)\n",
    "\n",
    "문서에서 속성 (단어사전, 빈도수, 의미 ..) \n",
    "\n",
    "token 을 vector 로 만들고 조건부 확률을 계산 <br>\n",
    "각각의 문서에 대해서 0과 1로 만듬 (폭력적, 비폭력적) <br>\n",
    "나이브 베이스를 수행하기 위한 분류기 생성 \n",
    "\n",
    "\n",
    "## 4.5.1 Prepare: making word vectors from text\n",
    "\n",
    "단어 벡터 : 단어 리스트 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 단어 리스트 만들기               \n",
    "def createVocabList(dataSet):\n",
    "    # 문서의 단어 목록 \n",
    "    vocabSet = set([]) # 집합변수 (비어있는 set을 만듬)\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) \n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cute', 'love', 'help', 'garbage', 'quit']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createVocabList(loadDataSet()[0])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    # [0] 빈 1차원 벡터를 만듬 \n",
    "    returnVec = [0]*len(vocabList)\n",
    "    print(\"\\n returnVec : {}\".format(returnVec))\n",
    "    for word in inputSet:\n",
    "        print(\"\\n word : {}\".format(word))\n",
    "        if word in vocabList:\n",
    "            print(\"   vocabList.index : {}\".format(vocabList.index(word)))\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "        print(\"   returnVec : {}\".format(returnVec))\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " returnVec : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      " word : my\n",
      "   vocabList.index : 31\n",
      "   returnVec : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      " word : dog\n",
      "   vocabList.index : 24\n",
      "   returnVec : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      " word : has\n",
      "   vocabList.index : 18\n",
      "   returnVec : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      " word : flea\n",
      "   vocabList.index : 10\n",
      "   returnVec : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      " word : problems\n",
      "   vocabList.index : 6\n",
      "   returnVec : [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      " word : help\n",
      "   vocabList.index : 2\n",
      "   returnVec : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      " word : please\n",
      "   vocabList.index : 23\n",
      "   returnVec : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postingList,classVec = loadDataSet()\n",
    "vocabList = createVocabList(postingList)\n",
    "setOfWords2Vec(vocabList, postingList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2 Train: calculating probabilities from word vectors\n",
    "\n",
    ">p(c|$W$) : w1 이 나왔을때 문서가 폭력적일 확률 \n",
    "\n",
    "p(c|$W$) = (p($W$|c)p(c)) / p($W$)\n",
    "\n",
    "* $W$ : 단어 벡터 \n",
    "* p($W$) : 그 단어나 문서에서 나타날 확률\n",
    "* p(c) : c1 이 폭력적인 문서일 확률 \n",
    "* p($W$|c) : 폭력적인 문서에서 해당 단어 W1 이 나올 확률 <br>\n",
    "      w0, w1.. wn 사건이 독립적이여서 <br>\n",
    "      p(w0|ci) * p(w1|ci) * .... * p(wn|ci) = p($W$|c) 구할 수 있다. \n",
    "    \n",
    "\n",
    "* 의사 코드 \n",
    "\n",
    "    분류항목에 대한 문서의 개수 세기 \n",
    "    for 훈련을 위한 모든 문서 개수 만큼 반복 \n",
    "        for 분류 항목 개수 만큼 반복\n",
    "            해당 토큰이 문서 내에 있다면 -> 토큰 개수 증가 \n",
    "            토큰에 대한 개수 증가 \n",
    "        for 분류 항목 개수 만큼 반복\n",
    "            for 토큰의 개수만큼 반복\n",
    "              조건부 확률를 구하기 위해 해당 토큰의 개수를 전체 토큰 개수로 나눔\n",
    "    각 분류 항목에 대한 조건부 확률 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_post, class_label = bayes.loadDataSet()\n",
    "my_vocab_list = bayes.createVocabList(list_of_post)\n",
    "\n",
    "trainMat = []\n",
    "for post in list_of_post:\n",
    "    trainMat.append(bayes.setOfWords2Vec(my_vocab_list, post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainMat2 = [bayes.setOfWords2Vec(my_vocab_list, post) for post in list_of_post]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "trainMat == trainMat2\n",
    "print(trainMat2[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 조건부 확률 구하기 (단어의 벡터, 정답 카테고리 )\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix) # 문서 6개 \n",
    "    numWords = len(trainMatrix[0])  # 단어 32개 \n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs) # p(Ci)\n",
    "    print(\"prob. of abusive: {}\".format(pAbusive))\n",
    "    \n",
    "#     p0Num = ones(numWords); p1Num = ones(numWords) # 분자 # 32개 어휘 단어   \n",
    "#     p0Denom = 2.0; p1Denom = 2.0  # 분모\n",
    "    p0Num = np.zeros(numWords); p1Num = np.zeros(numWords) # 분자 # 32개 어휘 단어   \n",
    "    p0Denom = 0.0; p1Denom = 0.0  # 분모\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        print(\"\\n -- for -- \")\n",
    "        print(\"  trainCategory[{}] : {}\".format(i, trainCategory[i]))\n",
    "        # 글이 폭력적일떄\n",
    "        if trainCategory[i] == 1: \n",
    "            #해당단어가 몇번 나왔는지  \n",
    "            p1Num += trainMatrix[i]\n",
    "            # 폭력적인 문서의 단어 개수 \n",
    "            p1Denom += sum(trainMatrix[i])            \n",
    "            print(\"    p1 NUM :{} \\n    Denom :{}\".format(p1Num, p1Denom))\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "            print(\"    p0 NUM :{} \\n    Denom :{}\".format(p0Num, p0Denom))\n",
    "    \n",
    "    # 32개 전체단어중에 단어별로 몇번 등장했는지 (에 로그)\n",
    "    p1Vect = p1Num/p1Denom #np.log(p1Num/p1Denom) # p(Wi|c1)\n",
    "    p0Vect = p0Num/p0Denom #np.log(p0Num/p0Denom) # p(Wi|c0)\n",
    "    print(\"\\n p1Vect : {}\".format(p1Vect))\n",
    "    print(\"\\n p0Vect : {}\".format(p0Vect))\n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "\n",
    "# 참고) p(Wi|c1) + p(Wi|c0) 한다고 1이 되지 않음 (조건부확률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob. of abusive: 0.5\n",
      "\n",
      " -- for -- \n",
      "  trainCategory[0] : 0\n",
      "    p0 NUM :[ 0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  1.] \n",
      "    Denom :7.0\n",
      "\n",
      " -- for -- \n",
      "  trainCategory[1] : 1\n",
      "    p1 NUM :[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.\n",
      "  0.  0.  0.  1.  1.  0.  1.  0.  1.  0.  1.  0.  0.  0.] \n",
      "    Denom :8.0\n",
      "\n",
      " -- for -- \n",
      "  trainCategory[2] : 0\n",
      "    p0 NUM :[ 1.  1.  1.  0.  0.  1.  1.  1.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.\n",
      "  1.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.  2.] \n",
      "    Denom :15.0\n",
      "\n",
      " -- for -- \n",
      "  trainCategory[3] : 1\n",
      "    p1 NUM :[ 0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.\n",
      "  0.  1.  0.  1.  1.  0.  1.  0.  2.  0.  1.  0.  0.  0.] \n",
      "    Denom :13.0\n",
      "\n",
      " -- for -- \n",
      "  trainCategory[4] : 0\n",
      "    p0 NUM :[ 1.  1.  1.  0.  0.  1.  1.  1.  0.  1.  1.  1.  1.  0.  0.  2.  0.  0.\n",
      "  1.  0.  1.  1.  0.  1.  1.  1.  0.  1.  0.  1.  1.  3.] \n",
      "    Denom :24.0\n",
      "\n",
      " -- for -- \n",
      "  trainCategory[5] : 1\n",
      "    p1 NUM :[ 0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.\n",
      "  0.  2.  0.  1.  1.  0.  2.  0.  3.  0.  1.  0.  0.  0.] \n",
      "    Denom :19.0\n",
      "\n",
      " p1Vect : [ 0.          0.          0.          0.05263158  0.05263158  0.          0.\n",
      "  0.          0.05263158  0.05263158  0.          0.          0.\n",
      "  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158  0.\n",
      "  0.10526316  0.          0.05263158  0.05263158  0.          0.10526316\n",
      "  0.          0.15789474  0.          0.05263158  0.          0.          0.        ]\n",
      "\n",
      " p0Vect : [ 0.04166667  0.04166667  0.04166667  0.          0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.04166667  0.04166667\n",
      "  0.04166667  0.          0.          0.08333333  0.          0.\n",
      "  0.04166667  0.          0.04166667  0.04166667  0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.          0.04166667\n",
      "  0.04166667  0.125     ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.15789473684210525)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainMat\n",
    "# class_label\n",
    "p0Vect,p1Vect,pAbusive = trainNB0(trainMat, class_label)\n",
    "p0Vect[26], p1Vect[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* -- for -- <br>\n",
    "trainCategory[5] : 1 <br>\n",
    "    p1 NUM :[ 0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1. 0.  2.  0.  1.  1.  0.  2.  0.  3.  0.  1.  0.  0.  0.] <br>\n",
    "    Denom :19.0\n",
    "\n",
    "확률로 폭력적인 문장에서 나오는 단어들 19개 (2번 나온 단어, 3번 나온 단어 stupid)\n",
    "\n",
    "\n",
    "* \n",
    "p1Vect : [ 0.          0.          0.          0.05263158  0.05263158  0.          0.  0.          0.05263158  0.05263158  0.          0.          0.  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158  0.   0.10526316  0.         0.05263158  0.05263158  0.          0.10526316  0.          0.15789474  0.          0.05263158  0.          0.          0.        ]\n",
    "  \n",
    "폭력적인 문서에서 1번째 단어가 나올 확률은 p(W1|c1)=0\n",
    "\n",
    "stupid 단어가 각 문서에서 확률 (0.0, 0.15789473684210525)\n",
    "\n",
    "pAbusive : p(C1)\n",
    "\n",
    "* p( $W$ ) 분모는 어짜피 비교하는 값이라 같은건 구할필요 없어서 구하지 않음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.3 Test: modifying the classifier for real-world conditions\n",
    "\n",
    "실행활에 맞추기 위해서 분류기를 수정함\n",
    "\n",
    "* trainNB0 <br>\n",
    "확률이 0인 경우가 있어서 확률을 곱하면 전체 확률이 0가 됨 <br>\n",
    "모든 단어가 적어도 한번은 나오는 것을 가정해서 확률이 0인것을 없게 함\n",
    "> p0Num = ones(numWords); p1Num = ones(numWords) # 분자 # 32개 어휘 단어 \n",
    "\n",
    "    모든단어를 1로 초기화했을 때 입력받은 문장이 하나도 없는 경우 <br>\n",
    "    모든 1/2 가 되어서 두가지 분류 기준에 어디든 들어갈수 있게 만듬 \n",
    "> p0Denom = 2.0; p1Denom = 2.0  # 다른단어에 영향을 주지 않고 영향력을 줄임 \n",
    "\n",
    "\n",
    "* 언더플로우 : 작은 숫자끼리 너무 많이 곱해서 발행하는 문제 (0.1 * 0.000002 * ...)\n",
    "\n",
    "** 이러한 문제를 없애기 위해 log 를 씌움 **\n",
    "- 큰 스케일 값을 작게 표현하기 위해서 ($log_{10} 10=1$, $log_{10} 100=2$)\n",
    "- 곱샘이 -> 덧샘으로 변하게 되어 계산이 편해짐 (dB 데시벨, 지진규모 ..)\n",
    "- 증감의 범위나 분포가 바뀌지 않음\n",
    "    > p1Vect = np.log(p1Num/p1Denom) # p(Wi|c1)<br>\n",
    "    p0Vect = np.log(p0Num/p0Denom) # p(Wi|c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'bayes' from 'bayes.py'>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.2580965380214821, -1.6582280766035324)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변경된 train 함수 \n",
    "p0Vect,p1Vect,pAbusive = bayes.trainNB0(trainMat, class_label)\n",
    "p0Vect[26], p1Vect[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 검증 (분류하고 싶은 문서의 단어벡터, 폭력적일때 단어확률, 비폭력적일때 단어확률, 폭력적인 문석일확률)\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    # log 값들의 연산이라 * -> + 으로 연산함 \n",
    "    # sum(vec2Classify * p1Vec) == p(w0|ci) * p(w1|ci) * .... * p(wn|ci) = p($W$|c)\n",
    "    # log(pClass1) == p(C1)\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)    \n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    \n",
    "    print(\"\\nvec2Classify :{}\".format(vec2Classify))\n",
    "    print(\"\\np1Vec :{}\".format(p1Vec))\n",
    "    print(\"\\np0Vec :{}\".format(p0Vec))\n",
    "    print(\"\\npClass1 :{}\".format(pClass1))\n",
    "    print(\"\\np1 :{} p0 :{}\".format(p1,p0))\n",
    "    \n",
    "    # p(C1|W) > p(C0|W)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word: dumped is not in my Vocabulary!\n",
      "the word: for is not in my Vocabulary!\n",
      "\n",
      "vec2Classify :[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "p1Vec :[-3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -2.35137526 -2.35137526 -2.35137526\n",
      " -3.04452244 -1.94591015 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -1.94591015 -3.04452244 -1.65822808 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244]\n",
      "\n",
      "p0Vec :[-2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -3.25809654 -2.15948425 -3.25809654 -3.25809654\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -1.87180218]\n",
      "\n",
      "pClass1 :0.5\n",
      "\n",
      "p1 :-14.1239998999 p0 :-13.5178939679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0v,p1v,pAbus = bayes.trainNB0(trainMat, class_label)\n",
    "testEntry =  ['I', 'dumped', 'garbage', 'for', 'my', 'cute', 'dog']\n",
    "testVec = bayes.setOfWords2Vec(my_vocab_list, testEntry)\n",
    "classifyNB(testVec, p0v, p1v, pAbus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = bayes.createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(bayes.setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = bayes.trainNB0(np.array(trainMat),np.array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(bayes.setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ', bayes.classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(bayes.setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ', bayes.classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "\n",
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.4 Prepare: the bag-of-words document model\n",
    "\n",
    "\n",
    "중복단어모델 (bag Of Words)\n",
    "\n",
    "> setOfWords2Vec : 단어가 여러번 나와도 한번으로 치부함 (단어의집합)\n",
    "\n",
    "> bagOfWords2VecMN : 중복되어 나온 단어의 값을 + 여 처리 \n",
    "\n",
    "단어가 여러번 나오면 중요도가 높다고 생각 할 때\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1 #<- 요기 다름 \n",
    "    return returnVec\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Example: classifying spam email with naïve Bayes\n",
    "\n",
    "## 4.6.1 Prepare: tokenizing text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'python',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySent='This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "\n",
    "# 정규표현식을 이용 하여 문장 -> 단어 분류 \n",
    "# \\W : white space (공백, ',', '.', \\n, \\r, \\t..)\n",
    "def textParse(bigString):    \n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)  # white space\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] # 2글자 이상, 소문자로 변경 \n",
    "textParse(mySent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'since',\n",
       " 'you',\n",
       " 'are',\n",
       " 'owner',\n",
       " 'least',\n",
       " 'one',\n",
       " 'google',\n",
       " 'groups',\n",
       " 'group',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'customized',\n",
       " 'welcome',\n",
       " 'message',\n",
       " 'pages',\n",
       " 'files',\n",
       " 'are',\n",
       " 'writing',\n",
       " 'inform',\n",
       " 'you',\n",
       " 'that',\n",
       " 'will',\n",
       " 'longer',\n",
       " 'supporting',\n",
       " 'these',\n",
       " 'features',\n",
       " 'starting',\n",
       " 'february',\n",
       " '2011',\n",
       " 'made',\n",
       " 'this',\n",
       " 'decision',\n",
       " 'that',\n",
       " 'can',\n",
       " 'focus',\n",
       " 'improving',\n",
       " 'the',\n",
       " 'core',\n",
       " 'functionalities',\n",
       " 'google',\n",
       " 'groups',\n",
       " 'mailing',\n",
       " 'lists',\n",
       " 'and',\n",
       " 'forum',\n",
       " 'discussions',\n",
       " 'instead',\n",
       " 'these',\n",
       " 'features',\n",
       " 'encourage',\n",
       " 'you',\n",
       " 'use',\n",
       " 'products',\n",
       " 'that',\n",
       " 'are',\n",
       " 'designed',\n",
       " 'specifically',\n",
       " 'for',\n",
       " 'file',\n",
       " 'storage',\n",
       " 'and',\n",
       " 'page',\n",
       " 'creation',\n",
       " 'such',\n",
       " 'google',\n",
       " 'docs',\n",
       " 'and',\n",
       " 'google',\n",
       " 'sites',\n",
       " 'for',\n",
       " 'example',\n",
       " 'you',\n",
       " 'can',\n",
       " 'easily',\n",
       " 'create',\n",
       " 'your',\n",
       " 'pages',\n",
       " 'google',\n",
       " 'sites',\n",
       " 'and',\n",
       " 'share',\n",
       " 'the',\n",
       " 'site',\n",
       " 'http',\n",
       " 'www',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'sites',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'answer',\n",
       " '174623',\n",
       " 'with',\n",
       " 'the',\n",
       " 'members',\n",
       " 'your',\n",
       " 'group',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'store',\n",
       " 'your',\n",
       " 'files',\n",
       " 'the',\n",
       " 'site',\n",
       " 'attaching',\n",
       " 'files',\n",
       " 'pages',\n",
       " 'http',\n",
       " 'www',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'sites',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'answer',\n",
       " '90563',\n",
       " 'the',\n",
       " 'site',\n",
       " 'you',\n",
       " 'just',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'place',\n",
       " 'upload',\n",
       " 'your',\n",
       " 'files',\n",
       " 'that',\n",
       " 'your',\n",
       " 'group',\n",
       " 'members',\n",
       " 'can',\n",
       " 'download',\n",
       " 'them',\n",
       " 'suggest',\n",
       " 'you',\n",
       " 'try',\n",
       " 'google',\n",
       " 'docs',\n",
       " 'you',\n",
       " 'can',\n",
       " 'upload',\n",
       " 'files',\n",
       " 'http',\n",
       " 'docs',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'answer',\n",
       " '50092',\n",
       " 'and',\n",
       " 'share',\n",
       " 'access',\n",
       " 'with',\n",
       " 'either',\n",
       " 'group',\n",
       " 'http',\n",
       " 'docs',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'answer',\n",
       " '66343',\n",
       " 'individual',\n",
       " 'http',\n",
       " 'docs',\n",
       " 'google',\n",
       " 'com',\n",
       " 'support',\n",
       " 'bin',\n",
       " 'answer',\n",
       " 'answer',\n",
       " '86152',\n",
       " 'assigning',\n",
       " 'either',\n",
       " 'edit',\n",
       " 'download',\n",
       " 'only',\n",
       " 'access',\n",
       " 'the',\n",
       " 'files',\n",
       " 'you',\n",
       " 'have',\n",
       " 'received',\n",
       " 'this',\n",
       " 'mandatory',\n",
       " 'email',\n",
       " 'service',\n",
       " 'announcement',\n",
       " 'update',\n",
       " 'you',\n",
       " 'about',\n",
       " 'important',\n",
       " 'changes',\n",
       " 'google',\n",
       " 'groups']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailText = open('data/ch04/email/ham/6.txt').read()\n",
    "textParse(emailText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.2 Test: cross validation with naïve Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 스팸 분류기 \n",
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    # 인스턴스 25개 읽기\n",
    "    for i in range(1,26):\n",
    "        # 바로 분류 시킴 \n",
    "        wordList = bayes.textParse(open('data/ch04/email/spam/%d.txt' % i).read())\n",
    "        # 배열로 추가함\n",
    "        docList.append(wordList)\n",
    "        # 한문장으로 모든 스팸 인트턴스 넣음 \n",
    "        fullText.extend(wordList)\n",
    "        # 스팸\n",
    "        classList.append(1)\n",
    "        \n",
    "        wordList = bayes.textParse(open('data/ch04/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    \n",
    "    # docList 이차원 배열 \n",
    "    vocabList = bayes.createVocabList(docList) # 단어사전 만들기 \n",
    "    trainingSet = range(50); testSet=[]  #0~49 까지 index 리스트 \n",
    "    \n",
    "    # test 에 사용할 인스턴스 추출\n",
    "    for i in range(10):\n",
    "        # uniform : 균등분포로 난수 발생 (int 로 정수화시킴)\n",
    "        randIndex = int(np.random.uniform(0,len(trainingSet))) \n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        # test 데이터로 선정된 값 삭제\n",
    "        del(trainingSet[randIndex])  \n",
    "    \n",
    "    trainMat=[]; trainClasses = []\n",
    "    # trainingSet : training data 의 index 값 \n",
    "    for docIndex in trainingSet:\n",
    "        # docIndex 모든 이메일에 있는 단어들 리스트 \n",
    "        # 단어가 나온 횟수만큼 count\n",
    "        trainMat.append(bayes.bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "      \n",
    "    p0V,p1V,pSpam = bayes.trainNB0(np.array(trainMat),np.array(trainClasses))\n",
    "    print(\"p0v: {} \\np1v: {} \\nspam: {}\\n\".format(p0V[:10],p1V[:10],pSpam))\n",
    "    \n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bayes.bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if bayes.classifyNB(np.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print(\"classification error \",docList[docIndex])\n",
    "            print(\"correct answer \", classList[docIndex])\n",
    "    print('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    #return vocabList,fullText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 49, 31, 12, 10, 33, 11, 49, 39, 23]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원래 다른 값인데 int 하면서 같은 값이 됨\n",
    "[int(np.random.uniform(0,50)) for _ in range(10)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'bayes' from 'bayes.pyc'>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0v: [-5.31811999 -6.70441435 -6.01126717 -6.01126717 -6.70441435 -6.70441435\n",
      " -6.01126717 -6.01126717 -6.01126717 -6.70441435] \n",
      "p1v: [-4.95053137 -6.33682573 -6.33682573 -6.33682573 -4.95053137 -5.64367855\n",
      " -6.33682573 -6.33682573 -6.33682573 -4.13960115] \n",
      "spam: 0.45\n",
      "\n",
      "classification error  ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "correct answer  0\n",
      "classification error  ['scifinance', 'now', 'automatically', 'generates', 'gpu', 'enabled', 'pricing', 'risk', 'model', 'source', 'code', 'that', 'runs', '300x', 'faster', 'than', 'serial', 'code', 'using', 'new', 'nvidia', 'fermi', 'class', 'tesla', 'series', 'gpu', 'scifinance', 'derivatives', 'pricing', 'and', 'risk', 'model', 'development', 'tool', 'that', 'automatically', 'generates', 'and', 'gpu', 'enabled', 'source', 'code', 'from', 'concise', 'high', 'level', 'model', 'specifications', 'parallel', 'computing', 'cuda', 'programming', 'expertise', 'required', 'scifinance', 'automatic', 'gpu', 'enabled', 'monte', 'carlo', 'pricing', 'model', 'source', 'code', 'generation', 'capabilities', 'have', 'been', 'significantly', 'extended', 'the', 'latest', 'release', 'this', 'includes']\n",
      "correct answer  0\n",
      "the error rate is:  0.2\n"
     ]
    }
   ],
   "source": [
    "reload(bayes)\n",
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spam 메일의 확률에 error rate 가 크게 영향을 받진 않았다.\n",
    "\n",
    "신규 데이터가 들어오면 신규데이터가 추가된 모든 데이터를 다시 학습해야한다. \n",
    "\n",
    "    >> Knowledge Representation : p0V, p1V, pSpam \n",
    "    \n",
    "참고) 신경망에서는 노드들 간의 연결정보를 조절하는데 훈련된 데이터를 다음 epoch 에 사용해도 큰 영향을 주지 않는다. (cross-validataion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.7 Example: using naïve Bayes to reveal local attitudes from personal ads\n",
    "\n",
    "* RSS LIST 사이트 <br>\n",
    "https://newyork.craigslist.org/\n",
    "\n",
    "개인광고에 포함된 지역의 특색 도출 \n",
    "\n",
    "## 4.7.1 Collect: importing RSS feeds\n",
    "\n",
    "* rss feed parser 설치 <br>\n",
    "    http://code.google.com/p/feedparser/ <br>\n",
    "    (C:\\Anaconda3) C:\\Users\\student>conda install -n ml27 feedparser\n",
    "\n",
    "\n",
    "* stop word (중지단어) : usually the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "u\"average guy seeking a cool female friend that I can chat with laugh bug out and just be ourselves around each other. Someone who has a good head on a shoulders that's funny mature and cool to hang with it doesnt matter what borough you're from just b ...\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "ny = feedparser.parse(\"https://newyork.craigslist.org/search/stp?format=rss\")\n",
    "len(ny)\n",
    "ny['entries'][0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list 에서 가장 많이 나온 상위 30 개만 \n",
    "def calcMostFreq(vocabList,fullText, num):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    \n",
    "    #value 값으로 정렬 내림차순 \n",
    "    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True) #py3 freqDict\n",
    "    return sortedFreq[:num]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    \n",
    "    # 두개를 비교하기 위해 최소값로 비교 \n",
    "    for i in range(minLen):\n",
    "        # 요약 정보만 \n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "        \n",
    "    # 모든 단어 리스트 \n",
    "    vocabList = bayes.createVocabList(docList)#create vocabulary\n",
    "    # 가장 빈번하게 나온것 \n",
    "    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n",
    "    \n",
    "    # 상위 30 개의 단어를 단어리스트에서 제거 (for, you, the, ... )\n",
    "    for pairW in top30Words:\n",
    "        # 사전의 pairW = key, value\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    \n",
    "    # ny 30 개, sf 25 개에서 각각 25개씩 50개 가져옴\n",
    "    trainingSet = range(2*minLen); testSet=[]           #create test set\n",
    "    \n",
    "    # test set 20개\n",
    "    for i in range(20):\n",
    "        randIndex = int(np.random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    \n",
    "    trainMat=[]; trainClasses = []\n",
    "    # training matrix 만들기 \n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bayes.bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    \n",
    "    # training 시키기 pSpam : 뉴욕인 확률 \n",
    "    p0V,p1V,pSpam = bayes.trainNB0(np.array(trainMat),np.array(trainClasses))\n",
    "    \n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bayes.bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if bayes.classifyNB(np.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print('the error is: ',docList[docIndex])\n",
    "            print('the correct is: ',classList[docIndex])\n",
    "    print('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'bayes' from 'bayes.pyc'>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the error rate is: ', 0.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-5.6347896 , -5.6347896 , -4.94164242, -5.6347896 , -5.6347896 ,\n",
       "       -4.94164242, -5.6347896 , -5.6347896 , -5.6347896 , -5.6347896 ])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-5.86646806, -5.86646806, -5.86646806, -5.86646806, -5.86646806,\n",
       "       -5.86646806, -4.76785577, -5.86646806, -4.4801737 , -4.76785577])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(bayes)\n",
    "ny = feedparser.parse(\"https://newyork.craigslist.org/search/stp?format=rss\")\n",
    "sf = feedparser.parse(\"https://sfbay.craigslist.org/search/stp?format=rss\")\n",
    "vocabList, pSF, pNY = bayes.localWords(ny, sf)\n",
    "len(vocabList)\n",
    "pSF[:10]\n",
    "pNY[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.2 Analyze: displaying locally used words\n",
    "\n",
    "지역적으로 사용된 단어 표시 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny,sf):\n",
    "    import operator\n",
    "    vocabList,p0V,p1V=bayes.localWords(ny,sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sortedSF:\n",
    "        print(item[0])\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n",
    "    for item in sortedNY:\n",
    "        print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the error rate is: ', 0.3)\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "long\n",
      "friend\n",
      "male\n",
      "doing\n",
      "think\n",
      "make\n",
      "safe\n",
      "lady\n",
      "females\n",
      "women\n",
      "orral\n",
      "want\n",
      "pleasure\n",
      "good\n",
      "got\n",
      "sensual\n",
      "face\n",
      "clean\n",
      "please\n",
      "only\n",
      "feels\n",
      "bay\n",
      "come\n",
      "many\n",
      "blonde\n",
      "wants\n",
      "things\n",
      "other\n",
      "host\n",
      "drama\n",
      "range\n",
      "your\n",
      "drive\n",
      "interested\n",
      "newbie\n",
      "asian\n",
      "skin\n",
      "passion\n",
      "conversating\n",
      "friendly\n",
      "woman\n",
      "confounding\n",
      "drinking\n",
      "accidentally\n",
      "discrete\n",
      "enjoy\n",
      "hanging\n",
      "what\n",
      "giving\n",
      "while\n",
      "lights\n",
      "new\n",
      "here\n",
      "hours\n",
      "wherever\n",
      "family\n",
      "prefer\n",
      "husbands\n",
      "strrings\n",
      "takes\n",
      "lickking\n",
      "few\n",
      "high\n",
      "room\n",
      "hour\n",
      "this\n",
      "ride\n",
      "movies\n",
      "meet\n",
      "minimum\n",
      "lacks\n",
      "sit\n",
      "hou\n",
      "possibly\n",
      "beauty\n",
      "may\n",
      "such\n",
      "maybe\n",
      "wine\n",
      "hobby\n",
      "into\n",
      "they\n",
      "now\n",
      "society\n",
      "event\n",
      "special\n",
      "receiving\n",
      "free\n",
      "struggle\n",
      "teach\n",
      "definition\n",
      "times\n",
      "place\n",
      "eatting\n",
      "feel\n",
      "thick\n",
      "little\n",
      "families\n",
      "their\n",
      "white\n",
      "foreplay\n",
      "hotel\n",
      "believe\n",
      "herself\n",
      "than\n",
      "boyfriend\n",
      "matter\n",
      "thhongs\n",
      "loves\n",
      "look\n",
      "take\n",
      "seductive\n",
      "sure\n",
      "serviced\n",
      "most\n",
      "but\n",
      "don\n",
      "disease\n",
      "judgmental\n",
      "find\n",
      "busy\n",
      "going\n",
      "black\n",
      "pretty\n",
      "his\n",
      "photography\n",
      "cannot\n",
      "married\n",
      "sex\n",
      "individual\n",
      "ways\n",
      "pictures\n",
      "gentleman\n",
      "horny\n",
      "email\n",
      "taking\n",
      "etc\n",
      "path\n",
      "been\n",
      "photos\n",
      "non\n",
      "ready\n",
      "different\n",
      "pay\n",
      "firearms\n",
      "hang\n",
      "hand\n",
      "keep\n",
      "off\n",
      "hubby\n",
      "less\n",
      "sporting\n",
      "weekly\n",
      "blow\n",
      "pleasuring\n",
      "womans\n",
      "had\n",
      "has\n",
      "around\n",
      "desi\n",
      "lost\n",
      "message\n",
      "works\n",
      "nude\n",
      "hair\n",
      "duration\n",
      "super\n",
      "mixed\n",
      "female\n",
      "lingerrie\n",
      "area\n",
      "there\n",
      "hey\n",
      "lonely\n",
      "happy\n",
      "boyf\n",
      "girlfriends\n",
      "info\n",
      "videography\n",
      "happen\n",
      "mature\n",
      "cit\n",
      "similar\n",
      "moved\n",
      "life\n",
      "right\n",
      "door\n",
      "shooting\n",
      "thinks\n",
      "artistic\n",
      "lend\n",
      "longer\n",
      "age\n",
      "alw\n",
      "someplace\n",
      "buddy\n",
      "technique\n",
      "young\n",
      "send\n",
      "charge\n",
      "brown\n",
      "jerk\n",
      "egyptian\n",
      "very\n",
      "every\n",
      "bestie\n",
      "cool\n",
      "did\n",
      "gender\n",
      "standards\n",
      "try\n",
      "race\n",
      "small\n",
      "says\n",
      "erotic\n",
      "portfolio\n",
      "shiatsu\n",
      "drinks\n",
      "even\n",
      "sun\n",
      "learned\n",
      "ever\n",
      "told\n",
      "full\n",
      "shoreline\n",
      "exchange\n",
      "men\n",
      "shouldn\n",
      "let\n",
      "sexy\n",
      "safer\n",
      "great\n",
      "kids\n",
      "healthy\n",
      "pics\n",
      "social\n",
      "existent\n",
      "massage\n",
      "private\n",
      "wit\n",
      "use\n",
      "two\n",
      "37yr\n",
      "type\n",
      "separated\n",
      "relax\n",
      "phone\n",
      "include\n",
      "work\n",
      "cuddle\n",
      "spontaneous\n",
      "topic\n",
      "something\n",
      "court\n",
      "rather\n",
      "six\n",
      "how\n",
      "swore\n",
      "designer\n",
      "watch\n",
      "after\n",
      "adventure\n",
      "suck\n",
      "meaningful\n",
      "man\n",
      "stress\n",
      "short\n",
      "african\n",
      "playing\n",
      "talk\n",
      "cute\n",
      "help\n",
      "over\n",
      "looki\n",
      "still\n",
      "its\n",
      "perfect\n",
      "group\n",
      "better\n",
      "occasionally\n",
      "might\n",
      "then\n",
      "them\n",
      "tonig\n",
      "fling\n",
      "practice\n",
      "break\n",
      "indulge\n",
      "easily\n",
      "name\n",
      "always\n",
      "didn\n",
      "each\n",
      "friendship\n",
      "weight\n",
      "hard\n",
      "related\n",
      "our\n",
      "girl\n",
      "36dd\n",
      "attend\n",
      "motel\n",
      "weeekend\n",
      "rubdown\n",
      "mess\n",
      "hangouts\n",
      "newly\n",
      "shows\n",
      "imagine\n",
      "put\n",
      "wanted\n",
      "care\n",
      "could\n",
      "american\n",
      "loud\n",
      "first\n",
      "yourself\n",
      "feet\n",
      "another\n",
      "tomorrow\n",
      "size\n",
      "start\n",
      "twenty\n",
      "top\n",
      "girls\n",
      "least\n",
      "anyone\n",
      "too\n",
      "legs\n",
      "inner\n",
      "eyes\n",
      "relationship\n",
      "park\n",
      "grocery\n",
      "unfortunately\n",
      "boring\n",
      "marriage\n",
      "were\n",
      "suspicious\n",
      "ages\n",
      "tonight\n",
      "talking\n",
      "say\n",
      "need\n",
      "borough\n",
      "any\n",
      "built\n",
      "dishes\n",
      "able\n",
      "also\n",
      "finding\n",
      "wanting\n",
      "hello\n",
      "tall\n",
      "queens\n",
      "garments\n",
      "considered\n",
      "average\n",
      "soho\n",
      "movie\n",
      "hear\n",
      "cuddled\n",
      "walking\n",
      "cheat\n",
      "session\n",
      "bedroom\n",
      "earth\n",
      "one\n",
      "based\n",
      "nice\n",
      "title\n",
      "situations\n",
      "hope\n",
      "wearing\n",
      "hit\n",
      "get\n",
      "hopefully\n",
      "stop\n",
      "reply\n",
      "during\n",
      "barneys\n",
      "him\n",
      "runs\n",
      "spots\n",
      "bad\n",
      "stuff\n",
      "she\n",
      "release\n",
      "husband\n",
      "view\n",
      "ballroom\n",
      "see\n",
      "college\n",
      "techniques\n",
      "said\n",
      "outside\n",
      "between\n",
      "reading\n",
      "screen\n",
      "attention\n",
      "subject\n",
      "coffee\n",
      "key\n",
      "modeling\n",
      "figured\n",
      "limits\n",
      "became\n",
      "asked\n",
      "comes\n",
      "prospect\n",
      "afterwards\n",
      "point\n",
      "whatever\n",
      "laugh\n",
      "openness\n",
      "trust\n",
      "ourselves\n",
      "arts\n",
      "pocketbook\n",
      "honeyraexo\n",
      "much\n",
      "bbw\n",
      "shoulders\n",
      "630pm\n",
      "shopping\n",
      "relaxing\n",
      "drugs\n",
      "puerto\n",
      "fellow\n",
      "volunteer\n",
      "case\n",
      "informatio\n",
      "crowds\n",
      "gucci\n",
      "hates\n",
      "fun\n",
      "shops\n",
      "almost\n",
      "strain\n",
      "partner\n",
      "trade\n",
      "spaces\n",
      "funny\n",
      "paying\n",
      "perhaps\n",
      "seeking\n",
      "same\n",
      "advise\n",
      "gets\n",
      "week\n",
      "libido\n",
      "drink\n",
      "kik\n",
      "louis\n",
      "older\n",
      "well\n",
      "smoker\n",
      "thought\n",
      "person\n",
      "contact\n",
      "model\n",
      "spend\n",
      "know\n",
      "being\n",
      "money\n",
      "foot\n",
      "years\n",
      "guys\n",
      "dining\n",
      "except\n",
      "kick\n",
      "real\n",
      "big\n",
      "honest\n",
      "matters\n",
      "discreet\n",
      "sephora\n",
      "panties\n",
      "pounds\n",
      "buddies\n",
      "benefit\n",
      "night\n",
      "kingsman\n",
      "because\n",
      "old\n",
      "deal\n",
      "pals\n",
      "some\n",
      "back\n",
      "martial\n",
      "hassle\n",
      "highlinr\n",
      "rican\n",
      "heartache\n",
      "tube\n",
      "pen\n",
      "asking\n",
      "sneakers\n",
      "dating\n",
      "business\n",
      "asap\n",
      "rub\n",
      "lose\n",
      "exciting\n",
      "vuitton\n",
      "anything\n",
      "getting\n",
      "duo\n",
      "favorite\n",
      "plus\n",
      "afternoon\n",
      "road\n",
      "ladies\n",
      "own\n",
      "nsa\n",
      "image\n",
      "commitment\n",
      "socks\n",
      "down\n",
      "doesnt\n",
      "cent\n",
      "sports\n",
      "her\n",
      "submit\n",
      "strict\n",
      "low\n",
      "lot\n",
      "was\n",
      "head\n",
      "plunge\n",
      "jus\n",
      "link\n",
      "line\n",
      "bug\n",
      "buying\n",
      "handsome\n",
      "attached\n",
      "hosting\n",
      "cursed\n",
      "called\n",
      "videos\n",
      "doesn\n",
      "single\n",
      "chat\n",
      "nerd\n",
      "abyss\n",
      "when\n",
      "really\n",
      "problems\n",
      "diet\n",
      "chance\n",
      "missed\n",
      "calls\n",
      "wife\n",
      "having\n",
      "oral\n",
      "noon\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "stress\n",
      "take\n",
      "relaxing\n",
      "ever\n",
      "full\n",
      "massage\n",
      "over\n",
      "rubdown\n",
      "come\n",
      "old\n",
      "your\n",
      "technique\n",
      "send\n",
      "charge\n",
      "very\n",
      "says\n",
      "let\n",
      "break\n",
      "weight\n",
      "special\n",
      "size\n",
      "built\n",
      "but\n",
      "session\n",
      "title\n",
      "she\n",
      "release\n",
      "email\n",
      "strain\n",
      "week\n",
      "well\n",
      "foot\n",
      "back\n",
      "down\n",
      "there\n",
      "oral\n",
      "small\n",
      "kids\n",
      "pics\n",
      "two\n",
      "phone\n",
      "work\n",
      "meet\n",
      "african\n",
      "into\n",
      "our\n",
      "free\n",
      "times\n",
      "american\n",
      "girls\n",
      "white\n",
      "were\n",
      "please\n",
      "only\n",
      "get\n",
      "husband\n",
      "point\n",
      "arts\n",
      "bbw\n",
      "fun\n",
      "partner\n",
      "make\n",
      "being\n",
      "years\n",
      "guys\n",
      "had\n",
      "message\n",
      "works\n",
      "some\n",
      "martial\n",
      "anything\n",
      "favorite\n",
      "single\n",
      "when\n",
      "interested\n",
      "diet\n",
      "young\n",
      "brown\n",
      "woman\n",
      "every\n",
      "bestie\n",
      "cool\n",
      "standards\n",
      "try\n",
      "race\n",
      "portfolio\n",
      "what\n",
      "sun\n",
      "learned\n",
      "exchange\n",
      "shouldn\n",
      "sexy\n",
      "safer\n",
      "great\n",
      "healthy\n",
      "existent\n",
      "family\n",
      "37yr\n",
      "relax\n",
      "include\n",
      "hour\n",
      "want\n",
      "court\n",
      "six\n",
      "how\n",
      "possibly\n",
      "swore\n",
      "designer\n",
      "may\n",
      "suck\n",
      "man\n",
      "short\n",
      "maybe\n",
      "playing\n",
      "help\n",
      "still\n",
      "perfect\n",
      "occasionally\n",
      "them\n",
      "tonig\n",
      "practice\n",
      "indulge\n",
      "now\n",
      "didn\n",
      "friendship\n",
      "hard\n",
      "related\n",
      "girl\n",
      "36dd\n",
      "event\n",
      "attend\n",
      "motel\n",
      "newly\n",
      "shows\n",
      "imagine\n",
      "wanted\n",
      "care\n",
      "first\n",
      "feet\n",
      "long\n",
      "start\n",
      "twenty\n",
      "least\n",
      "friend\n",
      "eyes\n",
      "relationship\n",
      "park\n",
      "believe\n",
      "than\n",
      "grocery\n",
      "unfortunately\n",
      "boring\n",
      "matter\n",
      "marriage\n",
      "suspicious\n",
      "ages\n",
      "tonight\n",
      "need\n",
      "borough\n",
      "dishes\n",
      "hello\n",
      "most\n",
      "queens\n",
      "don\n",
      "soho\n",
      "walking\n",
      "cheat\n",
      "bedroom\n",
      "earth\n",
      "nice\n",
      "hit\n",
      "stop\n",
      "reply\n",
      "during\n",
      "barneys\n",
      "runs\n",
      "ballroom\n",
      "sex\n",
      "college\n",
      "techniques\n",
      "said\n",
      "pictures\n",
      "gentleman\n",
      "outside\n",
      "screen\n",
      "modeling\n",
      "figured\n",
      "became\n",
      "asked\n",
      "comes\n",
      "prospect\n",
      "whatever\n",
      "pocketbook\n",
      "honeyraexo\n",
      "much\n",
      "630pm\n",
      "shopping\n",
      "photos\n",
      "puerto\n",
      "fellow\n",
      "volunteer\n",
      "case\n",
      "informatio\n",
      "gucci\n",
      "shops\n",
      "almost\n",
      "non\n",
      "trade\n",
      "funny\n",
      "paying\n",
      "seeking\n",
      "same\n",
      "gets\n",
      "libido\n",
      "kik\n",
      "louis\n",
      "keep\n",
      "off\n",
      "older\n",
      "thought\n",
      "person\n",
      "contact\n",
      "model\n",
      "spend\n",
      "money\n",
      "except\n",
      "around\n",
      "big\n",
      "matters\n",
      "sephora\n",
      "panties\n",
      "pounds\n",
      "benefit\n",
      "hair\n",
      "highlinr\n",
      "rican\n",
      "tube\n",
      "asking\n",
      "sneakers\n",
      "business\n",
      "asap\n",
      "lose\n",
      "vuitton\n",
      "getting\n",
      "duo\n",
      "plus\n",
      "ladies\n",
      "own\n",
      "image\n",
      "socks\n",
      "cent\n",
      "female\n",
      "her\n",
      "area\n",
      "hey\n",
      "submit\n",
      "strict\n",
      "was\n",
      "happy\n",
      "jus\n",
      "link\n",
      "buying\n",
      "hosting\n",
      "cursed\n",
      "videos\n",
      "nerd\n",
      "really\n",
      "chance\n",
      "calls\n",
      "wife\n",
      "having\n",
      "newbie\n",
      "asian\n",
      "skin\n",
      "alw\n",
      "someplace\n",
      "buddy\n",
      "passion\n",
      "conversating\n",
      "friendly\n",
      "jerk\n",
      "egyptian\n",
      "confounding\n",
      "drinking\n",
      "did\n",
      "gender\n",
      "accidentally\n",
      "discrete\n",
      "enjoy\n",
      "hanging\n",
      "erotic\n",
      "shiatsu\n",
      "drinks\n",
      "even\n",
      "giving\n",
      "while\n",
      "lights\n",
      "new\n",
      "told\n",
      "shoreline\n",
      "men\n",
      "here\n",
      "hours\n",
      "wherever\n",
      "social\n",
      "prefer\n",
      "private\n",
      "wit\n",
      "husbands\n",
      "strrings\n",
      "use\n",
      "takes\n",
      "lickking\n",
      "few\n",
      "type\n",
      "separated\n",
      "females\n",
      "women\n",
      "high\n",
      "room\n",
      "this\n",
      "ride\n",
      "movies\n",
      "cuddle\n",
      "male\n",
      "spontaneous\n",
      "orral\n",
      "topic\n",
      "minimum\n",
      "something\n",
      "lacks\n",
      "sit\n",
      "rather\n",
      "hou\n",
      "beauty\n",
      "watch\n",
      "after\n",
      "adventure\n",
      "such\n",
      "meaningful\n",
      "pleasure\n",
      "talk\n",
      "wine\n",
      "cute\n",
      "looki\n",
      "its\n",
      "group\n",
      "better\n",
      "hobby\n",
      "might\n",
      "then\n",
      "good\n",
      "fling\n",
      "they\n",
      "easily\n",
      "name\n",
      "always\n",
      "each\n",
      "doing\n",
      "society\n",
      "weeekend\n",
      "mess\n",
      "hangouts\n",
      "got\n",
      "receiving\n",
      "struggle\n",
      "put\n",
      "teach\n",
      "definition\n",
      "could\n",
      "place\n",
      "eatting\n",
      "loud\n",
      "think\n",
      "feel\n",
      "yourself\n",
      "another\n",
      "thick\n",
      "tomorrow\n",
      "little\n",
      "families\n",
      "top\n",
      "anyone\n",
      "their\n",
      "too\n",
      "legs\n",
      "inner\n",
      "foreplay\n",
      "hotel\n",
      "herself\n",
      "boyfriend\n",
      "thhongs\n",
      "loves\n",
      "look\n",
      "talking\n",
      "say\n",
      "any\n",
      "sensual\n",
      "able\n",
      "also\n",
      "finding\n",
      "wanting\n",
      "seductive\n",
      "sure\n",
      "serviced\n",
      "tall\n",
      "garments\n",
      "considered\n",
      "average\n",
      "disease\n",
      "face\n",
      "judgmental\n",
      "clean\n",
      "movie\n",
      "hear\n",
      "cuddled\n",
      "find\n",
      "one\n",
      "busy\n",
      "based\n",
      "situations\n",
      "going\n",
      "black\n",
      "pretty\n",
      "hope\n",
      "wearing\n",
      "his\n",
      "photography\n",
      "hopefully\n",
      "feels\n",
      "cannot\n",
      "him\n",
      "spots\n",
      "married\n",
      "bay\n",
      "bad\n",
      "stuff\n",
      "view\n",
      "see\n",
      "individual\n",
      "ways\n",
      "horny\n",
      "between\n",
      "reading\n",
      "attention\n",
      "subject\n",
      "coffee\n",
      "key\n",
      "limits\n",
      "many\n",
      "taking\n",
      "etc\n",
      "afterwards\n",
      "laugh\n",
      "path\n",
      "openness\n",
      "blonde\n",
      "trust\n",
      "ourselves\n",
      "been\n",
      "wants\n",
      "shoulders\n",
      "drugs\n",
      "crowds\n",
      "hates\n",
      "ready\n",
      "spaces\n",
      "different\n",
      "perhaps\n",
      "pay\n",
      "firearms\n",
      "advise\n",
      "drink\n",
      "hang\n",
      "hand\n",
      "safe\n",
      "smoker\n",
      "hubby\n",
      "know\n",
      "things\n",
      "less\n",
      "sporting\n",
      "weekly\n",
      "blow\n",
      "dining\n",
      "pleasuring\n",
      "womans\n",
      "other\n",
      "has\n",
      "kick\n",
      "real\n",
      "honest\n",
      "discreet\n",
      "desi\n",
      "lady\n",
      "lost\n",
      "buddies\n",
      "night\n",
      "kingsman\n",
      "nude\n",
      "because\n",
      "deal\n",
      "pals\n",
      "hassle\n",
      "duration\n",
      "heartache\n",
      "pen\n",
      "dating\n",
      "rub\n",
      "host\n",
      "exciting\n",
      "super\n",
      "drama\n",
      "range\n",
      "afternoon\n",
      "mixed\n",
      "road\n",
      "nsa\n",
      "commitment\n",
      "doesnt\n",
      "sports\n",
      "lingerrie\n",
      "lonely\n",
      "low\n",
      "lot\n",
      "head\n",
      "plunge\n",
      "boyf\n",
      "girlfriends\n",
      "line\n",
      "bug\n",
      "handsome\n",
      "info\n",
      "videography\n",
      "happen\n",
      "attached\n",
      "mature\n",
      "cit\n",
      "similar\n",
      "called\n",
      "moved\n",
      "life\n",
      "doesn\n",
      "drive\n",
      "right\n",
      "chat\n",
      "door\n",
      "shooting\n",
      "abyss\n",
      "thinks\n",
      "artistic\n",
      "lend\n",
      "problems\n",
      "missed\n",
      "longer\n",
      "age\n",
      "noon\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
