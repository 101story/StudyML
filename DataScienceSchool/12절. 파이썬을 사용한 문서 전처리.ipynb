{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NLTK 자연어 처리 패키지 소개\n",
    "\n",
    "- 샘플 corpus 및 사전\n",
    "- 토큰 생성(tokenizing)\n",
    "- 형태소 분석(stemming/lemmatizing)\n",
    "- 품사 태깅(part-of-speech tagging)\n",
    "- 구문 분석(syntax parsing)\n",
    "\n",
    "## 샘플 corpus\n",
    "샘플 문서 집합\n",
    "\n",
    "download 명령 <br>\n",
    "nltk.download('averaged_perceptron_tagger')<br>\n",
    "nltk.download(\"gutenberg\")<br>\n",
    "nltk.download('punkt')<br>\n",
    "nltk.download('reuters')<br>\n",
    "nltk.download(\"stopwords\")<br>\n",
    "nltk.download(\"taggers\")<br>\n",
    "nltk.download(\"webtext\")<br>\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.  Between _them_ it was more the intimacy\n",
      "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness of her temper had hardly allowed\n",
      "her to impose any restraint; and the shadow of authority being\n",
      "now long passed away, they had been living together as friend and\n",
      "friend very mutually attached, and Emma doing just what she liked;\n",
      "highly esteeming Miss Taylor's judgment, but directed chiefly by\n",
      "her own.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emma_raw = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "print(emma_raw[:1302])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰 생성(tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich',\n",
       " ',',\n",
       " 'with',\n",
       " 'a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(emma_raw[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma', 'Woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "t = RegexpTokenizer(\"[\\w]+\")\n",
    "t.tokenize(emma_raw[50:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석\n",
    "\n",
    " 어근, 접두사/접미사, 품사(POS, part-of-speech) 등 언어적 속성의 구조를 파악\n",
    " \n",
    "- stemming (어근 추출)\n",
    "- lemmatizing (원형 복원)\n",
    "- POS tagging (품사 태깅)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "st.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'design'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "st.stem(\"designed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "st = RegexpStemmer(\"ing\")\n",
    "st.stem(\"cooking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "print(lm.lemmatize(\"cooking\"))\n",
    "print(lm.lemmatize(\"cooking\", pos=\"v\"))\n",
    "print(lm.lemmatize(\"cookbooks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belief\n",
      "believ\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize(\"believes\"))\n",
    "print(LancasterStemmer().stem(\"believes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "POS(part-of-speech)\n",
    "\n",
    "Part-of-Speech Tagset <br>\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.htm<br>\n",
    "http://www.ibm.com/support/knowledgecenter/ko/SS5RWK_3.5.0/com.ibm.discovery.es.ta.doc/iiysspostagset.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'NNS'),\n",
       " ('Emma', 'NNP'),\n",
       " ('by', 'IN'),\n",
       " ('Jane', 'NNP'),\n",
       " ('Austen', 'NNP'),\n",
       " ('1816', 'CD'),\n",
       " (']', 'NNP'),\n",
       " ('VOLUME', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('CHAPTER', 'VBP'),\n",
       " ('I', 'PRP'),\n",
       " ('Emma', 'NNP'),\n",
       " ('Woodhouse', 'NNP'),\n",
       " (',', ','),\n",
       " ('handsome', 'NN'),\n",
       " (',', ','),\n",
       " ('clever', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('rich', 'JJ'),\n",
       " (',', ','),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "tagged_list = pos_tag(word_tokenize(emma_raw[:100]))\n",
    "tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Emma',\n",
       " 'by',\n",
       " 'Jane',\n",
       " 'Austen',\n",
       " '1816',\n",
       " ']',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich',\n",
       " ',',\n",
       " 'with',\n",
       " 'a']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "untag(tagged_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# konlpy 한국어 처리 패키지 소개\n",
    "\n",
    "http://konlpy.org/ko/latest/ <br>\n",
    "https://github.com/konlpy/konlpy\n",
    "\n",
    "* Kkma<br>\n",
    "    http://kkma.snu.ac.kr/<br>\n",
    "* Hannanum<br>\n",
    "    http://semanticweb.kaist.ac.kr/hannanum/<br>\n",
    "* Twitter<br>\n",
    "    https://github.com/twitter/twitter-korean-text/<br>\n",
    "* Komoran<br>\n",
    "    http://www.shineware.co.kr/?page_id=835<br>\n",
    "* Mecab<br>\n",
    "    https://bitbucket.org/eunjeon/mecab-ko-dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: JPype1==0.6.2 from file:///C:/Users/student/Documents/jhfolder/GitRepositories/StudyML/DataScienceSchool/JPype1-0.6.2-cp36-cp36m-win_amd64.whl in c:\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Requirement 'JPype1-0.6.2-cp36-cp36m-win_amd64.whl' looks like a filename, but the file does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install JPype1-0.6.2-cp36-cp36m-win_amd64.whl\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constitution.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "from konlpy.corpus import kolaw\n",
    "kolaw.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n",
      "\n",
      "유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, 조국의 민주개혁과 평화적 통일의\n"
     ]
    }
   ],
   "source": [
    "c = kolaw.open('constitution.txt').read()\n",
    "print(c[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1809890.txt',\n",
       " '1809891.txt',\n",
       " '1809892.txt',\n",
       " '1809893.txt',\n",
       " '1809894.txt',\n",
       " '1809895.txt',\n",
       " '1809896.txt',\n",
       " '1809897.txt',\n",
       " '1809898.txt',\n",
       " '1809899.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.corpus import kobill\n",
    "kobill.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지방공무원법 일부개정법률안\n",
      "\n",
      "(정의화의원 대표발의 )\n",
      "\n",
      " 의 안\n",
      " 번 호\n",
      "\n",
      "9890\n",
      "\n",
      "발의연월일 : 2010.  11.  12.  \n",
      "\n",
      "발  의  자 : 정의화․이명수․김을동 \n",
      "\n",
      "이\n"
     ]
    }
   ],
   "source": [
    "d = kobill.open('1809890.txt').read()\n",
    "print(d[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 처리 유틸리티\n",
    "\n",
    "pprint 유틸리티 함수로 한국어 프린트 해줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한글', {'한글 키': ['한글 밸류1', '한글 밸류2']}]\n"
     ]
    }
   ],
   "source": [
    "x = [u\"한글\", {u\"한글 키\": [u\"한글 밸류1\", u\"한글 밸류2\"]}]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한글', {'한글 키': ['한글 밸류1', '한글 밸류2']}]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.utils import pprint\n",
    "pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석\n",
    "\n",
    "- Kkma\n",
    "- Hannanum\n",
    "- Twitter\n",
    "- Komoran\n",
    "- Mecab\n",
    " <br>-------\n",
    "- morphs : 형태소 추출\n",
    "- nouns : 명사 추출\n",
    "- pos : pos 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import *\n",
    "\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 명사 추출\n",
    "noun 명령"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국헌법',\n",
      " '유구',\n",
      " '역사',\n",
      " '전통',\n",
      " '빛',\n",
      " '우리',\n",
      " '대한국민',\n",
      " '3·1운동',\n",
      " '건립',\n",
      " '대한민국임시정부',\n",
      " '법통',\n",
      " '불의',\n",
      " '항거']\n"
     ]
    }
   ],
   "source": [
    "pprint(hannanum.nouns(c[:65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한',\n",
      " '대한민국',\n",
      " '대한민국헌법',\n",
      " '민국',\n",
      " '헌법',\n",
      " '유구',\n",
      " '역사',\n",
      " '전통',\n",
      " '우리',\n",
      " '국민',\n",
      " '3',\n",
      " '1',\n",
      " '1운동',\n",
      " '운동',\n",
      " '건립',\n",
      " '대한민국임시정부',\n",
      " '임시',\n",
      " '정부',\n",
      " '법통',\n",
      " '불의',\n",
      " '항거']\n"
     ]
    }
   ],
   "source": [
    "pprint(kkma.nouns(c[:65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국',\n",
      " '헌법',\n",
      " '유구',\n",
      " '역사',\n",
      " '전통',\n",
      " '우리',\n",
      " '대한',\n",
      " '국민',\n",
      " '운동',\n",
      " '건립',\n",
      " '대한민국',\n",
      " '임시정부',\n",
      " '법',\n",
      " '통과',\n",
      " '불의',\n",
      " '항거']\n"
     ]
    }
   ],
   "source": [
    "pprint(twitter.nouns(c[:65]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 추출\n",
    "\n",
    "모든 품사의 형태소를 알아내려면 morphs라는 명령을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국헌법',\n",
      " '유구',\n",
      " '하',\n",
      " 'ㄴ',\n",
      " '역사',\n",
      " '와',\n",
      " '전통',\n",
      " '에',\n",
      " '빛',\n",
      " '나는',\n",
      " '우리',\n",
      " '대한국민',\n",
      " '은',\n",
      " '3·1운동',\n",
      " '으로',\n",
      " '건립',\n",
      " '되',\n",
      " 'ㄴ',\n",
      " '대한민국임시정부',\n",
      " '의',\n",
      " '법통',\n",
      " '과',\n",
      " '불의',\n",
      " '에',\n",
      " '항거']\n"
     ]
    }
   ],
   "source": [
    "pprint(hannanum.morphs(c[:65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국',\n",
      " '헌법',\n",
      " '유구',\n",
      " '하',\n",
      " 'ㄴ',\n",
      " '역사',\n",
      " '와',\n",
      " '전통',\n",
      " '에',\n",
      " '빛나',\n",
      " '는',\n",
      " '우리',\n",
      " '대하',\n",
      " 'ㄴ',\n",
      " '국민',\n",
      " '은',\n",
      " '3',\n",
      " '·',\n",
      " '1',\n",
      " '운동',\n",
      " '으로',\n",
      " '건립',\n",
      " '되',\n",
      " 'ㄴ',\n",
      " '대한민국',\n",
      " '임시',\n",
      " '정부',\n",
      " '의',\n",
      " '법통',\n",
      " '과',\n",
      " '불의',\n",
      " '에',\n",
      " '항거']\n"
     ]
    }
   ],
   "source": [
    "pprint(kkma.morphs(c[:65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국',\n",
      " '헌법',\n",
      " '유구',\n",
      " '한',\n",
      " '역사',\n",
      " '와',\n",
      " '전통',\n",
      " '에',\n",
      " '빛나는',\n",
      " '우리',\n",
      " '대한',\n",
      " '국민',\n",
      " '은',\n",
      " '3',\n",
      " '·',\n",
      " '1',\n",
      " '운동',\n",
      " '으로',\n",
      " '건립',\n",
      " '된',\n",
      " '대한민국',\n",
      " '임시정부',\n",
      " '의',\n",
      " '법',\n",
      " '통과',\n",
      " '불의',\n",
      " '에',\n",
      " '항거']\n"
     ]
    }
   ],
   "source": [
    "pprint(twitter.morphs(c[:65]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사 태깅\n",
    "\n",
    "품사(POS)가 붙어있는(tagging) 형태로 형태소 분석\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('대한민국헌법', 'N'),\n",
      " ('유구', 'N'),\n",
      " ('하', 'X'),\n",
      " ('ㄴ', 'E'),\n",
      " ('역사', 'N'),\n",
      " ('와', 'J'),\n",
      " ('전통', 'N'),\n",
      " ('에', 'J'),\n",
      " ('빛', 'N'),\n",
      " ('나는', 'J'),\n",
      " ('우리', 'N'),\n",
      " ('대한국민', 'N'),\n",
      " ('은', 'J'),\n",
      " ('3·1운동', 'N'),\n",
      " ('으로', 'J'),\n",
      " ('건립', 'N'),\n",
      " ('되', 'X'),\n",
      " ('ㄴ', 'E'),\n",
      " ('대한민국임시정부', 'N'),\n",
      " ('의', 'J'),\n",
      " ('법통', 'N'),\n",
      " ('과', 'J'),\n",
      " ('불의', 'N'),\n",
      " ('에', 'J'),\n",
      " ('항거', 'N')]\n"
     ]
    }
   ],
   "source": [
    "pprint(hannanum.pos(c[:65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('대한민국', 'NNG'),\n",
      " ('헌법', 'NNG'),\n",
      " ('유구', 'NNG'),\n",
      " ('하', 'XSV'),\n",
      " ('ㄴ', 'ETD'),\n",
      " ('역사', 'NNG'),\n",
      " ('와', 'JC'),\n",
      " ('전통', 'NNG'),\n",
      " ('에', 'JKM'),\n",
      " ('빛나', 'VV'),\n",
      " ('는', 'ETD'),\n",
      " ('우리', 'NNM'),\n",
      " ('대하', 'VV'),\n",
      " ('ㄴ', 'ETD'),\n",
      " ('국민', 'NNG'),\n",
      " ('은', 'JX'),\n",
      " ('3', 'NR'),\n",
      " ('·', 'SP'),\n",
      " ('1', 'NR'),\n",
      " ('운동', 'NNG'),\n",
      " ('으로', 'JKM'),\n",
      " ('건립', 'NNG'),\n",
      " ('되', 'XSV'),\n",
      " ('ㄴ', 'ETD'),\n",
      " ('대한민국', 'NNG'),\n",
      " ('임시', 'NNG'),\n",
      " ('정부', 'NNG'),\n",
      " ('의', 'JKG'),\n",
      " ('법통', 'NNG'),\n",
      " ('과', 'JC'),\n",
      " ('불의', 'NNG'),\n",
      " ('에', 'JKM'),\n",
      " ('항거', 'NNG')]\n"
     ]
    }
   ],
   "source": [
    "pprint(kkma.pos(c[:65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('대한민국', 'Noun'),\n",
      " ('헌법', 'Noun'),\n",
      " ('유구', 'Noun'),\n",
      " ('한', 'Josa'),\n",
      " ('역사', 'Noun'),\n",
      " ('와', 'Josa'),\n",
      " ('전통', 'Noun'),\n",
      " ('에', 'Josa'),\n",
      " ('빛나는', 'Verb'),\n",
      " ('우리', 'Noun'),\n",
      " ('대한', 'Noun'),\n",
      " ('국민', 'Noun'),\n",
      " ('은', 'Josa'),\n",
      " ('3', 'Number'),\n",
      " ('·', 'Foreign'),\n",
      " ('1', 'Number'),\n",
      " ('운동', 'Noun'),\n",
      " ('으로', 'Josa'),\n",
      " ('건립', 'Noun'),\n",
      " ('된', 'Verb'),\n",
      " ('대한민국', 'Noun'),\n",
      " ('임시정부', 'Noun'),\n",
      " ('의', 'Josa'),\n",
      " ('법', 'Noun'),\n",
      " ('통과', 'Noun'),\n",
      " ('불의', 'Noun'),\n",
      " ('에', 'Josa'),\n",
      " ('항거', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "pprint(twitter.pos(c[:65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 전처리\n",
    "\n",
    "특징 벡터(feature vector)를 문서로부터 추출하는 과정\n",
    "\n",
    "## BOW (Bag of Words)\n",
    "\n",
    "전체 문서  {D1,D2,…,Dn}{D1,D2,…,Dn}  를 구성하는 <br>\n",
    "고정된 단어장(vocabulary)  {W1,W2,…,Wm}{W1,W2,…,Wm}  를 만들고  <br>\n",
    "Di 라는 개별 문서에 단어장에 해당하는 단어들이 포함되어 있는지를 표시하는 방법\n",
    "\n",
    "## Scikit-Learn 의 문서 전처리 기능\n",
    "\n",
    "feature_extraction.text \n",
    "- CountVectorizer:<br>\n",
    "    문서 집합으로부터 단어의 수를 세어 카운트 행렬을 만든다.\n",
    "- TfidfVectorizer:<br>\n",
    "    문서 집합으로부터 단어의 수를 세고 TF-IDF 방식으로 단어의 가중치를 조정한 카운트 행렬을 만든다.\n",
    "- HashingVectorizer: <br>\n",
    "    hashing trick 을 사용하여 빠르게 카운트 행렬을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'last': 4,\n",
       " 'one': 5,\n",
       " 'second': 6,\n",
       " 'the': 7,\n",
       " 'third': 8,\n",
       " 'this': 9}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',    \n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['This is the second document.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 문서 처리 옵션\n",
    "* CountVectorizer는 다양한 인수\n",
    "    * stop_words : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    "        * stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "    * analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    "        * 단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "    * tokenizer : 함수 또는 None (디폴트)\n",
    "        * 토큰 생성 함수 .\n",
    "    * token_pattern : string\n",
    "        * 토큰 정의용 정규 표현식\n",
    "    * ngram_range : (min_n, max_n) 튜플\n",
    "        * n-그램 범위\n",
    "    * max_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "        * 단어장에 포함되기 위한 최대 빈도\n",
    "    * min_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "        * 단어장에 포함되기 위한 최소 빈도\n",
    "    * vocabulary : 사전이나 리스트\n",
    "        * 단어장\n",
    "        \n",
    "        \n",
    "## Stop Words\n",
    "무시할 단어 관사나 접속사, 한국어의 조사 등 .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'first': 1, 'last': 2, 'one': 3, 'second': 4, 'third': 5}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰(token)\n",
    "\n",
    "하나의 단어가 되는 단위\n",
    "\n",
    "analyzer, tokenizer, token_pattern 등의 인수로 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " '?': 2,\n",
       " 'a': 3,\n",
       " 'c': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'l': 10,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " 'o': 13,\n",
       " 'r': 14,\n",
       " 's': 15,\n",
       " 't': 16,\n",
       " 'u': 17}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " '?': 1,\n",
       " 'and': 2,\n",
       " 'document': 3,\n",
       " 'first': 4,\n",
       " 'is': 5,\n",
       " 'last': 6,\n",
       " 'one': 7,\n",
       " 'second': 8,\n",
       " 'the': 9,\n",
       " 'third': 10,\n",
       " 'this': 11}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'third': 1, 'this': 2}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-그램\n",
    "\n",
    "토큰의 크기를 결정<br>\n",
    "1-그램은 토큰 하나만 단어로 사용 2-그램은 두 개의 연결된 토큰을 하나의 단어로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and the': 0,\n",
       " 'first document': 1,\n",
       " 'is the': 2,\n",
       " 'is this': 3,\n",
       " 'last document': 4,\n",
       " 'second document': 5,\n",
       " 'second second': 6,\n",
       " 'the first': 7,\n",
       " 'the last': 8,\n",
       " 'the second': 9,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'this is': 12,\n",
       " 'this the': 13}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'the third': 1, 'third': 2, 'this': 3, 'this the': 4}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도수\n",
    "\n",
    "토큰이 나타난 횟수를 기준으로 단어장을 구성<br>\n",
    "min_df ~ max_df 미만, 초과 인 경우 무시<br>\n",
    "정수인 경우 횟수, 부동소수점인 경우 비중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'document': 0, 'first': 1, 'is': 2, 'this': 3},\n",
       " {'and', 'last', 'one', 'second', 'the', 'third'})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_, vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "\n",
    "모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법\n",
    "\n",
    "문서  d (document)와 단어  t\n",
    "\n",
    "$\\text{tf-idf}(d, t) = \\text{tf}(d, t) \\cdot $$\\text{idf}(d, t)$\n",
    "\n",
    "$\\text{idf}(d, t) = \\log \\dfrac{n_d}{1 + \\text{df}(t)}$\n",
    "\n",
    "* $\\text{tf}(d, t) $ : 단어의 빈도수 \n",
    "\n",
    "* $\\text{idf}(d, t) $ :  inverse document frequency\n",
    "\n",
    "* $n_d$ : 전체 문서의 수\n",
    "\n",
    "* $\\text{df}(t)$ : 단어  t 를 가진 문서의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.38947624,  0.55775063,  0.4629834 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941651,  0.        ,  0.4629834 ],\n",
       "       [ 0.        ,  0.24151532,  0.        ,  0.28709733,  0.        ,\n",
       "         0.        ,  0.85737594,  0.20427211,  0.        ,  0.28709733],\n",
       "       [ 0.55666851,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.55666851,  0.        ,  0.26525553,  0.55666851,  0.        ],\n",
       "       [ 0.        ,  0.38947624,  0.55775063,  0.4629834 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941651,  0.        ,  0.4629834 ],\n",
       "       [ 0.        ,  0.45333103,  0.        ,  0.        ,  0.80465933,\n",
       "         0.        ,  0.        ,  0.38342448,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'last': 4,\n",
       " 'one': 5,\n",
       " 'second': 6,\n",
       " 'the': 7,\n",
       " 'third': 8,\n",
       " 'this': 9}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Trick\n",
    "\n",
    "단어에 대한 인덱스 번호를 생성하기 때문에 메모리 및 실행 시간을 줄임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "len(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%time CountVectorizer().fit(twenty.data).transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 112863 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time hv.transform(twenty.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석기 이용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bought': 0,\n",
       " 'buying': 1,\n",
       " 'buys': 2,\n",
       " 'image': 3,\n",
       " 'imagination': 4,\n",
       " 'imagine': 5,\n",
       " 'imaging': 6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"imaging\", \"image\", \"imagination\", \"imagine\", \"buys\", \"buying\", \"bought\"]\n",
    "vect = CountVectorizer().fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "docs = twenty.data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'write': 0,\n",
       " 'writer': 1,\n",
       " 'writers': 2,\n",
       " 'writes': 3,\n",
       " 'writing': 4,\n",
       " 'writing_': 5,\n",
       " 'written': 6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\", token_pattern=\"wri\\w+\").fit(docs)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'write': 0, 'writer': 1, 'writing_': 2, 'written': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.s = SnowballStemmer('english')\n",
    "        self.t = CountVectorizer(stop_words=\"english\", token_pattern=\"wri\\w+\").build_tokenizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.s.stem(t) for t in self.t(doc)]\n",
    "\n",
    "vect = CountVectorizer(tokenizer=StemTokenizer()).fit(docs)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 웹사이트에 특정한 단어가 어느 정도 사용되었는지 빈도수를 알아보는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "import string\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "req = urllib2.Request(\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\")\n",
    "opener = urllib2.build_opener()\n",
    "f = opener.open(req)\n",
    "json = json.loads(f.read())\n",
    "cell = [\"\\n\".join(c[\"source\"]) for c in json[\"cells\"] if c[\"cell_type\"] == u\"markdown\"]\n",
    "docs = [w for w in hannanum.nouns(\" \".join(cell)) if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer().fit(docs)\n",
    "count = vect.transform(docs).toarray().sum(axis=0)\n",
    "plt.bar(range(len(count)), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprint(zip(vect.get_feature_names(), count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
