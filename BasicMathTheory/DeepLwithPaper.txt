
논문으로 시작하는 딥러닝 강의
https://www.edwith.org/deeplearningchoi/joinLectures/10979

### Convolution Neural Network 기초 
CNN 과정
	Convolution + subsampling + FC(denth layer : 어떤 문제일지 구분) 
	Convolution + subsampling : feature extraction : feature 를 뽑아서 틍징을 알아냄 

왜 잘되나
	local invariance : loosely speaking 
	동일한 filter 가 모두 돌아다님 필터와 얼마나 비슷한지 비슷하면 해당 위치 값이 올라감 
	필터가 학습됨 학습된 필터 적용
	compositionality 계층 구조 hierarchy 

GoogLeNet 
stride 널리뛰다 건너뛰다 
zero-padding, channel
batch 사이즈 이미지 몇장읽는지 
out-channel : 서로 다른 filter 개수 

CNN
	bias 추가 : 각 채널에 동일한 숫자 한개씩 더함
	activation function : leru
	maxpooling : 가장큰숫자
	한줄로 피기 fully connected layer 
		derse -> out 
		연산이 너무 커서 간소화 시키는게 대세 
	
### 4가지 CNN 살펴보기: AlexNET, VGG, GoogLeNet, ResNet

CNN Architectures
- AlexNet
- VGG
- GoogLeNet
- ResNet

Data augmentation 
    label 
    데이터를 늘림 
    좌우를 바꾸거나 이미지를 조각냄 
    augmentation 할때는 도매인을 알아야 한다. 6 을 뒤집으면 달라질수 있음 
AlexNet
    파라미터 수를 잘 계산해야됨 
    동일한 모양의 네트워크가 GPU 가 부족해 갈라짐 
    렐루 

VGG
   옥스포드 대학 간단한 방법으로 좋은 성적을 냄 
   conv3x3
   stright 은 1  
   레이어가 16, 19 단위를 많이 사용함
 
GoogLeNet
    딥러닝이 안된다고 할떄 딥러닝 세계를 연 논문 

Inception module
    conv 가 1, 3, 5, 로 나눠져 있다 
    Actual inception 1x1 이 나이브 inception 보다 더 추가되어있음     
    1x1로 채널의 수를 한번 줄였을때 파라미터의 수가 줄어든다
    1x1x18 + 3x3x5 로 레이어를 정의하는 파라미터 수가 절반이상 줄어든다. 
    채널이 줄어들면 줄어든 채널에서 컨볼루션에 들어드는 파라미터를 줄였다. 
    very clever idea of using one by one convolution for dimension reduction. 
    열개로 1, 3, 5, 컨볼류션으로 갈라진 장점은 같은 컨볼루션이라면 receiptfilter 늘 같이 나온다. receiptfilter 가 여러 값이 나오게되고 다양한 결과를 받을 수 있다. multiple cap.. 이 된다. 
    구글넷 22단 딥해지고 파라미터수는 줄고 

Inception v4
    5x5 는 등장하지 않음 
    7x1 -> 1x7 로 파라메터를 계속 줄여감 
     
ResNet
   다 일등함 범용성 
   네트워크가 딥한게 좋을까? 문제정의로 시작 
   vanishing, exploding gradients 
   좋은 초기화, batch normalization, ReLU 를 잘하면 vanishing 은 덜 중요해진다. 
   오버피팅보다 Degradation problem 이 더 중요하다.  
   트레이닝도 잘되고 테스트도 잘 떨어지는데 성능이 안나옴 
   
   Residual learning building block 
   입력과 출력의 값이 같아야 하는데 출력의 값을 그냥 더해버리면 타겟과 입력의 차이만 학습하겠다. 
   
    Why residual?
    we hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. Shortcut connection are used. 
   The extremely deep residual nets are easy to optimize.
   
    단점은 인풋아웃풋이 같아야 한다. 
    34단 실제는 152단..


### Overfitting 을 막는 regularization

Regularization 
    오버피팅을 막자 
    딥한 레이어를 쓸수록 오버피팅 가능성이 커짐 
    노이즈 때문에 주로 발생함 (관측의 에러) 
    노이즈를 완전히 제거하는건 원데이터와 구분하는건 힘듬
   
Mathematically
    VC dimension 함수의 복잡도 
    in-sample error 트레인에러 - out-of-sample error 테스트 에러 = 갭이 적어지는 것 
    vc 가 복잡해질수록 갭은 더욱 더 커짐 

Preventin OverFitting
	- 데이터를 많이 모은다.
	- 적절한 capacity (능력) 을 같는 모델을 사용 
	- 앙상블을 사용 (배깅 : 모델 다섯개에 결과중 다수 모델이 선택한것을만듬) 
	- dropout, dropconnect, batchNorm 테크닉

Limiting the capacity 
	- architecture, early stopping, weight-decay(어떤 모델이 학습하는 파라메터를 커지지 않게 패털티를 줌) 

Drop connect 
	- 웨이트를 꺼줌
 
Batch Normalization 
	- 미니 배치를 쓸때 평균을 빼고 variance 를 나눠줌 
	- 러닝레잇을 늘려도됨 (발산이 안됨) 
	- 데이터 셋들 사이의 다른 통계치들을 정규화 시켜줘서 러닝레잇을 많이 해도됨 빠르게 수렴하게 해줌 
	- 다른 테크닉 안써도 잘된다. 
	- LRN 안써도 된다. (?)

Book review 
	1. parameter norm penalties : 뉴널넷의 웨이트가 커지지 않게 하기 위해서 제곱을 더하거나 절대값을 더하는 방법으로 파라메터 패널티를 주게 됨
	2. dataset augmentation : 데이터를 많이 써야 하지만 결국 fake data 를 넣어서 학습 데이터를 늘림 label preserving transformation, hand-designed dataset augmentation 을 사용함. 
	3. Noise Robustness : 레이어마다 노이즈를 집어 넣음 또는 웨이트에 노이즈를 넣거나 혹은 label-smoothing 1,0,0 을 0.8, 0.2, 0.2 로 만들어짐
	4. semi-supervised learning : 딥러닝에서 representation 을 찾는것과 관련되어 있다. auto encoder... 이건 무슨말인지..(?)
	5. multi-task learning : 한번에 많은 것을 찾아냄 shared 구조 서로다른 문제들 중에서 그중에 몇가지들 중에서 공통된 무언가가 있다. representation 을 찾는것이 shared 구조에서 찾아짐. 나이, 성별, 등을 한번에 구분해 낼 수 있다. 최근 구글에서 문장을 번역도 하고 감정분석도하고 하는 멀티테스트가 각각의 성능이 좋아짐 
	6. early stopping
	7. parameter tying and parameter sharing : 입력이 다른데 같은 테스트를 하는 것 tying 레이어를 공유하거나 웨이터를 같게 해서 파라메터 수를 줄임, parameter sharing 한 필터가 모든 곳을 돔 cnn
	8. spare representation : 어떤 아웃풋이 나왔을때 대부분이 0 이 나오길 기대, sparse weight 앞단에 0 이 많은것, sparse activation 뒷단에 0 이 많은 것. relu 0보다 작은걸 모두 0 으로 바꾸줌 아웃풋이 0 이 많아짐. 
	9. bagging and other ensemble method : variance 너무 다양하게 나옴, bias 틀림 평균에서 멀어짐. 모델이 안좋을때. Bagging ( high variance -> low Variance 로 만듬. 평균을 만듬. 만개에서 5천개씩 각각의 셋을 다른모델에 돌려서 평균 또는 취합함). boosting (High bias -> low bias 로 각가의 약학습기를 돌려서 틀린 차이 만큼을 다른 모델에 붙임. 학습모델을 붙임 adaboost) 
	10.  drop out
	11. adversarial training : 사람이 볼수 없는 노이즈를 섞어서 학습하게됨. 입력이 아주 조금 변해도 아웃풋이 많이 변함 


### Nature 논문으로 살펴보는 AlphaGo 알고리즘

- complexity of go
- monte-carlo tree search
- training networks
- searching with policy and value networks
- experiments

체스는 사람이 절대 이길수 없다. 하지만 경우의 바둑의 수는 250^150 ~ 10^360 우주의 원자의 수보다 많다. 아직은 사람이 이길 수 있다.


monte-carlo tree search 
	나와 상대방이 번가라서 수를 두는 방법 
	나와 상대방이 각 다르게 둘때마다 다른 전개가 일어남
	제일 상대방이 둘꺼 같은 수 
	
	selection : 리프노드가 나올때까지 계속 셀랙션이 일어남
	expansion : 늘려보는것 다음번 노드로
	simulation : 시뮬레이션이 끝날떄까지 경기를 계속해봄 
	backpropagation : 시뮬레이션이 끝날때까 경기를 계속했다가 결과를 반영함 
		내가 둔 수 이후에 무슨 경우를 하더라도 졌다 (안좋은 수였다) 
	
	ex) 서로 다른 수를 전부 둬서 평가를 하는 방식 3번 둬서 2번 이겼다. 각 이긴 수/둔 횟수

MCTS in AlphaGo
	Rollout policy
	SL policy network
	RL policy network
	value network 

	Rollout policy
	SL policy network
	두개는 사람의 기보만 가지고 학습이 됨 
	입출력은 같음 사람이 두면 다음의 바둑판을 찾음 
	네트워크의 모양에 차이가 좀 있음 얇고, 깊고
	같은걸 학습해도 SL 이 성능은 좋다 
	rollout 빠르게 연산 
	
	RL policy network
	알파고끼리 둔 수 
	
	policy 상대방이 두고 다음 어디에 둘지 
	
	value network 
	바둑이 끝난것을 알려면 바둑판을 전부 채워봐야 한다. 
	시뮬이 진행될떄마다 판새를 판단하기 위함 (언제 끝내야할지) 

	Rollout policy
	SL policy network
	expansion 에 활용됨 

	RL policy 는 value network 에만 활용됨 
	RL policy 로 부터 나오는 기보로 value network 가 학습됨 
	상대방(사람)이 어디에 둘지 예측해서 탐색함 (rollout, SL)
	value 는 알파고가 기준이 되서 이게 좋은지 나쁜지 판단함 (사람이 두는수랑 다른이유)
	알파고끼리 학습한 기보 
	
Policy net vs Value net
	Policy Net
		인풋 아웃풋 다 바둑판		

	Value	
		인풋 바둑판 아웃풋 숫자 하나 좋다 나쁘다

	selection, expansion : SL policy 활용
	simulation, backpropagation : rollout, value net 활용
	value net 학습 시에만 RL net 활용 

Training
	Rollout Policy Network (rollout net) 
	Supervised Learning Policy Network (sl net)
	Reinforcement Learning Policy Network (RL net)
	Value Network (value net) 
	
	policy net
		1. 바둑판 필터 ReLU
		2~12. 바둑판 필너 ReLU
		13. SoftMax 

	value network 
		1. 바둑판 필너 ReLU
		2~11. 바둑판 필너 ReLU
		12, 13. No Info
		14. ReLU, Fc
		15. Tanh, Fc : -1~1 까지 예측 output 을 normalize 해줌 

Input Features
	이용해서 도메인지식을 input 을 넣음 
	planes (채널)
	stone colour 돌색 흑, 백, 비어있음
	one 1 로 가득차 있는 것 
	feature 쓸떄 안쓸떄 성능차이 

Rollout, SL 
	학습 간단, 분류문제 

Policy Gradient Reinforcement learning
	이긴판에 대해서는 reward 1 
	지면 reward  -1 로 줌 
	해당 판 t 에서 s 엑션을 취하면 a 확률을 높임 
	이긴판은 모든걸 positive
	진판은 모든걸 negative 하게 놓음

Training value nets
	사람이 둔 기록만 가지고는 데이터가 작아 over fitting 의 문제가 있고
	알파고가 두는 수는 사람과 다를 수 있기 때문에 self-play data 를 추가함 

Computational Power
	분산 알파고 연산
		40 개 스레드 1202 개 cpu, 176 gpu
	train policy 
		50 gpu 로 하루 
	train value 
		50 gpu 로 한주
	(유럽 파노이랑 경기) 

	1202 개 cpu, 176 gpu 는 런타입에 매번 경기할때마다 사용됨 

게임 결과 예측 
	각각의 방법론으로 게임 결과를 예측한 결과 
	policy networks, 100 rollout 해서 얻어진 결과로 판결냄

selection rule
	q : status 가 주어졌을때 다음번 어디에 둘지 예측 
	u : policy function 에 Nr 몇번 들어왔는지 
	분산 시스템을 위해서 cpu 가 각각의 다른 수를 볼 수 있또록 많은 수를 보고 있으면 패널티를 줌 

expansion rule
	visit count 를 넘으면 다음번 stats 가 search tree 에 추가
	트리로부터 다시 expansion 적용됨 
	하나가 결정되면 selection expansion 후 마지막 판까지 전부 시뮬레이션하고 다시 백으로 결과 반영 roll out 

each edge 
	Nv Nr : value net, rollout net 통해서 얻어진 정보 
		그 네트워크를 몇번 체크 했는지 (많이 들어가면 중요하다)
	Wv Wr : 정말 이겼는지 졌는지 
		value net 일정값 이상이나 이하가되면 네트워크를 끊고 해당 네트워크에 결과 반영
		rollout 끝까지 다 했을때 얻어지는 score 
	
final execution 
	람다 튜닝 파라미터 0.5 두개의 결과를 반반 합침 

결과 
	싱글 분산 비교 
	분산으로 돌린것이 훨씬 성능이 좋다. 

도메인 지식에 기존의 문제 풀이 방법(MCTS)에 딥러닝을 기존의 방법론에 어떻게 넣을지 좋은 연구 흐름
	
