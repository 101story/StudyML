
논문으로 시작하는 딥러닝 강의
https://www.edwith.org/deeplearningchoi/joinLectures/10979

### Convolution Neural Network 기초 


### 4가지 CNN 살펴보기: AlexNET, VGG, GoogLeNet, ResNet

CNN Architectures
- AlexNet
- VGG
- GoogLeNet
- ResNet

Data augmentation 
    label 
    데이터를 늘림 
    좌우를 바꾸거나 이미지를 조각냄 
    augmentation 할때는 도매인을 알아야 한다. 6 을 뒤집으면 달라질수 있음 
AlexNet
    파라미터 수를 잘 계산해야됨 
    동일한 모양의 네트워크가 GPU 가 부족해 갈라짐 
    렐루 

VGG
   옥스포드 대학 간단한 방법으로 좋은 성적을 냄 
   conv3x3
   stright 은 1  
   레이어가 16, 19 단위를 많이 사용함
 
GoogLeNet
    딥러닝이 안된다고 할떄 딥러닝 세계를 연 논문 

Inception module
    conv 가 1, 3, 5, 로 나눠져 있다 
    Actual inception 1x1 이 나이브 inception 보다 더 추가되어있음     
    1x1로 채널의 수를 한번 줄였을때 파라미터의 수가 줄어든다
    1x1x18 + 3x3x5 로 레이어를 정의하는 파라미터 수가 절반이상 줄어든다. 
    채널이 줄어들면 줄어든 채널에서 컨볼루션에 들어드는 파라미터를 줄였다. 
    very clever idea of using one by one convolution for dimension reduction. 
    열개로 1, 3, 5, 컨볼류션으로 갈라진 장점은 같은 컨볼루션이라면 receiptfilter 늘 같이 나온다. receiptfilter 가 여러 값이 나오게되고 다양한 결과를 받을 수 있다. multiple cap.. 이 된다. 
    구글넷 22단 딥해지고 파라미터수는 줄고 

Inception v4
    5x5 는 등장하지 않음 
    7x1 -> 1x7 로 파라메터를 계속 줄여감 
     
ResNet
   다 일등함 범용성 
   네트워크가 딥한게 좋을까? 문제정의로 시작 
   vanishing, exploding gradients 
   좋은 초기화, batch normalization, ReLU 를 잘하면 vanishing 은 덜 중요해진다. 
   오버피팅보다 Degradation problem 이 더 중요하다.  
   트레이닝도 잘되고 테스트도 잘 떨어지는데 성능이 안나옴 
   
   Residual learning building block 
   입력과 출력의 값이 같아야 하는데 출력의 값을 그냥 더해버리면 타겟과 입력의 차이만 학습하겠다. 
   
    Why residual?
    we hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. Shortcut connection are used. 
   The extremely deep residual nets are easy to optimize.
   
    단점은 인풋아웃풋이 같아야 한다. 
    34단 실제는 152단..


### Overfitting 을 막는 regularization

Regularization 
    오버피팅을 막자 
    딥한 레이어를 쓸수록 오버피팅 가능성이 커짐 
    노이즈 때문에 주로 발생함 (관측의 에러) 
    노이즈를 완전히 제거하는건 원데이터와 구분하는건 힘듬
   
Mathematically
    VC dimension 함수의 복잡도 
    in-sample error 트레인에러 - out-of-sample error 테스트 에러 = 갭이 적어지는 것 
    vc 가 복잡해질수록 갭은 더욱 더 커짐 

Preventin OverFitting
- 데이터를 많이 모은다.
- 적절한 capacity (능력) 을 같는 모델을 사용 
- 앙상블을 사용 (배깅 : 모델 다섯개에 결과중 다수 모델이 선택한것을만듬) 
- dropout, dropconnect, batchNorm 테크닉

Limiting the capacity 
- architecture, early stopping, weight-decay(어떤 모델이 학습하는 파라메터를 커지지 않게 패털티를 줌) 

Drop connect 
- 웨이트를 꺼줌
 
Batch Normalization 
- 미니 배치를 쓸때 평균을 빼고 variance 를 나눠줌 
- 러닝레잇을 늘려도됨 (발산이 안됨) 
- 데이터 셋들 사이의 다른 통계치들을 정규화 시켜줘서 러닝레잇을 많이 해도됨 빠르게 수렴하게 해줌 
- 다른 테크닉 안써도 잘된다. 
- LRN 안써도 된다. (?)

Book review 
1. parameter norm penalties : 뉴널넷의 웨이트가 커지지 않게 하기 위해서 제곱을 더하거나 절대값을 더하는 방법으로 파라메터 패널티를 주게 됨
2. dataset augmentation : 데이터를 많이 써야 하지만 결국 fake data 를 넣어서 학습 데이터를 늘림 label preserving transformation, hand-designed dataset augmentation 을 사용함. 
3. Noise Robustness : 레이어마다 노이즈를 집어 넣음 또는 웨이트에 노이즈를 넣거나 혹은 label-smoothing 1,0,0 을 0.8, 0.2, 0.2 로 만들어짐
4. semi-supervised learning : 딥러닝에서 representation 을 찾는것과 관련되어 있다. auto encoder... 이건 무슨말인지..(?)
5. multi-task learning : 한번에 많은 것을 찾아냄 shared 구조 서로다른 문제들 중에서 그중에 몇가지들 중에서 공통된 무언가가 있다. representation 을 찾는것이 shared 구조에서 찾아짐. 나이, 성별, 등을 한번에 구분해 낼 수 있다. 최근 구글에서 문장을 번역도 하고 감정분석도하고 하는 멀티테스트가 각각의 성능이 좋아짐 
6. early stopping
7. parameter tying and parameter sharing : 입력이 다른데 같은 테스트를 하는 것 tying 레이어를 공유하거나 웨이터를 같게 해서 파라메터 수를 줄임, parameter sharing 한 필터가 모든 곳을 돔 cnn
8. spare representation : 어떤 아웃풋이 나왔을때 대부분이 0 이 나오길 기대, sparse weight 앞단에 0 이 많은것, sparse activation 뒷단에 0 이 많은 것. relu 0보다 작은걸 모두 0 으로 바꾸줌 아웃풋이 0 이 많아짐. 
9. bagging and other ensemble method : variance 너무 다양하게 나옴, bias 틀림 평균에서 멀어짐. 모델이 안좋을때. Bagging ( high variance -> low Variance 로 만듬. 평균을 만듬. 만개에서 5천개씩 각각의 셋을 다른모델에 돌려서 평균 또는 취합함). boosting (High bias -> low bias 로 각가의 약학습기를 돌려서 틀린 차이 만큼을 다른 모델에 붙임. 학습모델을 붙임 adaboost) 
10.  drop out
11. adversarial training : 사람이 볼수 없는 노이즈를 섞어서 학습하게됨. 입력이 아주 조금 변해도 아웃풋이 많이 변함 


